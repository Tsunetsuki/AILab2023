{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x17cac1649a0>"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "1. Write a custom dataset class for the titanic data (see the data folder on GitHub).\n",
    "2. Use only the features: \"Pclass\", \"Age\", \"SibSp\", \"Parch\", „Fare“, „Sex“, „Embarked“.\n",
    "3. Preprocess the features accordingly in that class (scaling, one-hot-encoding, etc) and\n",
    "4. split the data into train and validation data (80% and 20%). The constructor of that class\n",
    "should look like this:\n",
    "```\n",
    "titanic_train = TitanicDataSet('titanic.csv', train=True)\n",
    "titanic_val = TitanicDataSet('titanic.csv', train=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TitanicDataSet(root_dir, train):\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    onehot_enc = OneHotEncoder()\n",
    "\n",
    "    titanic = pd.read_csv(root_dir)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(69)\n",
    "    train_indices, test_indices = [ds.indices for ds in torch.utils.data.random_split(titanic, [0.8, 0.2], generator=generator)]\n",
    "    \n",
    "    if train:\n",
    "      titanic = titanic.iloc[train_indices]\n",
    "    else:\n",
    "      titanic = titanic.iloc[test_indices]\n",
    "\n",
    "    # only need \"Pclass\", \"Age\", \"SibSp\", \"Parch\", „Fare“, „Sex“, „Embarked“\n",
    "    titanic = titanic[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\", \"Survived\"]]\n",
    "\n",
    "    # because i found NaN in 'Age' column, so filled it with mean value\n",
    "    # remove the NaN data in the dataset, which is from Embarked column, two rows\n",
    "    mean_values = titanic[titanic.select_dtypes(exclude=['object']).columns].mean()\n",
    "    titanic = titanic.fillna(mean_values)\n",
    "    titanic = titanic.dropna()\n",
    "    titanic = titanic.reset_index(drop=True) # reset the index, or combine_features will cause wrong index and length\n",
    "\n",
    "    # devide the data into categorical features and numerical features, and put the 'Survived' column into categorical features\n",
    "    categorical_features = titanic[titanic.select_dtypes(include=['object']).columns.tolist()]\n",
    "    numerical_features = titanic[titanic.select_dtypes(exclude=['object']).columns].drop('Survived', axis=1)\n",
    "    label_features = titanic['Survived']\n",
    "\n",
    "    # use one-hot encoding to transform categorical features to numerical features\n",
    "    numerical_features_arr = minmax_scaler.fit_transform(numerical_features)\n",
    "    categorical_features_arr = onehot_enc.fit_transform(categorical_features).toarray()\n",
    "\n",
    "    # combine the numerical features and categorical features\n",
    "    combined_features = pd.DataFrame(data=numerical_features_arr, columns=numerical_features.columns)\n",
    "    combined_features = pd.concat([combined_features, pd.DataFrame(data=categorical_features_arr)], axis=1)\n",
    "    combined_features = pd.concat([combined_features, label_features], axis=1).reset_index(drop=True)\n",
    "\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset len: 711\n",
      "val_dataset len: 178\n",
      "total_dataset len: 889\n"
     ]
    }
   ],
   "source": [
    "titanic_train = TitanicDataSet('./data/titanic.csv', train=True)\n",
    "titanic_val = TitanicDataSet('./data/titanic.csv', train=False)\n",
    "print('train_dataset len:', len(titanic_train))\n",
    "print('val_dataset len:', len(titanic_val))\n",
    "print('total_dataset len:', len(titanic_train) + len(titanic_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.296306</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.032596</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258608</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.421965</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.028107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.296306</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.036598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.044986</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.054457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.366566</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384267</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.371701</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>711 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Age  SibSp     Parch      Fare    0    1    2    3    4  \\\n",
       "0       1.0  0.296306  0.000  0.333333  0.032596  1.0  0.0  0.0  0.0  1.0   \n",
       "1       1.0  0.258608  0.000  0.000000  0.016908  0.0  1.0  0.0  0.0  1.0   \n",
       "2       1.0  0.421965  0.125  0.166667  0.028107  0.0  1.0  0.0  0.0  1.0   \n",
       "3       1.0  0.271174  0.000  0.000000  0.014680  0.0  1.0  0.0  0.0  1.0   \n",
       "4       0.5  0.296306  0.250  0.500000  0.036598  1.0  0.0  0.0  0.0  1.0   \n",
       "..      ...       ...    ...       ...       ...  ...  ...  ...  ...  ...   \n",
       "706     1.0  0.044986  0.375  0.333333  0.054457  0.0  1.0  0.0  0.0  1.0   \n",
       "707     1.0  0.366566  0.000  0.000000  0.015412  0.0  1.0  0.0  0.0  1.0   \n",
       "708     0.0  0.384267  0.125  0.000000  0.101497  0.0  1.0  0.0  0.0  1.0   \n",
       "709     0.0  0.723549  0.000  0.000000  0.285990  1.0  0.0  1.0  0.0  0.0   \n",
       "710     1.0  0.371701  0.125  0.000000  0.031425  0.0  1.0  0.0  0.0  1.0   \n",
       "\n",
       "     Survived  \n",
       "0           1  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           1  \n",
       "..        ...  \n",
       "706         0  \n",
       "707         0  \n",
       "708         0  \n",
       "709         1  \n",
       "710         0  \n",
       "\n",
       "[711 rows x 11 columns]"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "Build a neural network with: \n",
    "1. [v] one hidden layer of size 3 that predicts the survival of the\n",
    "passengers. \n",
    "2. [v] Use a BCE loss (Hint: you need a sigmoid activation in the output layer).\n",
    "3. [v] Use a data loader to train in batches of size 16 and shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        features = torch.FloatTensor(sample[:-1])  # Exclude the 'Survived' column\n",
    "        label = torch.FloatTensor([sample['Survived']])  # 'Survived' column as label\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # the weight and bias of linear1 will be initialized \n",
    "        # you can access them by self.linear1.weight and self.linear1.bias\n",
    "        self.linear1 = nn.Linear(D_in, H) # this will create weight, bias for linear1\n",
    "        self.linear2 = nn.Linear(H, D_out) # this will create weight, bias for linear2\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        y_pred = self.sigmoid(self.linear2(h_relu))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManyLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, num_hidden_layers):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(ManyLayerNet, self).__init__()\n",
    "        # the weight and bias of linear1 will be initialized \n",
    "        # you can access them by self.linear1.weight and self.linear1.bias\n",
    "        # slightly more irregular test results with dropout set to 0.0\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        self.first = nn.Linear(D_in, H) # this will create weight, bias for linear1       \n",
    "        self.hidden_layers = [nn.Linear(H, H) for _ in range(num_hidden_layers)]\n",
    "        self.last = nn.Linear(H, D_out)\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.first(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        y_pred = self.sigmoid(self.last(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; \n",
    "# D_in is input dimension; \t10 features from Pclass/Age/SibSp/Parch/Fare/Sex[0\t1]/Embarked[2\t3\t4]\n",
    "# H is hidden dimension (Only one hidden layer, but containing 3 neurons.); \n",
    "# D_out is output dimension: 1 or 0 (Survived or not) 1 dimension for binary classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N, D_in, H, D_out = 16, 10, 40, 1\n",
    "lr = 0.1\n",
    "num_hidden_layers = 10\n",
    "\n",
    "network = ManyLayerNet(D_in, H, D_out, num_hidden_layers)  # H=3 for one hidden layer with 3 neurons\n",
    "optimizer = optim.Adam(network.parameters(), lr)  # RMSProp + Momentum \n",
    "criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss\n",
    "\n",
    "n_epochs = 30 # You can adjust the number of epochs as needed\n",
    "log_interval = 10 # Print the training status every log_interval epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(titanic_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=N, shuffle=True)\n",
    "test_dataset = CustomDataset(titanic_val)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=N, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataloader))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    train_losses = [] # Save the loss value of each training loop (epoch) of the neural network model during the training process\n",
    "    train_accuracies = []\n",
    "    network.train()\n",
    "    correct = 0\n",
    "    cur_count = 0 \n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "                \n",
    "        # Accuracy\n",
    "        pred = (output >= 0.5).float()  # survival_rate is the threshold\n",
    "        correct += (pred == target).sum().item()\n",
    "        cur_count += len(data)\n",
    "\n",
    "        # backword propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                epoch, \n",
    "                cur_count, \n",
    "                len(train_dataloader.dataset),\n",
    "                100. * cur_count / len(train_dataloader.dataset), \n",
    "                loss.item(), \n",
    "                correct, len(train_dataloader.dataset),\n",
    "                100. * correct / len(train_dataloader.dataset))\n",
    "            )\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # train_counter.append((batch_idx*16) + ((epoch-1)*len(train_dataloader.dataset)))\n",
    "    return correct / len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # test_counter = [i*len(titanic_train) for i in range(n_epochs+1)] # how many data for training so far\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # forward propagation\n",
    "            output = network(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            # Accuracy\n",
    "            pred = (output >= 0.5).float()  # 0.5 is the threshold\n",
    "            correct += (pred == target).sum().item()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_accuracy = correct / len(test_dataloader.dataset)\n",
    "\n",
    "    return test_accuracy\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, \n",
    "        correct, \n",
    "        len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [16/711 (2%)]\tLoss: 0.685529\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 1 [176/711 (25%)]\tLoss: 0.691927\t Accuracy: 105/711 (15%)\n",
      "Train Epoch: 1 [336/711 (47%)]\tLoss: 0.644201\t Accuracy: 203/711 (29%)\n",
      "Train Epoch: 1 [496/711 (70%)]\tLoss: 0.738649\t Accuracy: 300/711 (42%)\n",
      "Train Epoch: 1 [656/711 (92%)]\tLoss: 0.664100\t Accuracy: 401/711 (56%)\n",
      "Train Epoch: 2 [16/711 (2%)]\tLoss: 0.625327\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 2 [176/711 (25%)]\tLoss: 0.692423\t Accuracy: 110/711 (15%)\n",
      "Train Epoch: 2 [336/711 (47%)]\tLoss: 0.661067\t Accuracy: 211/711 (30%)\n",
      "Train Epoch: 2 [496/711 (70%)]\tLoss: 0.685320\t Accuracy: 306/711 (43%)\n",
      "Train Epoch: 2 [656/711 (92%)]\tLoss: 0.639810\t Accuracy: 405/711 (57%)\n",
      "Train Epoch: 3 [16/711 (2%)]\tLoss: 0.721444\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 3 [176/711 (25%)]\tLoss: 0.748442\t Accuracy: 114/711 (16%)\n",
      "Train Epoch: 3 [336/711 (47%)]\tLoss: 0.638049\t Accuracy: 210/711 (30%)\n",
      "Train Epoch: 3 [496/711 (70%)]\tLoss: 0.673105\t Accuracy: 301/711 (42%)\n",
      "Train Epoch: 3 [656/711 (92%)]\tLoss: 0.728344\t Accuracy: 399/711 (56%)\n",
      "Train Epoch: 4 [16/711 (2%)]\tLoss: 0.615487\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 4 [176/711 (25%)]\tLoss: 0.608128\t Accuracy: 116/711 (16%)\n",
      "Train Epoch: 4 [336/711 (47%)]\tLoss: 0.648530\t Accuracy: 212/711 (30%)\n",
      "Train Epoch: 4 [496/711 (70%)]\tLoss: 0.725193\t Accuracy: 306/711 (43%)\n",
      "Train Epoch: 4 [656/711 (92%)]\tLoss: 0.655034\t Accuracy: 399/711 (56%)\n",
      "Train Epoch: 5 [16/711 (2%)]\tLoss: 0.718470\t Accuracy: 7/711 (1%)\n",
      "Train Epoch: 5 [176/711 (25%)]\tLoss: 0.650304\t Accuracy: 100/711 (14%)\n",
      "Train Epoch: 5 [336/711 (47%)]\tLoss: 0.619310\t Accuracy: 198/711 (28%)\n",
      "Train Epoch: 5 [496/711 (70%)]\tLoss: 0.606296\t Accuracy: 291/711 (41%)\n",
      "Train Epoch: 5 [656/711 (92%)]\tLoss: 0.559209\t Accuracy: 399/711 (56%)\n",
      "Train Epoch: 6 [16/711 (2%)]\tLoss: 0.775369\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 6 [176/711 (25%)]\tLoss: 0.680908\t Accuracy: 107/711 (15%)\n",
      "Train Epoch: 6 [336/711 (47%)]\tLoss: 0.622790\t Accuracy: 209/711 (29%)\n",
      "Train Epoch: 6 [496/711 (70%)]\tLoss: 0.630588\t Accuracy: 317/711 (45%)\n",
      "Train Epoch: 6 [656/711 (92%)]\tLoss: 0.588455\t Accuracy: 423/711 (59%)\n",
      "Train Epoch: 7 [16/711 (2%)]\tLoss: 0.555243\t Accuracy: 12/711 (2%)\n",
      "Train Epoch: 7 [176/711 (25%)]\tLoss: 0.627742\t Accuracy: 116/711 (16%)\n",
      "Train Epoch: 7 [336/711 (47%)]\tLoss: 0.679596\t Accuracy: 222/711 (31%)\n",
      "Train Epoch: 7 [496/711 (70%)]\tLoss: 0.638530\t Accuracy: 334/711 (47%)\n",
      "Train Epoch: 7 [656/711 (92%)]\tLoss: 0.568253\t Accuracy: 442/711 (62%)\n",
      "Train Epoch: 8 [16/711 (2%)]\tLoss: 0.663847\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 8 [176/711 (25%)]\tLoss: 0.608990\t Accuracy: 132/711 (19%)\n",
      "Train Epoch: 8 [336/711 (47%)]\tLoss: 0.569092\t Accuracy: 235/711 (33%)\n",
      "Train Epoch: 8 [496/711 (70%)]\tLoss: 0.709676\t Accuracy: 345/711 (49%)\n",
      "Train Epoch: 8 [656/711 (92%)]\tLoss: 0.686350\t Accuracy: 466/711 (66%)\n",
      "Train Epoch: 9 [16/711 (2%)]\tLoss: 0.549305\t Accuracy: 12/711 (2%)\n",
      "Train Epoch: 9 [176/711 (25%)]\tLoss: 0.498422\t Accuracy: 125/711 (18%)\n",
      "Train Epoch: 9 [336/711 (47%)]\tLoss: 0.634058\t Accuracy: 236/711 (33%)\n",
      "Train Epoch: 9 [496/711 (70%)]\tLoss: 0.689185\t Accuracy: 348/711 (49%)\n",
      "Train Epoch: 9 [656/711 (92%)]\tLoss: 0.457567\t Accuracy: 470/711 (66%)\n",
      "Train Epoch: 10 [16/711 (2%)]\tLoss: 0.653609\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 10 [176/711 (25%)]\tLoss: 0.570471\t Accuracy: 117/711 (16%)\n",
      "Train Epoch: 10 [336/711 (47%)]\tLoss: 0.574300\t Accuracy: 234/711 (33%)\n",
      "Train Epoch: 10 [496/711 (70%)]\tLoss: 0.462250\t Accuracy: 349/711 (49%)\n",
      "Train Epoch: 10 [656/711 (92%)]\tLoss: 0.701398\t Accuracy: 463/711 (65%)\n",
      "Train Epoch: 11 [16/711 (2%)]\tLoss: 0.543195\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 11 [176/711 (25%)]\tLoss: 0.504053\t Accuracy: 132/711 (19%)\n",
      "Train Epoch: 11 [336/711 (47%)]\tLoss: 0.453644\t Accuracy: 254/711 (36%)\n",
      "Train Epoch: 11 [496/711 (70%)]\tLoss: 0.595508\t Accuracy: 372/711 (52%)\n",
      "Train Epoch: 11 [656/711 (92%)]\tLoss: 0.591035\t Accuracy: 491/711 (69%)\n",
      "Train Epoch: 12 [16/711 (2%)]\tLoss: 0.427807\t Accuracy: 13/711 (2%)\n",
      "Train Epoch: 12 [176/711 (25%)]\tLoss: 0.718516\t Accuracy: 133/711 (19%)\n",
      "Train Epoch: 12 [336/711 (47%)]\tLoss: 0.526413\t Accuracy: 249/711 (35%)\n",
      "Train Epoch: 12 [496/711 (70%)]\tLoss: 0.595820\t Accuracy: 363/711 (51%)\n",
      "Train Epoch: 12 [656/711 (92%)]\tLoss: 0.547854\t Accuracy: 477/711 (67%)\n",
      "Train Epoch: 13 [16/711 (2%)]\tLoss: 0.559609\t Accuracy: 12/711 (2%)\n",
      "Train Epoch: 13 [176/711 (25%)]\tLoss: 0.489621\t Accuracy: 135/711 (19%)\n",
      "Train Epoch: 13 [336/711 (47%)]\tLoss: 0.593099\t Accuracy: 251/711 (35%)\n",
      "Train Epoch: 13 [496/711 (70%)]\tLoss: 0.679226\t Accuracy: 370/711 (52%)\n",
      "Train Epoch: 13 [656/711 (92%)]\tLoss: 0.563448\t Accuracy: 481/711 (68%)\n",
      "Train Epoch: 14 [16/711 (2%)]\tLoss: 0.804727\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 14 [176/711 (25%)]\tLoss: 0.443882\t Accuracy: 130/711 (18%)\n",
      "Train Epoch: 14 [336/711 (47%)]\tLoss: 0.654288\t Accuracy: 244/711 (34%)\n",
      "Train Epoch: 14 [496/711 (70%)]\tLoss: 0.499346\t Accuracy: 367/711 (52%)\n",
      "Train Epoch: 14 [656/711 (92%)]\tLoss: 0.448003\t Accuracy: 488/711 (69%)\n",
      "Train Epoch: 15 [16/711 (2%)]\tLoss: 0.505394\t Accuracy: 12/711 (2%)\n",
      "Train Epoch: 15 [176/711 (25%)]\tLoss: 0.506611\t Accuracy: 129/711 (18%)\n",
      "Train Epoch: 15 [336/711 (47%)]\tLoss: 0.585409\t Accuracy: 248/711 (35%)\n",
      "Train Epoch: 15 [496/711 (70%)]\tLoss: 0.686622\t Accuracy: 360/711 (51%)\n",
      "Train Epoch: 15 [656/711 (92%)]\tLoss: 0.436535\t Accuracy: 477/711 (67%)\n",
      "Train Epoch: 16 [16/711 (2%)]\tLoss: 0.443423\t Accuracy: 14/711 (2%)\n",
      "Train Epoch: 16 [176/711 (25%)]\tLoss: 0.424321\t Accuracy: 130/711 (18%)\n",
      "Train Epoch: 16 [336/711 (47%)]\tLoss: 0.762178\t Accuracy: 246/711 (35%)\n",
      "Train Epoch: 16 [496/711 (70%)]\tLoss: 0.516155\t Accuracy: 365/711 (51%)\n",
      "Train Epoch: 16 [656/711 (92%)]\tLoss: 0.332297\t Accuracy: 480/711 (68%)\n",
      "Train Epoch: 17 [16/711 (2%)]\tLoss: 0.265311\t Accuracy: 15/711 (2%)\n",
      "Train Epoch: 17 [176/711 (25%)]\tLoss: 0.619728\t Accuracy: 140/711 (20%)\n",
      "Train Epoch: 17 [336/711 (47%)]\tLoss: 0.744396\t Accuracy: 256/711 (36%)\n",
      "Train Epoch: 17 [496/711 (70%)]\tLoss: 0.521666\t Accuracy: 381/711 (54%)\n",
      "Train Epoch: 17 [656/711 (92%)]\tLoss: 0.513733\t Accuracy: 498/711 (70%)\n",
      "Train Epoch: 18 [16/711 (2%)]\tLoss: 0.572812\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 18 [176/711 (25%)]\tLoss: 0.517990\t Accuracy: 132/711 (19%)\n",
      "Train Epoch: 18 [336/711 (47%)]\tLoss: 0.724393\t Accuracy: 254/711 (36%)\n",
      "Train Epoch: 18 [496/711 (70%)]\tLoss: 0.518796\t Accuracy: 366/711 (51%)\n",
      "Train Epoch: 18 [656/711 (92%)]\tLoss: 0.819654\t Accuracy: 486/711 (68%)\n",
      "Train Epoch: 19 [16/711 (2%)]\tLoss: 0.636109\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 19 [176/711 (25%)]\tLoss: 0.691419\t Accuracy: 123/711 (17%)\n",
      "Train Epoch: 19 [336/711 (47%)]\tLoss: 0.453392\t Accuracy: 234/711 (33%)\n",
      "Train Epoch: 19 [496/711 (70%)]\tLoss: 0.445117\t Accuracy: 359/711 (50%)\n",
      "Train Epoch: 19 [656/711 (92%)]\tLoss: 0.388408\t Accuracy: 478/711 (67%)\n",
      "Train Epoch: 20 [16/711 (2%)]\tLoss: 0.366847\t Accuracy: 15/711 (2%)\n",
      "Train Epoch: 20 [176/711 (25%)]\tLoss: 0.261300\t Accuracy: 137/711 (19%)\n",
      "Train Epoch: 20 [336/711 (47%)]\tLoss: 0.412589\t Accuracy: 256/711 (36%)\n",
      "Train Epoch: 20 [496/711 (70%)]\tLoss: 0.784756\t Accuracy: 371/711 (52%)\n",
      "Train Epoch: 20 [656/711 (92%)]\tLoss: 0.418981\t Accuracy: 490/711 (69%)\n",
      "Train Epoch: 21 [16/711 (2%)]\tLoss: 0.518805\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 21 [176/711 (25%)]\tLoss: 0.324892\t Accuracy: 137/711 (19%)\n",
      "Train Epoch: 21 [336/711 (47%)]\tLoss: 0.886275\t Accuracy: 263/711 (37%)\n",
      "Train Epoch: 21 [496/711 (70%)]\tLoss: 0.414652\t Accuracy: 376/711 (53%)\n",
      "Train Epoch: 21 [656/711 (92%)]\tLoss: 0.449544\t Accuracy: 489/711 (69%)\n",
      "Train Epoch: 22 [16/711 (2%)]\tLoss: 0.832280\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 22 [176/711 (25%)]\tLoss: 0.534317\t Accuracy: 131/711 (18%)\n",
      "Train Epoch: 22 [336/711 (47%)]\tLoss: 0.462769\t Accuracy: 252/711 (35%)\n",
      "Train Epoch: 22 [496/711 (70%)]\tLoss: 0.476660\t Accuracy: 374/711 (53%)\n",
      "Train Epoch: 22 [656/711 (92%)]\tLoss: 0.410933\t Accuracy: 496/711 (70%)\n",
      "Train Epoch: 23 [16/711 (2%)]\tLoss: 0.471388\t Accuracy: 13/711 (2%)\n",
      "Train Epoch: 23 [176/711 (25%)]\tLoss: 0.264922\t Accuracy: 138/711 (19%)\n",
      "Train Epoch: 23 [336/711 (47%)]\tLoss: 0.649104\t Accuracy: 253/711 (36%)\n",
      "Train Epoch: 23 [496/711 (70%)]\tLoss: 0.677401\t Accuracy: 374/711 (53%)\n",
      "Train Epoch: 23 [656/711 (92%)]\tLoss: 0.668416\t Accuracy: 489/711 (69%)\n",
      "Train Epoch: 24 [16/711 (2%)]\tLoss: 0.570908\t Accuracy: 12/711 (2%)\n",
      "Train Epoch: 24 [176/711 (25%)]\tLoss: 0.483326\t Accuracy: 125/711 (18%)\n",
      "Train Epoch: 24 [336/711 (47%)]\tLoss: 0.747757\t Accuracy: 242/711 (34%)\n",
      "Train Epoch: 24 [496/711 (70%)]\tLoss: 0.431326\t Accuracy: 359/711 (50%)\n",
      "Train Epoch: 24 [656/711 (92%)]\tLoss: 0.611789\t Accuracy: 466/711 (66%)\n",
      "Train Epoch: 25 [16/711 (2%)]\tLoss: 0.377797\t Accuracy: 14/711 (2%)\n",
      "Train Epoch: 25 [176/711 (25%)]\tLoss: 0.718121\t Accuracy: 118/711 (17%)\n",
      "Train Epoch: 25 [336/711 (47%)]\tLoss: 0.369493\t Accuracy: 242/711 (34%)\n",
      "Train Epoch: 25 [496/711 (70%)]\tLoss: 0.596823\t Accuracy: 365/711 (51%)\n",
      "Train Epoch: 25 [656/711 (92%)]\tLoss: 0.334989\t Accuracy: 483/711 (68%)\n",
      "Train Epoch: 26 [16/711 (2%)]\tLoss: 0.658968\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 26 [176/711 (25%)]\tLoss: 0.673140\t Accuracy: 125/711 (18%)\n",
      "Train Epoch: 26 [336/711 (47%)]\tLoss: 0.440130\t Accuracy: 250/711 (35%)\n",
      "Train Epoch: 26 [496/711 (70%)]\tLoss: 0.617783\t Accuracy: 368/711 (52%)\n",
      "Train Epoch: 26 [656/711 (92%)]\tLoss: 0.562947\t Accuracy: 484/711 (68%)\n",
      "Train Epoch: 27 [16/711 (2%)]\tLoss: 0.595105\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 27 [176/711 (25%)]\tLoss: 0.690159\t Accuracy: 122/711 (17%)\n",
      "Train Epoch: 27 [336/711 (47%)]\tLoss: 0.742436\t Accuracy: 252/711 (35%)\n",
      "Train Epoch: 27 [496/711 (70%)]\tLoss: 0.563968\t Accuracy: 369/711 (52%)\n",
      "Train Epoch: 27 [656/711 (92%)]\tLoss: 0.558512\t Accuracy: 485/711 (68%)\n",
      "Train Epoch: 28 [16/711 (2%)]\tLoss: 0.633638\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 28 [176/711 (25%)]\tLoss: 0.372712\t Accuracy: 134/711 (19%)\n",
      "Train Epoch: 28 [336/711 (47%)]\tLoss: 0.454692\t Accuracy: 256/711 (36%)\n",
      "Train Epoch: 28 [496/711 (70%)]\tLoss: 0.622648\t Accuracy: 372/711 (52%)\n",
      "Train Epoch: 28 [656/711 (92%)]\tLoss: 0.503719\t Accuracy: 488/711 (69%)\n",
      "Train Epoch: 29 [16/711 (2%)]\tLoss: 0.711629\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 29 [176/711 (25%)]\tLoss: 0.472348\t Accuracy: 133/711 (19%)\n",
      "Train Epoch: 29 [336/711 (47%)]\tLoss: 0.444560\t Accuracy: 255/711 (36%)\n",
      "Train Epoch: 29 [496/711 (70%)]\tLoss: 0.404468\t Accuracy: 379/711 (53%)\n",
      "Train Epoch: 29 [656/711 (92%)]\tLoss: 0.550306\t Accuracy: 499/711 (70%)\n",
      "Train Epoch: 30 [16/711 (2%)]\tLoss: 0.395148\t Accuracy: 14/711 (2%)\n",
      "Train Epoch: 30 [176/711 (25%)]\tLoss: 0.556640\t Accuracy: 123/711 (17%)\n",
      "Train Epoch: 30 [336/711 (47%)]\tLoss: 0.599166\t Accuracy: 241/711 (34%)\n",
      "Train Epoch: 30 [496/711 (70%)]\tLoss: 0.634506\t Accuracy: 365/711 (51%)\n",
      "Train Epoch: 30 [656/711 (92%)]\tLoss: 0.549877\t Accuracy: 487/711 (68%)\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_accuracy = train(epoch)\n",
    "    test_accuracy = test()\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(train_accuracies), color='blue', label='Training Accuracy')\n",
    "plt.plot(np.array(test_accuracies), color='red', label='Test Accuracy')\n",
    "# plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
