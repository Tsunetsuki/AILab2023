{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x17cb2fe27a0>"
      ]
     },
     "execution_count": 1105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "1. Write a custom dataset class for the titanic data (see the data folder on GitHub).\n",
    "2. Use only the features: \"Pclass\", \"Age\", \"SibSp\", \"Parch\", „Fare“, „Sex“, „Embarked“.\n",
    "3. Preprocess the features accordingly in that class (scaling, one-hot-encoding, etc) and\n",
    "4. split the data into train and validation data (80% and 20%). The constructor of that class\n",
    "should look like this:\n",
    "```\n",
    "titanic_train = TitanicDataSet('titanic.csv', train=True)\n",
    "titanic_val = TitanicDataSet('titanic.csv', train=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TitanicDataSet(root_dir, train):\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    onehot_enc = OneHotEncoder()\n",
    "\n",
    "    titanic = pd.read_csv(root_dir)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(69)\n",
    "    train_indices, test_indices = [ds.indices for ds in torch.utils.data.random_split(titanic, [0.8, 0.2], generator=generator)]\n",
    "    \n",
    "    if train:\n",
    "      titanic = titanic.iloc[train_indices]\n",
    "    else:\n",
    "      titanic = titanic.iloc[test_indices]\n",
    "\n",
    "    # only need \"Pclass\", \"Age\", \"SibSp\", \"Parch\", „Fare“, „Sex“, „Embarked“\n",
    "    titanic = titanic[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\", \"Survived\"]]\n",
    "\n",
    "    # because i found NaN in 'Age' column, so filled it with mean value\n",
    "    # remove the NaN data in the dataset, which is from Embarked column, two rows\n",
    "    mean_values = titanic[titanic.select_dtypes(exclude=['object']).columns].mean()\n",
    "    titanic = titanic.fillna(mean_values)\n",
    "    titanic = titanic.dropna()\n",
    "    titanic = titanic.reset_index(drop=True) # reset the index, or combine_features will cause wrong index and length\n",
    "\n",
    "    # devide the data into categorical features and numerical features, and put the 'Survived' column into categorical features\n",
    "    categorical_features = titanic[titanic.select_dtypes(include=['object']).columns.tolist()]\n",
    "    numerical_features = titanic[titanic.select_dtypes(exclude=['object']).columns].drop('Survived', axis=1)\n",
    "    label_features = titanic['Survived']\n",
    "\n",
    "    # use one-hot encoding to transform categorical features to numerical features\n",
    "    numerical_features_arr = minmax_scaler.fit_transform(numerical_features)\n",
    "    categorical_features_arr = onehot_enc.fit_transform(categorical_features).toarray()\n",
    "\n",
    "    # combine the numerical features and categorical features\n",
    "    combined_features = pd.DataFrame(data=numerical_features_arr, columns=numerical_features.columns)\n",
    "    combined_features = pd.concat([combined_features, pd.DataFrame(data=categorical_features_arr)], axis=1)\n",
    "    combined_features = pd.concat([combined_features, label_features], axis=1).reset_index(drop=True)\n",
    "\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset len: 711\n",
      "val_dataset len: 178\n",
      "total_dataset len: 889\n"
     ]
    }
   ],
   "source": [
    "titanic_train = TitanicDataSet('./data/titanic.csv', train=True)\n",
    "titanic_val = TitanicDataSet('./data/titanic.csv', train=False)\n",
    "print('train_dataset len:', len(titanic_train))\n",
    "print('val_dataset len:', len(titanic_val))\n",
    "print('total_dataset len:', len(titanic_train) + len(titanic_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.296306</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.032596</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258608</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.421965</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.028107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.296306</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.036598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.044986</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.054457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.366566</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384267</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.371701</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>711 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Age  SibSp     Parch      Fare    0    1    2    3    4  \\\n",
       "0       1.0  0.296306  0.000  0.333333  0.032596  1.0  0.0  0.0  0.0  1.0   \n",
       "1       1.0  0.258608  0.000  0.000000  0.016908  0.0  1.0  0.0  0.0  1.0   \n",
       "2       1.0  0.421965  0.125  0.166667  0.028107  0.0  1.0  0.0  0.0  1.0   \n",
       "3       1.0  0.271174  0.000  0.000000  0.014680  0.0  1.0  0.0  0.0  1.0   \n",
       "4       0.5  0.296306  0.250  0.500000  0.036598  1.0  0.0  0.0  0.0  1.0   \n",
       "..      ...       ...    ...       ...       ...  ...  ...  ...  ...  ...   \n",
       "706     1.0  0.044986  0.375  0.333333  0.054457  0.0  1.0  0.0  0.0  1.0   \n",
       "707     1.0  0.366566  0.000  0.000000  0.015412  0.0  1.0  0.0  0.0  1.0   \n",
       "708     0.0  0.384267  0.125  0.000000  0.101497  0.0  1.0  0.0  0.0  1.0   \n",
       "709     0.0  0.723549  0.000  0.000000  0.285990  1.0  0.0  1.0  0.0  0.0   \n",
       "710     1.0  0.371701  0.125  0.000000  0.031425  0.0  1.0  0.0  0.0  1.0   \n",
       "\n",
       "     Survived  \n",
       "0           1  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           1  \n",
       "..        ...  \n",
       "706         0  \n",
       "707         0  \n",
       "708         0  \n",
       "709         1  \n",
       "710         0  \n",
       "\n",
       "[711 rows x 11 columns]"
      ]
     },
     "execution_count": 1108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "Build a neural network with: \n",
    "1. [v] one hidden layer of size 3 that predicts the survival of the\n",
    "passengers. \n",
    "2. [v] Use a BCE loss (Hint: you need a sigmoid activation in the output layer).\n",
    "3. [v] Use a data loader to train in batches of size 16 and shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        features = torch.FloatTensor(sample[:-1])  # Exclude the 'Survived' column\n",
    "        label = torch.FloatTensor([sample['Survived']])  # 'Survived' column as label\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # the weight and bias of linear1 will be initialized \n",
    "        # you can access them by self.linear1.weight and self.linear1.bias\n",
    "        self.linear1 = nn.Linear(D_in, H) # this will create weight, bias for linear1\n",
    "        self.linear2 = nn.Linear(H, D_out) # this will create weight, bias for linear2\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        y_pred = self.sigmoid(self.linear2(h_relu))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManyLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, num_hidden_layers):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(ManyLayerNet, self).__init__()\n",
    "        # the weight and bias of linear1 will be initialized \n",
    "        # you can access them by self.linear1.weight and self.linear1.bias\n",
    "        # slightly more irregular test results with dropout set to 0.0\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        self.first = nn.Linear(D_in, H) # this will create weight, bias for linear1       \n",
    "        # self.hidden_layers = [nn.Linear(H, H) for _ in range(num_hidden_layers)]\n",
    "        self.hidden_layers=[]\n",
    "        # alternating Linear and ReLU layers, adding two layers per loop:\n",
    "        for _ in range(num_hidden_layers/2):\n",
    "            self.hidden_layers.append(nn.Linear(H, H))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "        self.last = nn.Linear(H, D_out)\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        Dropout is applied before making the prediction in order to ignore some Neurons\n",
    "        \"\"\"\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.first(x))\n",
    "        # can't be applied to ReLU layers which are now part of the hidden layers:\n",
    "        # for hidden_layer in self.hidden_layers:\n",
    "        #     x = F.relu(hidden_layer(x))\n",
    "        y_pred = self.sigmoid(self.last(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; \n",
    "# D_in is input dimension; \t10 features from Pclass/Age/SibSp/Parch/Fare/Sex[0\t1]/Embarked[2\t3\t4]\n",
    "# H is hidden dimension (Only one hidden layer, but containing 3 neurons.); \n",
    "# D_out is output dimension: 1 or 0 (Survived or not) 1 dimension for binary classification\n",
    "\n",
    "\n",
    "\n",
    "# using a single batch increases both training and test accuracies\n",
    "# this might be a case of the network being too confident\n",
    "N, D_in, H, D_out = 711, 10, 50, 1\n",
    "lr = 0.1\n",
    "num_hidden_layers = 80\n",
    "\n",
    "network = ManyLayerNet(D_in, H, D_out, num_hidden_layers)  # H=3 for one hidden layer with 3 neurons\n",
    "optimizer = optim.Adam(network.parameters(), lr)  # RMSProp + Momentum \n",
    "criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss\n",
    "\n",
    "n_epochs = 300 # You can adjust the number of epochs as needed\n",
    "log_interval = 10 # Print the training status every log_interval epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(titanic_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=N, shuffle=True)\n",
    "test_dataset = CustomDataset(titanic_val)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=N, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([711, 1])"
      ]
     },
     "execution_count": 1114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataloader))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    train_losses = [] # Save the loss value of each training loop (epoch) of the neural network model during the training process\n",
    "    train_accuracies = []\n",
    "    network.train()\n",
    "    correct = 0\n",
    "    cur_count = 0 \n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "                \n",
    "        # Accuracy\n",
    "        pred = (output >= 0.5).float()  # survival_rate is the threshold\n",
    "        correct += (pred == target).sum().item()\n",
    "        cur_count += len(data)\n",
    "\n",
    "        # backword propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                epoch, \n",
    "                cur_count, \n",
    "                len(train_dataloader.dataset),\n",
    "                100. * cur_count / len(train_dataloader.dataset), \n",
    "                loss.item(), \n",
    "                correct, len(train_dataloader.dataset),\n",
    "                100. * correct / len(train_dataloader.dataset))\n",
    "            )\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # train_counter.append((batch_idx*16) + ((epoch-1)*len(train_dataloader.dataset)))\n",
    "    return correct / len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # test_counter = [i*len(titanic_train) for i in range(n_epochs+1)] # how many data for training so far\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # forward propagation\n",
    "            output = network(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            # Accuracy\n",
    "            pred = (output >= 0.5).float()  # 0.5 is the threshold\n",
    "            correct += (pred == target).sum().item()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_accuracy = correct / len(test_dataloader.dataset)\n",
    "\n",
    "    return test_accuracy\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, \n",
    "        correct, \n",
    "        len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [711/711 (100%)]\tLoss: 0.751237\t Accuracy: 276/711 (39%)\n",
      "Train Epoch: 2 [711/711 (100%)]\tLoss: 0.599375\t Accuracy: 471/711 (66%)\n",
      "Train Epoch: 3 [711/711 (100%)]\tLoss: 0.541471\t Accuracy: 525/711 (74%)\n",
      "Train Epoch: 4 [711/711 (100%)]\tLoss: 0.501272\t Accuracy: 550/711 (77%)\n",
      "Train Epoch: 5 [711/711 (100%)]\tLoss: 0.506263\t Accuracy: 547/711 (77%)\n",
      "Train Epoch: 6 [711/711 (100%)]\tLoss: 0.502337\t Accuracy: 551/711 (77%)\n",
      "Train Epoch: 7 [711/711 (100%)]\tLoss: 0.498489\t Accuracy: 555/711 (78%)\n",
      "Train Epoch: 8 [711/711 (100%)]\tLoss: 0.476345\t Accuracy: 553/711 (78%)\n",
      "Train Epoch: 9 [711/711 (100%)]\tLoss: 0.463555\t Accuracy: 554/711 (78%)\n",
      "Train Epoch: 10 [711/711 (100%)]\tLoss: 0.465990\t Accuracy: 559/711 (79%)\n",
      "Train Epoch: 11 [711/711 (100%)]\tLoss: 0.461179\t Accuracy: 568/711 (80%)\n",
      "Train Epoch: 12 [711/711 (100%)]\tLoss: 0.460051\t Accuracy: 573/711 (81%)\n",
      "Train Epoch: 13 [711/711 (100%)]\tLoss: 0.457678\t Accuracy: 572/711 (80%)\n",
      "Train Epoch: 14 [711/711 (100%)]\tLoss: 0.451090\t Accuracy: 572/711 (80%)\n",
      "Train Epoch: 15 [711/711 (100%)]\tLoss: 0.451563\t Accuracy: 569/711 (80%)\n",
      "Train Epoch: 16 [711/711 (100%)]\tLoss: 0.451168\t Accuracy: 568/711 (80%)\n",
      "Train Epoch: 17 [711/711 (100%)]\tLoss: 0.446620\t Accuracy: 573/711 (81%)\n",
      "Train Epoch: 18 [711/711 (100%)]\tLoss: 0.444388\t Accuracy: 578/711 (81%)\n",
      "Train Epoch: 19 [711/711 (100%)]\tLoss: 0.439803\t Accuracy: 577/711 (81%)\n",
      "Train Epoch: 20 [711/711 (100%)]\tLoss: 0.437704\t Accuracy: 571/711 (80%)\n",
      "Train Epoch: 21 [711/711 (100%)]\tLoss: 0.437012\t Accuracy: 574/711 (81%)\n",
      "Train Epoch: 22 [711/711 (100%)]\tLoss: 0.433155\t Accuracy: 575/711 (81%)\n",
      "Train Epoch: 23 [711/711 (100%)]\tLoss: 0.431901\t Accuracy: 576/711 (81%)\n",
      "Train Epoch: 24 [711/711 (100%)]\tLoss: 0.429209\t Accuracy: 580/711 (82%)\n",
      "Train Epoch: 25 [711/711 (100%)]\tLoss: 0.429473\t Accuracy: 578/711 (81%)\n",
      "Train Epoch: 26 [711/711 (100%)]\tLoss: 0.427624\t Accuracy: 577/711 (81%)\n",
      "Train Epoch: 27 [711/711 (100%)]\tLoss: 0.425309\t Accuracy: 582/711 (82%)\n",
      "Train Epoch: 28 [711/711 (100%)]\tLoss: 0.424271\t Accuracy: 580/711 (82%)\n",
      "Train Epoch: 29 [711/711 (100%)]\tLoss: 0.423448\t Accuracy: 582/711 (82%)\n",
      "Train Epoch: 30 [711/711 (100%)]\tLoss: 0.423659\t Accuracy: 588/711 (83%)\n",
      "Train Epoch: 31 [711/711 (100%)]\tLoss: 0.422074\t Accuracy: 588/711 (83%)\n",
      "Train Epoch: 32 [711/711 (100%)]\tLoss: 0.421912\t Accuracy: 587/711 (83%)\n",
      "Train Epoch: 33 [711/711 (100%)]\tLoss: 0.421382\t Accuracy: 589/711 (83%)\n",
      "Train Epoch: 34 [711/711 (100%)]\tLoss: 0.420188\t Accuracy: 590/711 (83%)\n",
      "Train Epoch: 35 [711/711 (100%)]\tLoss: 0.419557\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 36 [711/711 (100%)]\tLoss: 0.419292\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 37 [711/711 (100%)]\tLoss: 0.419268\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 38 [711/711 (100%)]\tLoss: 0.418074\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 39 [711/711 (100%)]\tLoss: 0.417900\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 40 [711/711 (100%)]\tLoss: 0.417399\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 41 [711/711 (100%)]\tLoss: 0.416995\t Accuracy: 592/711 (83%)\n",
      "Train Epoch: 42 [711/711 (100%)]\tLoss: 0.416209\t Accuracy: 593/711 (83%)\n",
      "Train Epoch: 43 [711/711 (100%)]\tLoss: 0.416300\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 44 [711/711 (100%)]\tLoss: 0.415530\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 45 [711/711 (100%)]\tLoss: 0.415216\t Accuracy: 590/711 (83%)\n",
      "Train Epoch: 46 [711/711 (100%)]\tLoss: 0.414756\t Accuracy: 593/711 (83%)\n",
      "Train Epoch: 47 [711/711 (100%)]\tLoss: 0.414527\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 48 [711/711 (100%)]\tLoss: 0.413891\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 49 [711/711 (100%)]\tLoss: 0.413697\t Accuracy: 590/711 (83%)\n",
      "Train Epoch: 50 [711/711 (100%)]\tLoss: 0.413210\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 51 [711/711 (100%)]\tLoss: 0.412805\t Accuracy: 593/711 (83%)\n",
      "Train Epoch: 52 [711/711 (100%)]\tLoss: 0.412525\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 53 [711/711 (100%)]\tLoss: 0.412314\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 54 [711/711 (100%)]\tLoss: 0.411937\t Accuracy: 593/711 (83%)\n",
      "Train Epoch: 55 [711/711 (100%)]\tLoss: 0.411584\t Accuracy: 590/711 (83%)\n",
      "Train Epoch: 56 [711/711 (100%)]\tLoss: 0.411414\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 57 [711/711 (100%)]\tLoss: 0.410919\t Accuracy: 589/711 (83%)\n",
      "Train Epoch: 58 [711/711 (100%)]\tLoss: 0.410693\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 59 [711/711 (100%)]\tLoss: 0.410395\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 60 [711/711 (100%)]\tLoss: 0.409989\t Accuracy: 592/711 (83%)\n",
      "Train Epoch: 61 [711/711 (100%)]\tLoss: 0.409728\t Accuracy: 593/711 (83%)\n",
      "Train Epoch: 62 [711/711 (100%)]\tLoss: 0.409377\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 63 [711/711 (100%)]\tLoss: 0.409031\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 64 [711/711 (100%)]\tLoss: 0.408803\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 65 [711/711 (100%)]\tLoss: 0.408435\t Accuracy: 590/711 (83%)\n",
      "Train Epoch: 66 [711/711 (100%)]\tLoss: 0.408041\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 67 [711/711 (100%)]\tLoss: 0.407719\t Accuracy: 592/711 (83%)\n",
      "Train Epoch: 68 [711/711 (100%)]\tLoss: 0.407339\t Accuracy: 592/711 (83%)\n",
      "Train Epoch: 69 [711/711 (100%)]\tLoss: 0.407017\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 70 [711/711 (100%)]\tLoss: 0.406592\t Accuracy: 590/711 (83%)\n",
      "Train Epoch: 71 [711/711 (100%)]\tLoss: 0.406207\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 72 [711/711 (100%)]\tLoss: 0.405747\t Accuracy: 592/711 (83%)\n",
      "Train Epoch: 73 [711/711 (100%)]\tLoss: 0.405299\t Accuracy: 590/711 (83%)\n",
      "Train Epoch: 74 [711/711 (100%)]\tLoss: 0.404935\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 75 [711/711 (100%)]\tLoss: 0.404590\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 76 [711/711 (100%)]\tLoss: 0.404174\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 77 [711/711 (100%)]\tLoss: 0.403788\t Accuracy: 592/711 (83%)\n",
      "Train Epoch: 78 [711/711 (100%)]\tLoss: 0.403411\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 79 [711/711 (100%)]\tLoss: 0.402965\t Accuracy: 592/711 (83%)\n",
      "Train Epoch: 80 [711/711 (100%)]\tLoss: 0.402514\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 81 [711/711 (100%)]\tLoss: 0.402081\t Accuracy: 593/711 (83%)\n",
      "Train Epoch: 82 [711/711 (100%)]\tLoss: 0.401750\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 83 [711/711 (100%)]\tLoss: 0.401458\t Accuracy: 590/711 (83%)\n",
      "Train Epoch: 84 [711/711 (100%)]\tLoss: 0.401524\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 85 [711/711 (100%)]\tLoss: 0.402198\t Accuracy: 589/711 (83%)\n",
      "Train Epoch: 86 [711/711 (100%)]\tLoss: 0.402887\t Accuracy: 598/711 (84%)\n",
      "Train Epoch: 87 [711/711 (100%)]\tLoss: 0.402526\t Accuracy: 586/711 (82%)\n",
      "Train Epoch: 88 [711/711 (100%)]\tLoss: 0.399708\t Accuracy: 597/711 (84%)\n",
      "Train Epoch: 89 [711/711 (100%)]\tLoss: 0.399145\t Accuracy: 598/711 (84%)\n",
      "Train Epoch: 90 [711/711 (100%)]\tLoss: 0.400351\t Accuracy: 589/711 (83%)\n",
      "Train Epoch: 91 [711/711 (100%)]\tLoss: 0.398931\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 92 [711/711 (100%)]\tLoss: 0.397365\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 93 [711/711 (100%)]\tLoss: 0.397641\t Accuracy: 593/711 (83%)\n",
      "Train Epoch: 94 [711/711 (100%)]\tLoss: 0.397932\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 95 [711/711 (100%)]\tLoss: 0.396918\t Accuracy: 590/711 (83%)\n",
      "Train Epoch: 96 [711/711 (100%)]\tLoss: 0.395688\t Accuracy: 597/711 (84%)\n",
      "Train Epoch: 97 [711/711 (100%)]\tLoss: 0.395712\t Accuracy: 599/711 (84%)\n",
      "Train Epoch: 98 [711/711 (100%)]\tLoss: 0.396008\t Accuracy: 589/711 (83%)\n",
      "Train Epoch: 99 [711/711 (100%)]\tLoss: 0.395031\t Accuracy: 599/711 (84%)\n",
      "Train Epoch: 100 [711/711 (100%)]\tLoss: 0.394038\t Accuracy: 592/711 (83%)\n",
      "Train Epoch: 101 [711/711 (100%)]\tLoss: 0.393423\t Accuracy: 593/711 (83%)\n",
      "Train Epoch: 102 [711/711 (100%)]\tLoss: 0.393320\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 103 [711/711 (100%)]\tLoss: 0.393438\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 104 [711/711 (100%)]\tLoss: 0.393364\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 105 [711/711 (100%)]\tLoss: 0.392743\t Accuracy: 589/711 (83%)\n",
      "Train Epoch: 106 [711/711 (100%)]\tLoss: 0.391469\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 107 [711/711 (100%)]\tLoss: 0.390361\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 108 [711/711 (100%)]\tLoss: 0.390135\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 109 [711/711 (100%)]\tLoss: 0.390094\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 110 [711/711 (100%)]\tLoss: 0.390410\t Accuracy: 589/711 (83%)\n",
      "Train Epoch: 111 [711/711 (100%)]\tLoss: 0.389811\t Accuracy: 598/711 (84%)\n",
      "Train Epoch: 112 [711/711 (100%)]\tLoss: 0.389276\t Accuracy: 591/711 (83%)\n",
      "Train Epoch: 113 [711/711 (100%)]\tLoss: 0.387972\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 114 [711/711 (100%)]\tLoss: 0.386769\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 115 [711/711 (100%)]\tLoss: 0.385874\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 116 [711/711 (100%)]\tLoss: 0.384952\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 117 [711/711 (100%)]\tLoss: 0.384392\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 118 [711/711 (100%)]\tLoss: 0.383503\t Accuracy: 597/711 (84%)\n",
      "Train Epoch: 119 [711/711 (100%)]\tLoss: 0.382778\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 120 [711/711 (100%)]\tLoss: 0.381773\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 121 [711/711 (100%)]\tLoss: 0.381618\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 122 [711/711 (100%)]\tLoss: 0.385331\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 123 [711/711 (100%)]\tLoss: 0.402544\t Accuracy: 587/711 (83%)\n",
      "Train Epoch: 124 [711/711 (100%)]\tLoss: 0.405845\t Accuracy: 593/711 (83%)\n",
      "Train Epoch: 125 [711/711 (100%)]\tLoss: 0.386147\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 126 [711/711 (100%)]\tLoss: 0.383105\t Accuracy: 592/711 (83%)\n",
      "Train Epoch: 127 [711/711 (100%)]\tLoss: 0.393346\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 128 [711/711 (100%)]\tLoss: 0.375906\t Accuracy: 593/711 (83%)\n",
      "Train Epoch: 129 [711/711 (100%)]\tLoss: 0.383397\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 130 [711/711 (100%)]\tLoss: 0.380660\t Accuracy: 595/711 (84%)\n",
      "Train Epoch: 131 [711/711 (100%)]\tLoss: 0.373485\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 132 [711/711 (100%)]\tLoss: 0.377442\t Accuracy: 594/711 (84%)\n",
      "Train Epoch: 133 [711/711 (100%)]\tLoss: 0.371470\t Accuracy: 597/711 (84%)\n",
      "Train Epoch: 134 [711/711 (100%)]\tLoss: 0.374264\t Accuracy: 599/711 (84%)\n",
      "Train Epoch: 135 [711/711 (100%)]\tLoss: 0.370823\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 136 [711/711 (100%)]\tLoss: 0.368402\t Accuracy: 600/711 (84%)\n",
      "Train Epoch: 137 [711/711 (100%)]\tLoss: 0.371753\t Accuracy: 601/711 (85%)\n",
      "Train Epoch: 138 [711/711 (100%)]\tLoss: 0.366389\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 139 [711/711 (100%)]\tLoss: 0.366697\t Accuracy: 596/711 (84%)\n",
      "Train Epoch: 140 [711/711 (100%)]\tLoss: 0.366736\t Accuracy: 599/711 (84%)\n",
      "Train Epoch: 141 [711/711 (100%)]\tLoss: 0.363861\t Accuracy: 597/711 (84%)\n",
      "Train Epoch: 142 [711/711 (100%)]\tLoss: 0.365334\t Accuracy: 601/711 (85%)\n",
      "Train Epoch: 143 [711/711 (100%)]\tLoss: 0.362839\t Accuracy: 600/711 (84%)\n",
      "Train Epoch: 144 [711/711 (100%)]\tLoss: 0.362164\t Accuracy: 601/711 (85%)\n",
      "Train Epoch: 145 [711/711 (100%)]\tLoss: 0.363108\t Accuracy: 600/711 (84%)\n",
      "Train Epoch: 146 [711/711 (100%)]\tLoss: 0.360461\t Accuracy: 599/711 (84%)\n",
      "Train Epoch: 147 [711/711 (100%)]\tLoss: 0.360329\t Accuracy: 601/711 (85%)\n",
      "Train Epoch: 148 [711/711 (100%)]\tLoss: 0.360481\t Accuracy: 598/711 (84%)\n",
      "Train Epoch: 149 [711/711 (100%)]\tLoss: 0.358804\t Accuracy: 602/711 (85%)\n",
      "Train Epoch: 150 [711/711 (100%)]\tLoss: 0.358480\t Accuracy: 600/711 (84%)\n",
      "Train Epoch: 151 [711/711 (100%)]\tLoss: 0.358270\t Accuracy: 601/711 (85%)\n",
      "Train Epoch: 152 [711/711 (100%)]\tLoss: 0.357302\t Accuracy: 600/711 (84%)\n",
      "Train Epoch: 153 [711/711 (100%)]\tLoss: 0.357314\t Accuracy: 599/711 (84%)\n",
      "Train Epoch: 154 [711/711 (100%)]\tLoss: 0.356934\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 155 [711/711 (100%)]\tLoss: 0.356035\t Accuracy: 601/711 (85%)\n",
      "Train Epoch: 156 [711/711 (100%)]\tLoss: 0.355815\t Accuracy: 602/711 (85%)\n",
      "Train Epoch: 157 [711/711 (100%)]\tLoss: 0.355657\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 158 [711/711 (100%)]\tLoss: 0.355055\t Accuracy: 598/711 (84%)\n",
      "Train Epoch: 159 [711/711 (100%)]\tLoss: 0.354303\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 160 [711/711 (100%)]\tLoss: 0.354200\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 161 [711/711 (100%)]\tLoss: 0.354144\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 162 [711/711 (100%)]\tLoss: 0.353463\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 163 [711/711 (100%)]\tLoss: 0.352918\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 164 [711/711 (100%)]\tLoss: 0.352718\t Accuracy: 601/711 (85%)\n",
      "Train Epoch: 165 [711/711 (100%)]\tLoss: 0.352555\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 166 [711/711 (100%)]\tLoss: 0.352300\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 167 [711/711 (100%)]\tLoss: 0.351834\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 168 [711/711 (100%)]\tLoss: 0.351375\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 169 [711/711 (100%)]\tLoss: 0.351156\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 170 [711/711 (100%)]\tLoss: 0.350977\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 171 [711/711 (100%)]\tLoss: 0.350629\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 172 [711/711 (100%)]\tLoss: 0.350288\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 173 [711/711 (100%)]\tLoss: 0.349956\t Accuracy: 602/711 (85%)\n",
      "Train Epoch: 174 [711/711 (100%)]\tLoss: 0.350258\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 175 [711/711 (100%)]\tLoss: 0.350514\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 176 [711/711 (100%)]\tLoss: 0.352103\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 177 [711/711 (100%)]\tLoss: 0.355657\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 178 [711/711 (100%)]\tLoss: 0.359405\t Accuracy: 606/711 (85%)\n",
      "Train Epoch: 179 [711/711 (100%)]\tLoss: 0.359745\t Accuracy: 602/711 (85%)\n",
      "Train Epoch: 180 [711/711 (100%)]\tLoss: 0.352319\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 181 [711/711 (100%)]\tLoss: 0.348544\t Accuracy: 602/711 (85%)\n",
      "Train Epoch: 182 [711/711 (100%)]\tLoss: 0.351523\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 183 [711/711 (100%)]\tLoss: 0.354482\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 184 [711/711 (100%)]\tLoss: 0.353648\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 185 [711/711 (100%)]\tLoss: 0.348348\t Accuracy: 608/711 (86%)\n",
      "Train Epoch: 186 [711/711 (100%)]\tLoss: 0.347972\t Accuracy: 599/711 (84%)\n",
      "Train Epoch: 187 [711/711 (100%)]\tLoss: 0.351171\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 188 [711/711 (100%)]\tLoss: 0.350448\t Accuracy: 606/711 (85%)\n",
      "Train Epoch: 189 [711/711 (100%)]\tLoss: 0.347952\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 190 [711/711 (100%)]\tLoss: 0.346402\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 191 [711/711 (100%)]\tLoss: 0.347581\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 192 [711/711 (100%)]\tLoss: 0.349840\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 193 [711/711 (100%)]\tLoss: 0.348356\t Accuracy: 606/711 (85%)\n",
      "Train Epoch: 194 [711/711 (100%)]\tLoss: 0.346489\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 195 [711/711 (100%)]\tLoss: 0.345490\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 196 [711/711 (100%)]\tLoss: 0.346282\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 197 [711/711 (100%)]\tLoss: 0.348048\t Accuracy: 606/711 (85%)\n",
      "Train Epoch: 198 [711/711 (100%)]\tLoss: 0.348058\t Accuracy: 608/711 (86%)\n",
      "Train Epoch: 199 [711/711 (100%)]\tLoss: 0.346918\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 200 [711/711 (100%)]\tLoss: 0.345187\t Accuracy: 607/711 (85%)\n",
      "Train Epoch: 201 [711/711 (100%)]\tLoss: 0.344002\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 202 [711/711 (100%)]\tLoss: 0.344549\t Accuracy: 602/711 (85%)\n",
      "Train Epoch: 203 [711/711 (100%)]\tLoss: 0.345299\t Accuracy: 606/711 (85%)\n",
      "Train Epoch: 204 [711/711 (100%)]\tLoss: 0.346171\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 205 [711/711 (100%)]\tLoss: 0.345224\t Accuracy: 606/711 (85%)\n",
      "Train Epoch: 206 [711/711 (100%)]\tLoss: 0.343623\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 207 [711/711 (100%)]\tLoss: 0.343067\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 208 [711/711 (100%)]\tLoss: 0.342674\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 209 [711/711 (100%)]\tLoss: 0.342751\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 210 [711/711 (100%)]\tLoss: 0.342964\t Accuracy: 608/711 (86%)\n",
      "Train Epoch: 211 [711/711 (100%)]\tLoss: 0.343607\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 212 [711/711 (100%)]\tLoss: 0.344152\t Accuracy: 607/711 (85%)\n",
      "Train Epoch: 213 [711/711 (100%)]\tLoss: 0.345019\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 214 [711/711 (100%)]\tLoss: 0.344747\t Accuracy: 609/711 (86%)\n",
      "Train Epoch: 215 [711/711 (100%)]\tLoss: 0.342986\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 216 [711/711 (100%)]\tLoss: 0.341299\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 217 [711/711 (100%)]\tLoss: 0.340071\t Accuracy: 606/711 (85%)\n",
      "Train Epoch: 218 [711/711 (100%)]\tLoss: 0.339650\t Accuracy: 606/711 (85%)\n",
      "Train Epoch: 219 [711/711 (100%)]\tLoss: 0.339969\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 220 [711/711 (100%)]\tLoss: 0.341071\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 221 [711/711 (100%)]\tLoss: 0.342044\t Accuracy: 612/711 (86%)\n",
      "Train Epoch: 222 [711/711 (100%)]\tLoss: 0.343801\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 223 [711/711 (100%)]\tLoss: 0.343589\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 224 [711/711 (100%)]\tLoss: 0.341651\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 225 [711/711 (100%)]\tLoss: 0.339186\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 226 [711/711 (100%)]\tLoss: 0.337673\t Accuracy: 602/711 (85%)\n",
      "Train Epoch: 227 [711/711 (100%)]\tLoss: 0.336601\t Accuracy: 609/711 (86%)\n",
      "Train Epoch: 228 [711/711 (100%)]\tLoss: 0.336102\t Accuracy: 608/711 (86%)\n",
      "Train Epoch: 229 [711/711 (100%)]\tLoss: 0.336411\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 230 [711/711 (100%)]\tLoss: 0.337315\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 231 [711/711 (100%)]\tLoss: 0.340361\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 232 [711/711 (100%)]\tLoss: 0.344933\t Accuracy: 609/711 (86%)\n",
      "Train Epoch: 233 [711/711 (100%)]\tLoss: 0.352872\t Accuracy: 600/711 (84%)\n",
      "Train Epoch: 234 [711/711 (100%)]\tLoss: 0.348700\t Accuracy: 607/711 (85%)\n",
      "Train Epoch: 235 [711/711 (100%)]\tLoss: 0.339646\t Accuracy: 606/711 (85%)\n",
      "Train Epoch: 236 [711/711 (100%)]\tLoss: 0.334702\t Accuracy: 612/711 (86%)\n",
      "Train Epoch: 237 [711/711 (100%)]\tLoss: 0.340225\t Accuracy: 615/711 (86%)\n",
      "Train Epoch: 238 [711/711 (100%)]\tLoss: 0.347211\t Accuracy: 602/711 (85%)\n",
      "Train Epoch: 239 [711/711 (100%)]\tLoss: 0.343746\t Accuracy: 609/711 (86%)\n",
      "Train Epoch: 240 [711/711 (100%)]\tLoss: 0.334680\t Accuracy: 606/711 (85%)\n",
      "Train Epoch: 241 [711/711 (100%)]\tLoss: 0.333051\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 242 [711/711 (100%)]\tLoss: 0.338483\t Accuracy: 614/711 (86%)\n",
      "Train Epoch: 243 [711/711 (100%)]\tLoss: 0.344927\t Accuracy: 602/711 (85%)\n",
      "Train Epoch: 244 [711/711 (100%)]\tLoss: 0.338325\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 245 [711/711 (100%)]\tLoss: 0.331196\t Accuracy: 609/711 (86%)\n",
      "Train Epoch: 246 [711/711 (100%)]\tLoss: 0.334859\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 247 [711/711 (100%)]\tLoss: 0.339561\t Accuracy: 616/711 (87%)\n",
      "Train Epoch: 248 [711/711 (100%)]\tLoss: 0.340409\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 249 [711/711 (100%)]\tLoss: 0.331813\t Accuracy: 612/711 (86%)\n",
      "Train Epoch: 250 [711/711 (100%)]\tLoss: 0.332555\t Accuracy: 616/711 (87%)\n",
      "Train Epoch: 251 [711/711 (100%)]\tLoss: 0.338072\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 252 [711/711 (100%)]\tLoss: 0.337706\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 253 [711/711 (100%)]\tLoss: 0.334926\t Accuracy: 604/711 (85%)\n",
      "Train Epoch: 254 [711/711 (100%)]\tLoss: 0.330958\t Accuracy: 612/711 (86%)\n",
      "Train Epoch: 255 [711/711 (100%)]\tLoss: 0.335503\t Accuracy: 609/711 (86%)\n",
      "Train Epoch: 256 [711/711 (100%)]\tLoss: 0.336734\t Accuracy: 603/711 (85%)\n",
      "Train Epoch: 257 [711/711 (100%)]\tLoss: 0.332692\t Accuracy: 618/711 (87%)\n",
      "Train Epoch: 258 [711/711 (100%)]\tLoss: 0.328556\t Accuracy: 616/711 (87%)\n",
      "Train Epoch: 259 [711/711 (100%)]\tLoss: 0.332864\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 260 [711/711 (100%)]\tLoss: 0.334207\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 261 [711/711 (100%)]\tLoss: 0.330810\t Accuracy: 605/711 (85%)\n",
      "Train Epoch: 262 [711/711 (100%)]\tLoss: 0.327215\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 263 [711/711 (100%)]\tLoss: 0.329515\t Accuracy: 612/711 (86%)\n",
      "Train Epoch: 264 [711/711 (100%)]\tLoss: 0.332968\t Accuracy: 614/711 (86%)\n",
      "Train Epoch: 265 [711/711 (100%)]\tLoss: 0.328560\t Accuracy: 617/711 (87%)\n",
      "Train Epoch: 266 [711/711 (100%)]\tLoss: 0.325737\t Accuracy: 610/711 (86%)\n",
      "Train Epoch: 267 [711/711 (100%)]\tLoss: 0.325437\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 268 [711/711 (100%)]\tLoss: 0.327715\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 269 [711/711 (100%)]\tLoss: 0.326747\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 270 [711/711 (100%)]\tLoss: 0.324038\t Accuracy: 616/711 (87%)\n",
      "Train Epoch: 271 [711/711 (100%)]\tLoss: 0.323518\t Accuracy: 615/711 (86%)\n",
      "Train Epoch: 272 [711/711 (100%)]\tLoss: 0.324217\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 273 [711/711 (100%)]\tLoss: 0.325040\t Accuracy: 614/711 (86%)\n",
      "Train Epoch: 274 [711/711 (100%)]\tLoss: 0.323717\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 275 [711/711 (100%)]\tLoss: 0.321874\t Accuracy: 614/711 (86%)\n",
      "Train Epoch: 276 [711/711 (100%)]\tLoss: 0.320693\t Accuracy: 614/711 (86%)\n",
      "Train Epoch: 277 [711/711 (100%)]\tLoss: 0.321130\t Accuracy: 614/711 (86%)\n",
      "Train Epoch: 278 [711/711 (100%)]\tLoss: 0.322148\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 279 [711/711 (100%)]\tLoss: 0.323188\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 280 [711/711 (100%)]\tLoss: 0.322632\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 281 [711/711 (100%)]\tLoss: 0.322070\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 282 [711/711 (100%)]\tLoss: 0.320396\t Accuracy: 612/711 (86%)\n",
      "Train Epoch: 283 [711/711 (100%)]\tLoss: 0.319243\t Accuracy: 614/711 (86%)\n",
      "Train Epoch: 284 [711/711 (100%)]\tLoss: 0.318303\t Accuracy: 609/711 (86%)\n",
      "Train Epoch: 285 [711/711 (100%)]\tLoss: 0.318602\t Accuracy: 615/711 (86%)\n",
      "Train Epoch: 286 [711/711 (100%)]\tLoss: 0.319606\t Accuracy: 614/711 (86%)\n",
      "Train Epoch: 287 [711/711 (100%)]\tLoss: 0.321247\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 288 [711/711 (100%)]\tLoss: 0.322640\t Accuracy: 613/711 (86%)\n",
      "Train Epoch: 289 [711/711 (100%)]\tLoss: 0.325359\t Accuracy: 615/711 (86%)\n",
      "Train Epoch: 290 [711/711 (100%)]\tLoss: 0.328228\t Accuracy: 610/711 (86%)\n",
      "Train Epoch: 291 [711/711 (100%)]\tLoss: 0.326547\t Accuracy: 610/711 (86%)\n",
      "Train Epoch: 292 [711/711 (100%)]\tLoss: 0.319679\t Accuracy: 615/711 (86%)\n",
      "Train Epoch: 293 [711/711 (100%)]\tLoss: 0.316507\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 294 [711/711 (100%)]\tLoss: 0.319075\t Accuracy: 614/711 (86%)\n",
      "Train Epoch: 295 [711/711 (100%)]\tLoss: 0.326514\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 296 [711/711 (100%)]\tLoss: 0.328982\t Accuracy: 608/711 (86%)\n",
      "Train Epoch: 297 [711/711 (100%)]\tLoss: 0.325968\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 298 [711/711 (100%)]\tLoss: 0.319253\t Accuracy: 612/711 (86%)\n",
      "Train Epoch: 299 [711/711 (100%)]\tLoss: 0.315278\t Accuracy: 611/711 (86%)\n",
      "Train Epoch: 300 [711/711 (100%)]\tLoss: 0.315882\t Accuracy: 611/711 (86%)\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_accuracy = train(epoch)\n",
    "    test_accuracy = test()\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 1118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(train_accuracies), color='blue', label='Training Accuracy')\n",
    "plt.plot(np.array(test_accuracies), color='red', label='Test Accuracy')\n",
    "# plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
