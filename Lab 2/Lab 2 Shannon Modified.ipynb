{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x17cb9df3dc0>"
      ]
     },
     "execution_count": 1572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "1. Write a custom dataset class for the titanic data (see the data folder on GitHub).\n",
    "2. Use only the features: \"Pclass\", \"Age\", \"SibSp\", \"Parch\", „Fare“, „Sex“, „Embarked“.\n",
    "3. Preprocess the features accordingly in that class (scaling, one-hot-encoding, etc) and\n",
    "4. split the data into train and validation data (80% and 20%). The constructor of that class\n",
    "should look like this:\n",
    "```\n",
    "titanic_train = TitanicDataSet('titanic.csv', train=True)\n",
    "titanic_val = TitanicDataSet('titanic.csv', train=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TitanicDataSet(root_dir, train):\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    onehot_enc = OneHotEncoder()\n",
    "\n",
    "    titanic = pd.read_csv(root_dir)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(69)\n",
    "    train_indices, test_indices = [ds.indices for ds in torch.utils.data.random_split(titanic, [0.8, 0.2], generator=generator)]\n",
    "    \n",
    "    if train:\n",
    "      titanic = titanic.iloc[train_indices]\n",
    "    else:\n",
    "      titanic = titanic.iloc[test_indices]\n",
    "\n",
    "    # only need \"Pclass\", \"Age\", \"SibSp\", \"Parch\", „Fare“, „Sex“, „Embarked“\n",
    "    titanic = titanic[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\", \"Survived\"]]\n",
    "\n",
    "    # because i found NaN in 'Age' column, so filled it with mean value\n",
    "    # remove the NaN data in the dataset, which is from Embarked column, two rows\n",
    "    mean_values = titanic[titanic.select_dtypes(exclude=['object']).columns].mean()\n",
    "    titanic = titanic.fillna(mean_values)\n",
    "    titanic = titanic.dropna()\n",
    "    titanic = titanic.reset_index(drop=True) # reset the index, or combine_features will cause wrong index and length\n",
    "\n",
    "    # devide the data into categorical features and numerical features, and put the 'Survived' column into categorical features\n",
    "    categorical_features = titanic[titanic.select_dtypes(include=['object']).columns.tolist()]\n",
    "    numerical_features = titanic[titanic.select_dtypes(exclude=['object']).columns].drop('Survived', axis=1)\n",
    "    label_features = titanic['Survived']\n",
    "\n",
    "    # use one-hot encoding to transform categorical features to numerical features\n",
    "    numerical_features_arr = minmax_scaler.fit_transform(numerical_features)\n",
    "    categorical_features_arr = onehot_enc.fit_transform(categorical_features).toarray()\n",
    "\n",
    "    # combine the numerical features and categorical features\n",
    "    combined_features = pd.DataFrame(data=numerical_features_arr, columns=numerical_features.columns)\n",
    "    combined_features = pd.concat([combined_features, pd.DataFrame(data=categorical_features_arr)], axis=1)\n",
    "    combined_features = pd.concat([combined_features, label_features], axis=1).reset_index(drop=True)\n",
    "\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset len: 711\n",
      "val_dataset len: 178\n",
      "total_dataset len: 889\n"
     ]
    }
   ],
   "source": [
    "titanic_train = TitanicDataSet('./data/titanic.csv', train=True)\n",
    "titanic_val = TitanicDataSet('./data/titanic.csv', train=False)\n",
    "print('train_dataset len:', len(titanic_train))\n",
    "print('val_dataset len:', len(titanic_val))\n",
    "print('total_dataset len:', len(titanic_train) + len(titanic_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.296306</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.032596</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258608</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.421965</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.028107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.296306</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.036598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.044986</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.054457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.366566</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384267</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.371701</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>711 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Age  SibSp     Parch      Fare    0    1    2    3    4  \\\n",
       "0       1.0  0.296306  0.000  0.333333  0.032596  1.0  0.0  0.0  0.0  1.0   \n",
       "1       1.0  0.258608  0.000  0.000000  0.016908  0.0  1.0  0.0  0.0  1.0   \n",
       "2       1.0  0.421965  0.125  0.166667  0.028107  0.0  1.0  0.0  0.0  1.0   \n",
       "3       1.0  0.271174  0.000  0.000000  0.014680  0.0  1.0  0.0  0.0  1.0   \n",
       "4       0.5  0.296306  0.250  0.500000  0.036598  1.0  0.0  0.0  0.0  1.0   \n",
       "..      ...       ...    ...       ...       ...  ...  ...  ...  ...  ...   \n",
       "706     1.0  0.044986  0.375  0.333333  0.054457  0.0  1.0  0.0  0.0  1.0   \n",
       "707     1.0  0.366566  0.000  0.000000  0.015412  0.0  1.0  0.0  0.0  1.0   \n",
       "708     0.0  0.384267  0.125  0.000000  0.101497  0.0  1.0  0.0  0.0  1.0   \n",
       "709     0.0  0.723549  0.000  0.000000  0.285990  1.0  0.0  1.0  0.0  0.0   \n",
       "710     1.0  0.371701  0.125  0.000000  0.031425  0.0  1.0  0.0  0.0  1.0   \n",
       "\n",
       "     Survived  \n",
       "0           1  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           1  \n",
       "..        ...  \n",
       "706         0  \n",
       "707         0  \n",
       "708         0  \n",
       "709         1  \n",
       "710         0  \n",
       "\n",
       "[711 rows x 11 columns]"
      ]
     },
     "execution_count": 1575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "Build a neural network with: \n",
    "1. [v] one hidden layer of size 3 that predicts the survival of the\n",
    "passengers. \n",
    "2. [v] Use a BCE loss (Hint: you need a sigmoid activation in the output layer).\n",
    "3. [v] Use a data loader to train in batches of size 16 and shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1576,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        features = torch.FloatTensor(sample[:-1])  # Exclude the 'Survived' column\n",
    "        label = torch.FloatTensor([sample['Survived']])  # 'Survived' column as label\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1577,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # the weight and bias of linear1 will be initialized \n",
    "        # you can access them by self.linear1.weight and self.linear1.bias\n",
    "        self.linear1 = nn.Linear(D_in, H) # this will create weight, bias for linear1\n",
    "        self.linear2 = nn.Linear(H, D_out) # this will create weight, bias for linear2\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        y_pred = self.sigmoid(self.linear2(h_relu))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1578,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManyLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, num_hidden_layers):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(ManyLayerNet, self).__init__()\n",
    "        # the weight and bias of linear1 will be initialized \n",
    "        # you can access them by self.linear1.weight and self.linear1.bias\n",
    "        # slightly more irregular test results with dropout set to 0.0\n",
    "        self.dropout = nn.Dropout(0.0)\n",
    "        self.first = nn.Linear(D_in, H) # this will create weight, bias for linear1       \n",
    "        # self.hidden_layers = [nn.Linear(H, H) for _ in range(num_hidden_layers)]\n",
    "        self.hidden_layers=[]\n",
    "        self.relu_layers=[]\n",
    "        # alternating Linear and ReLU layers, adding two layers per loop:\n",
    "        for _ in range(int(num_hidden_layers/2)):\n",
    "            self.hidden_layers.append(nn.Linear(H, H))\n",
    "            self.relu_layers.append(nn.ReLU())\n",
    "        self.last = nn.Linear(H, D_out)\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        Dropout is applied before making the prediction in order to ignore some Neurons\n",
    "        \"\"\"\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.first(x))\n",
    "        # can't be applied to ReLU layers which are now part of the hidden layers:\n",
    "        for idx,hidden_layer in enumerate(self.hidden_layers):\n",
    "            if (idx % 5 == 0):\n",
    "                x = self.relu_layers[idx](hidden_layer(x))\n",
    "        y_pred = self.sigmoid(self.last(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; \n",
    "# D_in is input dimension; \t10 features from Pclass/Age/SibSp/Parch/Fare/Sex[0\t1]/Embarked[2\t3\t4]\n",
    "# H is hidden dimension (needs to be even number or will round down!); \n",
    "# D_out is output dimension: 1 or 0 (Survived or not) 1 dimension for binary classification\n",
    "\n",
    "\n",
    "\n",
    "# using a single batch increases both training and test accuracies\n",
    "# this might be a case of the network being too confident\n",
    "N, D_in, H, D_out = 711, 10, 500, 1\n",
    "lr = 0.1\n",
    "num_hidden_layers = 50\n",
    "\n",
    "network = ManyLayerNet(D_in, H, D_out, num_hidden_layers)  # H=3 for one hidden layer with 3 neurons\n",
    "optimizer = optim.Adam(network.parameters(), lr)  # RMSProp + Momentum \n",
    "criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss\n",
    "\n",
    "n_epochs = 40 # You can adjust the number of epochs as needed\n",
    "log_interval = 16 # Print the training status every log_interval epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1580,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(titanic_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=N, shuffle=True)\n",
    "test_dataset = CustomDataset(titanic_val)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=N, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([711, 1])"
      ]
     },
     "execution_count": 1581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataloader))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1582,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    train_losses = [] # Save the loss value of each training loop (epoch) of the neural network model during the training process\n",
    "    train_accuracies = []\n",
    "    network.train()\n",
    "    correct = 0\n",
    "    cur_count = 0 \n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "                \n",
    "        # Accuracy\n",
    "        pred = (output >= 0.5).float()  # survival_rate is the threshold\n",
    "        correct += (pred == target).sum().item()\n",
    "        cur_count += len(data)\n",
    "\n",
    "        # backword propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                epoch, \n",
    "                cur_count, \n",
    "                len(train_dataloader.dataset),\n",
    "                100. * cur_count / len(train_dataloader.dataset), \n",
    "                loss.item(), \n",
    "                correct, len(train_dataloader.dataset),\n",
    "                100. * correct / len(train_dataloader.dataset))\n",
    "            )\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # train_counter.append((batch_idx*16) + ((epoch-1)*len(train_dataloader.dataset)))\n",
    "    return correct / len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # test_counter = [i*len(titanic_train) for i in range(n_epochs+1)] # how many data for training so far\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # forward propagation\n",
    "            output = network(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            # Accuracy\n",
    "            pred = (output >= 0.5).float()  # 0.5 is the threshold\n",
    "            correct += (pred == target).sum().item()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_accuracy = correct / len(test_dataloader.dataset)\n",
    "\n",
    "    return test_accuracy\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, \n",
    "        correct, \n",
    "        len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [711/711 (100%)]\tLoss: 0.689789\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 2 [711/711 (100%)]\tLoss: 0.682213\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 3 [711/711 (100%)]\tLoss: 0.676155\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 4 [711/711 (100%)]\tLoss: 0.671476\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 5 [711/711 (100%)]\tLoss: 0.668070\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 6 [711/711 (100%)]\tLoss: 0.665795\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 7 [711/711 (100%)]\tLoss: 0.664440\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 8 [711/711 (100%)]\tLoss: 0.663687\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 9 [711/711 (100%)]\tLoss: 0.663209\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 10 [711/711 (100%)]\tLoss: 0.662736\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 11 [711/711 (100%)]\tLoss: 0.662048\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 12 [711/711 (100%)]\tLoss: 0.661007\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 13 [711/711 (100%)]\tLoss: 0.659555\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 14 [711/711 (100%)]\tLoss: 0.657709\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 15 [711/711 (100%)]\tLoss: 0.655498\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 16 [711/711 (100%)]\tLoss: 0.652950\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 17 [711/711 (100%)]\tLoss: 0.650130\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 18 [711/711 (100%)]\tLoss: 0.647085\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 19 [711/711 (100%)]\tLoss: 0.643867\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 20 [711/711 (100%)]\tLoss: 0.640471\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 21 [711/711 (100%)]\tLoss: 0.636912\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 22 [711/711 (100%)]\tLoss: 0.633186\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 23 [711/711 (100%)]\tLoss: 0.629306\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 24 [711/711 (100%)]\tLoss: 0.625240\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 25 [711/711 (100%)]\tLoss: 0.620954\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 26 [711/711 (100%)]\tLoss: 0.616424\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 27 [711/711 (100%)]\tLoss: 0.611627\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 28 [711/711 (100%)]\tLoss: 0.606593\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 29 [711/711 (100%)]\tLoss: 0.601362\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 30 [711/711 (100%)]\tLoss: 0.595961\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 31 [711/711 (100%)]\tLoss: 0.590440\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 32 [711/711 (100%)]\tLoss: 0.584828\t Accuracy: 435/711 (61%)\n",
      "Train Epoch: 33 [711/711 (100%)]\tLoss: 0.579129\t Accuracy: 460/711 (65%)\n",
      "Train Epoch: 34 [711/711 (100%)]\tLoss: 0.573364\t Accuracy: 487/711 (68%)\n",
      "Train Epoch: 35 [711/711 (100%)]\tLoss: 0.567592\t Accuracy: 524/711 (74%)\n",
      "Train Epoch: 36 [711/711 (100%)]\tLoss: 0.561865\t Accuracy: 548/711 (77%)\n",
      "Train Epoch: 37 [711/711 (100%)]\tLoss: 0.556213\t Accuracy: 568/711 (80%)\n",
      "Train Epoch: 38 [711/711 (100%)]\tLoss: 0.550647\t Accuracy: 572/711 (80%)\n",
      "Train Epoch: 39 [711/711 (100%)]\tLoss: 0.545160\t Accuracy: 571/711 (80%)\n",
      "Train Epoch: 40 [711/711 (100%)]\tLoss: 0.539766\t Accuracy: 573/711 (81%)\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_accuracy = train(epoch)\n",
    "    test_accuracy = test()\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 1585,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(train_accuracies), color='blue', label='Training Accuracy')\n",
    "plt.plot(np.array(test_accuracies), color='red', label='Test Accuracy')\n",
    "# plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
