{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x143e2330220>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "1. Write a custom dataset class for the titanic data (see the data folder on GitHub).\n",
    "2. Use only the features: \"Pclass\", \"Age\", \"SibSp\", \"Parch\", „Fare“, „Sex“, „Embarked“.\n",
    "3. Preprocess the features accordingly in that class (scaling, one-hot-encoding, etc) and\n",
    "4. split the data into train and validation data (80% and 20%). The constructor of that class\n",
    "should look like this:\n",
    "```\n",
    "titanic_train = TitanicDataSet('titanic.csv', train=True)\n",
    "titanic_val = TitanicDataSet('titanic.csv', train=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TitanicDataSet(root_dir, train):\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    onehot_enc = OneHotEncoder()\n",
    "\n",
    "    titanic = pd.read_csv(root_dir)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(69)\n",
    "    train_indices, test_indices = [ds.indices for ds in torch.utils.data.random_split(titanic, [0.8, 0.2], generator=generator)]\n",
    "    \n",
    "    if train:\n",
    "      titanic = titanic.iloc[train_indices]\n",
    "    else:\n",
    "      titanic = titanic.iloc[test_indices]\n",
    "\n",
    "    # only need \"Pclass\", \"Age\", \"SibSp\", \"Parch\", „Fare“, „Sex“, „Embarked“\n",
    "    titanic = titanic[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\", \"Survived\"]]\n",
    "\n",
    "    # because i found NaN in 'Age' column, so filled it with mean value\n",
    "    # remove the NaN data in the dataset, which is from Embarked column, two rows\n",
    "    mean_values = titanic[titanic.select_dtypes(exclude=['object']).columns].mean()\n",
    "    titanic = titanic.fillna(mean_values)\n",
    "    titanic = titanic.dropna()\n",
    "    titanic = titanic.reset_index(drop=True) # reset the index, or combine_features will cause wrong index and length\n",
    "\n",
    "    # devide the data into categorical features and numerical features, and put the 'Survived' column into categorical features\n",
    "    categorical_features = titanic[titanic.select_dtypes(include=['object']).columns.tolist()]\n",
    "    numerical_features = titanic[titanic.select_dtypes(exclude=['object']).columns].drop('Survived', axis=1)\n",
    "    label_features = titanic['Survived']\n",
    "\n",
    "    # use one-hot encoding to transform categorical features to numerical features\n",
    "    numerical_features_arr = minmax_scaler.fit_transform(numerical_features)\n",
    "    categorical_features_arr = onehot_enc.fit_transform(categorical_features).toarray()\n",
    "\n",
    "    # combine the numerical features and categorical features\n",
    "    combined_features = pd.DataFrame(data=numerical_features_arr, columns=numerical_features.columns)\n",
    "    combined_features = pd.concat([combined_features, pd.DataFrame(data=categorical_features_arr)], axis=1)\n",
    "    combined_features = pd.concat([combined_features, label_features], axis=1).reset_index(drop=True)\n",
    "\n",
    "    return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset len: 711\n",
      "val_dataset len: 178\n",
      "total_dataset len: 889\n"
     ]
    }
   ],
   "source": [
    "titanic_train = TitanicDataSet('./data/titanic.csv', train=True)\n",
    "titanic_val = TitanicDataSet('./data/titanic.csv', train=False)\n",
    "print('train_dataset len:', len(titanic_train))\n",
    "print('val_dataset len:', len(titanic_val))\n",
    "print('total_dataset len:', len(titanic_train) + len(titanic_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.296306</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.032596</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258608</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.421965</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.028107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.296306</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.036598</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.044986</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.054457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.366566</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384267</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.371701</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>711 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Age  SibSp     Parch      Fare    0    1    2    3    4  \\\n",
       "0       1.0  0.296306  0.000  0.333333  0.032596  1.0  0.0  0.0  0.0  1.0   \n",
       "1       1.0  0.258608  0.000  0.000000  0.016908  0.0  1.0  0.0  0.0  1.0   \n",
       "2       1.0  0.421965  0.125  0.166667  0.028107  0.0  1.0  0.0  0.0  1.0   \n",
       "3       1.0  0.271174  0.000  0.000000  0.014680  0.0  1.0  0.0  0.0  1.0   \n",
       "4       0.5  0.296306  0.250  0.500000  0.036598  1.0  0.0  0.0  0.0  1.0   \n",
       "..      ...       ...    ...       ...       ...  ...  ...  ...  ...  ...   \n",
       "706     1.0  0.044986  0.375  0.333333  0.054457  0.0  1.0  0.0  0.0  1.0   \n",
       "707     1.0  0.366566  0.000  0.000000  0.015412  0.0  1.0  0.0  0.0  1.0   \n",
       "708     0.0  0.384267  0.125  0.000000  0.101497  0.0  1.0  0.0  0.0  1.0   \n",
       "709     0.0  0.723549  0.000  0.000000  0.285990  1.0  0.0  1.0  0.0  0.0   \n",
       "710     1.0  0.371701  0.125  0.000000  0.031425  0.0  1.0  0.0  0.0  1.0   \n",
       "\n",
       "     Survived  \n",
       "0           1  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           1  \n",
       "..        ...  \n",
       "706         0  \n",
       "707         0  \n",
       "708         0  \n",
       "709         1  \n",
       "710         0  \n",
       "\n",
       "[711 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "Build a neural network with: \n",
    "1. [v] one hidden layer of size 3 that predicts the survival of the\n",
    "passengers. \n",
    "2. [v] Use a BCE loss (Hint: you need a sigmoid activation in the output layer).\n",
    "3. [v] Use a data loader to train in batches of size 16 and shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        features = torch.FloatTensor(sample[:-1])  # Exclude the 'Survived' column\n",
    "        label = torch.FloatTensor([sample['Survived']])  # 'Survived' column as label\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # the weight and bias of linear1 will be initialized \n",
    "        # you can access them by self.linear1.weight and self.linear1.bias\n",
    "        self.linear1 = nn.Linear(D_in, H) # this will create weight, bias for linear1\n",
    "        self.linear2 = nn.Linear(H, D_out) # this will create weight, bias for linear2\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        y_pred = self.sigmoid(self.linear2(h_relu))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManyLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, num_hidden_layers):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(ManyLayerNet, self).__init__()\n",
    "        # the weight and bias of linear1 will be initialized \n",
    "        # you can access them by self.linear1.weight and self.linear1.bias\n",
    "        self.first = nn.Linear(D_in, H) # this will create weight, bias for linear1       \n",
    "        self.hidden_layers = [nn.Linear(H, H) for _ in range(num_hidden_layers)]\n",
    "        self.last = nn.Linear(H, D_out)\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.first(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        y_pred = self.sigmoid(self.last(x))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; \n",
    "# D_in is input dimension; \t10 features from Pclass/Age/SibSp/Parch/Fare/Sex[0\t1]/Embarked[2\t3\t4]\n",
    "# H is hidden dimension (Only one hidden layer, but containing 3 neurons.); \n",
    "# D_out is output dimension: 1 or 0 (Survived or not) 1 dimension for binary classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N, D_in, H, D_out = 16, 10, 3, 1\n",
    "lr = 0.001\n",
    "num_hidden_layers = 1\n",
    "\n",
    "network = ManyLayerNet(D_in, H, D_out, num_hidden_layers)  # H=3 for one hidden layer with 3 neurons\n",
    "optimizer = optim.Adam(network.parameters(), lr)  # RMSProp + Momentum \n",
    "criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss\n",
    "\n",
    "n_epochs = 50 # You can adjust the number of epochs as needed\n",
    "log_interval = 10 # Print the training status every log_interval epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(titanic_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=N, shuffle=True)\n",
    "test_dataset = CustomDataset(titanic_val)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=N, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = next(iter(train_dataloader))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    train_losses = [] # Save the loss value of each training loop (epoch) of the neural network model during the training process\n",
    "    train_accuracies = []\n",
    "    network.train()\n",
    "    correct = 0\n",
    "    cur_count = 0 \n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # forward propagation\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "                \n",
    "        # Accuracy\n",
    "        pred = (output >= 0.5).float()  # survival_rate is the threshold\n",
    "        correct += (pred == target).sum().item()\n",
    "        cur_count += len(data)\n",
    "\n",
    "        # backword propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                epoch, \n",
    "                cur_count, \n",
    "                len(train_dataloader.dataset),\n",
    "                100. * cur_count / len(train_dataloader.dataset), \n",
    "                loss.item(), \n",
    "                correct, len(train_dataloader.dataset),\n",
    "                100. * correct / len(train_dataloader.dataset))\n",
    "            )\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # train_counter.append((batch_idx*16) + ((epoch-1)*len(train_dataloader.dataset)))\n",
    "    return correct / len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # test_counter = [i*len(titanic_train) for i in range(n_epochs+1)] # how many data for training so far\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # forward propagation\n",
    "            output = network(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            # Accuracy\n",
    "            pred = (output >= 0.5).float()  # 0.5 is the threshold\n",
    "            correct += (pred == target).sum().item()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_accuracy = correct / len(test_dataloader.dataset)\n",
    "\n",
    "    return test_accuracy\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, \n",
    "        correct, \n",
    "        len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [16/711 (2%)]\tLoss: 0.740707\t Accuracy: 5/711 (1%)\n",
      "Train Epoch: 1 [176/711 (25%)]\tLoss: 0.742101\t Accuracy: 70/711 (10%)\n",
      "Train Epoch: 1 [336/711 (47%)]\tLoss: 0.698626\t Accuracy: 136/711 (19%)\n",
      "Train Epoch: 1 [496/711 (70%)]\tLoss: 0.739331\t Accuracy: 198/711 (28%)\n",
      "Train Epoch: 1 [656/711 (92%)]\tLoss: 0.729406\t Accuracy: 254/711 (36%)\n",
      "Train Epoch: 2 [16/711 (2%)]\tLoss: 0.702906\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 2 [176/711 (25%)]\tLoss: 0.700856\t Accuracy: 70/711 (10%)\n",
      "Train Epoch: 2 [336/711 (47%)]\tLoss: 0.699305\t Accuracy: 121/711 (17%)\n",
      "Train Epoch: 2 [496/711 (70%)]\tLoss: 0.693608\t Accuracy: 169/711 (24%)\n",
      "Train Epoch: 2 [656/711 (92%)]\tLoss: 0.695336\t Accuracy: 232/711 (33%)\n",
      "Train Epoch: 3 [16/711 (2%)]\tLoss: 0.697644\t Accuracy: 5/711 (1%)\n",
      "Train Epoch: 3 [176/711 (25%)]\tLoss: 0.684501\t Accuracy: 112/711 (16%)\n",
      "Train Epoch: 3 [336/711 (47%)]\tLoss: 0.696260\t Accuracy: 202/711 (28%)\n",
      "Train Epoch: 3 [496/711 (70%)]\tLoss: 0.692815\t Accuracy: 299/711 (42%)\n",
      "Train Epoch: 3 [656/711 (92%)]\tLoss: 0.684884\t Accuracy: 394/711 (55%)\n",
      "Train Epoch: 4 [16/711 (2%)]\tLoss: 0.683373\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 4 [176/711 (25%)]\tLoss: 0.674843\t Accuracy: 104/711 (15%)\n",
      "Train Epoch: 4 [336/711 (47%)]\tLoss: 0.672833\t Accuracy: 205/711 (29%)\n",
      "Train Epoch: 4 [496/711 (70%)]\tLoss: 0.680748\t Accuracy: 295/711 (41%)\n",
      "Train Epoch: 4 [656/711 (92%)]\tLoss: 0.674047\t Accuracy: 400/711 (56%)\n",
      "Train Epoch: 5 [16/711 (2%)]\tLoss: 0.693675\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 5 [176/711 (25%)]\tLoss: 0.662372\t Accuracy: 101/711 (14%)\n",
      "Train Epoch: 5 [336/711 (47%)]\tLoss: 0.670524\t Accuracy: 194/711 (27%)\n",
      "Train Epoch: 5 [496/711 (70%)]\tLoss: 0.653696\t Accuracy: 297/711 (42%)\n",
      "Train Epoch: 5 [656/711 (92%)]\tLoss: 0.660810\t Accuracy: 399/711 (56%)\n",
      "Train Epoch: 6 [16/711 (2%)]\tLoss: 0.686481\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 6 [176/711 (25%)]\tLoss: 0.646703\t Accuracy: 97/711 (14%)\n",
      "Train Epoch: 6 [336/711 (47%)]\tLoss: 0.657023\t Accuracy: 196/711 (28%)\n",
      "Train Epoch: 6 [496/711 (70%)]\tLoss: 0.696164\t Accuracy: 286/711 (40%)\n",
      "Train Epoch: 6 [656/711 (92%)]\tLoss: 0.675527\t Accuracy: 394/711 (55%)\n",
      "Train Epoch: 7 [16/711 (2%)]\tLoss: 0.651839\t Accuracy: 12/711 (2%)\n",
      "Train Epoch: 7 [176/711 (25%)]\tLoss: 0.649431\t Accuracy: 112/711 (16%)\n",
      "Train Epoch: 7 [336/711 (47%)]\tLoss: 0.699329\t Accuracy: 210/711 (30%)\n",
      "Train Epoch: 7 [496/711 (70%)]\tLoss: 0.685850\t Accuracy: 312/711 (44%)\n",
      "Train Epoch: 7 [656/711 (92%)]\tLoss: 0.658570\t Accuracy: 404/711 (57%)\n",
      "Train Epoch: 8 [16/711 (2%)]\tLoss: 0.670277\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 8 [176/711 (25%)]\tLoss: 0.685370\t Accuracy: 113/711 (16%)\n",
      "Train Epoch: 8 [336/711 (47%)]\tLoss: 0.671669\t Accuracy: 213/711 (30%)\n",
      "Train Epoch: 8 [496/711 (70%)]\tLoss: 0.684845\t Accuracy: 317/711 (45%)\n",
      "Train Epoch: 8 [656/711 (92%)]\tLoss: 0.667370\t Accuracy: 404/711 (57%)\n",
      "Train Epoch: 9 [16/711 (2%)]\tLoss: 0.653859\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 9 [176/711 (25%)]\tLoss: 0.686595\t Accuracy: 113/711 (16%)\n",
      "Train Epoch: 9 [336/711 (47%)]\tLoss: 0.683245\t Accuracy: 208/711 (29%)\n",
      "Train Epoch: 9 [496/711 (70%)]\tLoss: 0.636105\t Accuracy: 317/711 (45%)\n",
      "Train Epoch: 9 [656/711 (92%)]\tLoss: 0.651470\t Accuracy: 400/711 (56%)\n",
      "Train Epoch: 10 [16/711 (2%)]\tLoss: 0.683877\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 10 [176/711 (25%)]\tLoss: 0.701581\t Accuracy: 110/711 (15%)\n",
      "Train Epoch: 10 [336/711 (47%)]\tLoss: 0.631881\t Accuracy: 221/711 (31%)\n",
      "Train Epoch: 10 [496/711 (70%)]\tLoss: 0.684581\t Accuracy: 310/711 (44%)\n",
      "Train Epoch: 10 [656/711 (92%)]\tLoss: 0.720571\t Accuracy: 403/711 (57%)\n",
      "Train Epoch: 11 [16/711 (2%)]\tLoss: 0.703979\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 11 [176/711 (25%)]\tLoss: 0.630673\t Accuracy: 102/711 (14%)\n",
      "Train Epoch: 11 [336/711 (47%)]\tLoss: 0.629403\t Accuracy: 206/711 (29%)\n",
      "Train Epoch: 11 [496/711 (70%)]\tLoss: 0.629524\t Accuracy: 297/711 (42%)\n",
      "Train Epoch: 11 [656/711 (92%)]\tLoss: 0.627760\t Accuracy: 403/711 (57%)\n",
      "Train Epoch: 12 [16/711 (2%)]\tLoss: 0.607991\t Accuracy: 13/711 (2%)\n",
      "Train Epoch: 12 [176/711 (25%)]\tLoss: 0.625165\t Accuracy: 107/711 (15%)\n",
      "Train Epoch: 12 [336/711 (47%)]\tLoss: 0.645233\t Accuracy: 209/711 (29%)\n",
      "Train Epoch: 12 [496/711 (70%)]\tLoss: 0.683679\t Accuracy: 303/711 (43%)\n",
      "Train Epoch: 12 [656/711 (92%)]\tLoss: 0.707933\t Accuracy: 404/711 (57%)\n",
      "Train Epoch: 13 [16/711 (2%)]\tLoss: 0.682728\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 13 [176/711 (25%)]\tLoss: 0.704737\t Accuracy: 102/711 (14%)\n",
      "Train Epoch: 13 [336/711 (47%)]\tLoss: 0.583388\t Accuracy: 197/711 (28%)\n",
      "Train Epoch: 13 [496/711 (70%)]\tLoss: 0.661889\t Accuracy: 300/711 (42%)\n",
      "Train Epoch: 13 [656/711 (92%)]\tLoss: 0.639831\t Accuracy: 405/711 (57%)\n",
      "Train Epoch: 14 [16/711 (2%)]\tLoss: 0.638219\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 14 [176/711 (25%)]\tLoss: 0.718763\t Accuracy: 97/711 (14%)\n",
      "Train Epoch: 14 [336/711 (47%)]\tLoss: 0.637394\t Accuracy: 194/711 (27%)\n",
      "Train Epoch: 14 [496/711 (70%)]\tLoss: 0.594655\t Accuracy: 294/711 (41%)\n",
      "Train Epoch: 14 [656/711 (92%)]\tLoss: 0.692328\t Accuracy: 400/711 (56%)\n",
      "Train Epoch: 15 [16/711 (2%)]\tLoss: 0.650706\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 15 [176/711 (25%)]\tLoss: 0.695492\t Accuracy: 103/711 (14%)\n",
      "Train Epoch: 15 [336/711 (47%)]\tLoss: 0.666725\t Accuracy: 198/711 (28%)\n",
      "Train Epoch: 15 [496/711 (70%)]\tLoss: 0.678406\t Accuracy: 302/711 (42%)\n",
      "Train Epoch: 15 [656/711 (92%)]\tLoss: 0.735550\t Accuracy: 398/711 (56%)\n",
      "Train Epoch: 16 [16/711 (2%)]\tLoss: 0.665662\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 16 [176/711 (25%)]\tLoss: 0.760191\t Accuracy: 97/711 (14%)\n",
      "Train Epoch: 16 [336/711 (47%)]\tLoss: 0.684282\t Accuracy: 203/711 (29%)\n",
      "Train Epoch: 16 [496/711 (70%)]\tLoss: 0.597029\t Accuracy: 301/711 (42%)\n",
      "Train Epoch: 16 [656/711 (92%)]\tLoss: 0.601178\t Accuracy: 399/711 (56%)\n",
      "Train Epoch: 17 [16/711 (2%)]\tLoss: 0.621067\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 17 [176/711 (25%)]\tLoss: 0.580788\t Accuracy: 113/711 (16%)\n",
      "Train Epoch: 17 [336/711 (47%)]\tLoss: 0.677408\t Accuracy: 203/711 (29%)\n",
      "Train Epoch: 17 [496/711 (70%)]\tLoss: 0.644055\t Accuracy: 302/711 (42%)\n",
      "Train Epoch: 17 [656/711 (92%)]\tLoss: 0.633090\t Accuracy: 397/711 (56%)\n",
      "Train Epoch: 18 [16/711 (2%)]\tLoss: 0.664081\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 18 [176/711 (25%)]\tLoss: 0.725387\t Accuracy: 101/711 (14%)\n",
      "Train Epoch: 18 [336/711 (47%)]\tLoss: 0.571287\t Accuracy: 199/711 (28%)\n",
      "Train Epoch: 18 [496/711 (70%)]\tLoss: 0.639228\t Accuracy: 294/711 (41%)\n",
      "Train Epoch: 18 [656/711 (92%)]\tLoss: 0.567087\t Accuracy: 396/711 (56%)\n",
      "Train Epoch: 19 [16/711 (2%)]\tLoss: 0.653034\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 19 [176/711 (25%)]\tLoss: 0.674577\t Accuracy: 105/711 (15%)\n",
      "Train Epoch: 19 [336/711 (47%)]\tLoss: 0.611420\t Accuracy: 217/711 (31%)\n",
      "Train Epoch: 19 [496/711 (70%)]\tLoss: 0.660344\t Accuracy: 312/711 (44%)\n",
      "Train Epoch: 19 [656/711 (92%)]\tLoss: 0.672709\t Accuracy: 404/711 (57%)\n",
      "Train Epoch: 20 [16/711 (2%)]\tLoss: 0.626437\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 20 [176/711 (25%)]\tLoss: 0.648733\t Accuracy: 108/711 (15%)\n",
      "Train Epoch: 20 [336/711 (47%)]\tLoss: 0.671589\t Accuracy: 210/711 (30%)\n",
      "Train Epoch: 20 [496/711 (70%)]\tLoss: 0.752261\t Accuracy: 310/711 (44%)\n",
      "Train Epoch: 20 [656/711 (92%)]\tLoss: 0.713203\t Accuracy: 404/711 (57%)\n",
      "Train Epoch: 21 [16/711 (2%)]\tLoss: 0.599290\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 21 [176/711 (25%)]\tLoss: 0.615957\t Accuracy: 114/711 (16%)\n",
      "Train Epoch: 21 [336/711 (47%)]\tLoss: 0.670671\t Accuracy: 202/711 (28%)\n",
      "Train Epoch: 21 [496/711 (70%)]\tLoss: 0.635875\t Accuracy: 301/711 (42%)\n",
      "Train Epoch: 21 [656/711 (92%)]\tLoss: 0.690003\t Accuracy: 400/711 (56%)\n",
      "Train Epoch: 22 [16/711 (2%)]\tLoss: 0.658847\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 22 [176/711 (25%)]\tLoss: 0.681738\t Accuracy: 101/711 (14%)\n",
      "Train Epoch: 22 [336/711 (47%)]\tLoss: 0.570854\t Accuracy: 210/711 (30%)\n",
      "Train Epoch: 22 [496/711 (70%)]\tLoss: 0.647884\t Accuracy: 308/711 (43%)\n",
      "Train Epoch: 22 [656/711 (92%)]\tLoss: 0.634433\t Accuracy: 405/711 (57%)\n",
      "Train Epoch: 23 [16/711 (2%)]\tLoss: 0.574984\t Accuracy: 12/711 (2%)\n",
      "Train Epoch: 23 [176/711 (25%)]\tLoss: 0.740692\t Accuracy: 104/711 (15%)\n",
      "Train Epoch: 23 [336/711 (47%)]\tLoss: 0.673851\t Accuracy: 208/711 (29%)\n",
      "Train Epoch: 23 [496/711 (70%)]\tLoss: 0.554605\t Accuracy: 302/711 (42%)\n",
      "Train Epoch: 23 [656/711 (92%)]\tLoss: 0.634602\t Accuracy: 401/711 (56%)\n",
      "Train Epoch: 24 [16/711 (2%)]\tLoss: 0.533142\t Accuracy: 14/711 (2%)\n",
      "Train Epoch: 24 [176/711 (25%)]\tLoss: 0.620210\t Accuracy: 114/711 (16%)\n",
      "Train Epoch: 24 [336/711 (47%)]\tLoss: 0.731613\t Accuracy: 203/711 (29%)\n",
      "Train Epoch: 24 [496/711 (70%)]\tLoss: 0.728793\t Accuracy: 305/711 (43%)\n",
      "Train Epoch: 24 [656/711 (92%)]\tLoss: 0.642231\t Accuracy: 401/711 (56%)\n",
      "Train Epoch: 25 [16/711 (2%)]\tLoss: 0.662336\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 25 [176/711 (25%)]\tLoss: 0.631023\t Accuracy: 114/711 (16%)\n",
      "Train Epoch: 25 [336/711 (47%)]\tLoss: 0.680545\t Accuracy: 206/711 (29%)\n",
      "Train Epoch: 25 [496/711 (70%)]\tLoss: 0.554873\t Accuracy: 301/711 (42%)\n",
      "Train Epoch: 25 [656/711 (92%)]\tLoss: 0.588511\t Accuracy: 400/711 (56%)\n",
      "Train Epoch: 26 [16/711 (2%)]\tLoss: 0.666112\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 26 [176/711 (25%)]\tLoss: 0.617441\t Accuracy: 113/711 (16%)\n",
      "Train Epoch: 26 [336/711 (47%)]\tLoss: 0.631708\t Accuracy: 208/711 (29%)\n",
      "Train Epoch: 26 [496/711 (70%)]\tLoss: 0.619201\t Accuracy: 298/711 (42%)\n",
      "Train Epoch: 26 [656/711 (92%)]\tLoss: 0.648615\t Accuracy: 399/711 (56%)\n",
      "Train Epoch: 27 [16/711 (2%)]\tLoss: 0.657337\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 27 [176/711 (25%)]\tLoss: 0.720344\t Accuracy: 111/711 (16%)\n",
      "Train Epoch: 27 [336/711 (47%)]\tLoss: 0.647936\t Accuracy: 202/711 (28%)\n",
      "Train Epoch: 27 [496/711 (70%)]\tLoss: 0.651499\t Accuracy: 304/711 (43%)\n",
      "Train Epoch: 27 [656/711 (92%)]\tLoss: 0.601213\t Accuracy: 407/711 (57%)\n",
      "Train Epoch: 28 [16/711 (2%)]\tLoss: 0.579668\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 28 [176/711 (25%)]\tLoss: 0.645573\t Accuracy: 99/711 (14%)\n",
      "Train Epoch: 28 [336/711 (47%)]\tLoss: 0.607938\t Accuracy: 204/711 (29%)\n",
      "Train Epoch: 28 [496/711 (70%)]\tLoss: 0.681594\t Accuracy: 302/711 (42%)\n",
      "Train Epoch: 28 [656/711 (92%)]\tLoss: 0.611341\t Accuracy: 401/711 (56%)\n",
      "Train Epoch: 29 [16/711 (2%)]\tLoss: 0.623883\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 29 [176/711 (25%)]\tLoss: 0.610961\t Accuracy: 100/711 (14%)\n",
      "Train Epoch: 29 [336/711 (47%)]\tLoss: 0.639638\t Accuracy: 198/711 (28%)\n",
      "Train Epoch: 29 [496/711 (70%)]\tLoss: 0.574119\t Accuracy: 299/711 (42%)\n",
      "Train Epoch: 29 [656/711 (92%)]\tLoss: 0.715496\t Accuracy: 401/711 (56%)\n",
      "Train Epoch: 30 [16/711 (2%)]\tLoss: 0.619820\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 30 [176/711 (25%)]\tLoss: 0.629183\t Accuracy: 103/711 (14%)\n",
      "Train Epoch: 30 [336/711 (47%)]\tLoss: 0.677883\t Accuracy: 200/711 (28%)\n",
      "Train Epoch: 30 [496/711 (70%)]\tLoss: 0.646952\t Accuracy: 300/711 (42%)\n",
      "Train Epoch: 30 [656/711 (92%)]\tLoss: 0.552069\t Accuracy: 404/711 (57%)\n",
      "Train Epoch: 31 [16/711 (2%)]\tLoss: 0.642678\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 31 [176/711 (25%)]\tLoss: 0.606085\t Accuracy: 116/711 (16%)\n",
      "Train Epoch: 31 [336/711 (47%)]\tLoss: 0.630033\t Accuracy: 212/711 (30%)\n",
      "Train Epoch: 31 [496/711 (70%)]\tLoss: 0.668893\t Accuracy: 302/711 (42%)\n",
      "Train Epoch: 31 [656/711 (92%)]\tLoss: 0.601512\t Accuracy: 402/711 (57%)\n",
      "Train Epoch: 32 [16/711 (2%)]\tLoss: 0.683719\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 32 [176/711 (25%)]\tLoss: 0.606955\t Accuracy: 105/711 (15%)\n",
      "Train Epoch: 32 [336/711 (47%)]\tLoss: 0.570629\t Accuracy: 209/711 (29%)\n",
      "Train Epoch: 32 [496/711 (70%)]\tLoss: 0.652394\t Accuracy: 306/711 (43%)\n",
      "Train Epoch: 32 [656/711 (92%)]\tLoss: 0.651213\t Accuracy: 402/711 (57%)\n",
      "Train Epoch: 33 [16/711 (2%)]\tLoss: 0.635590\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 33 [176/711 (25%)]\tLoss: 0.610796\t Accuracy: 110/711 (15%)\n",
      "Train Epoch: 33 [336/711 (47%)]\tLoss: 0.673950\t Accuracy: 207/711 (29%)\n",
      "Train Epoch: 33 [496/711 (70%)]\tLoss: 0.614404\t Accuracy: 302/711 (42%)\n",
      "Train Epoch: 33 [656/711 (92%)]\tLoss: 0.569607\t Accuracy: 399/711 (56%)\n",
      "Train Epoch: 34 [16/711 (2%)]\tLoss: 0.676089\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 34 [176/711 (25%)]\tLoss: 0.498836\t Accuracy: 113/711 (16%)\n",
      "Train Epoch: 34 [336/711 (47%)]\tLoss: 0.585402\t Accuracy: 210/711 (30%)\n",
      "Train Epoch: 34 [496/711 (70%)]\tLoss: 0.622406\t Accuracy: 310/711 (44%)\n",
      "Train Epoch: 34 [656/711 (92%)]\tLoss: 0.558495\t Accuracy: 404/711 (57%)\n",
      "Train Epoch: 35 [16/711 (2%)]\tLoss: 0.668898\t Accuracy: 8/711 (1%)\n",
      "Train Epoch: 35 [176/711 (25%)]\tLoss: 0.504726\t Accuracy: 111/711 (16%)\n",
      "Train Epoch: 35 [336/711 (47%)]\tLoss: 0.613492\t Accuracy: 215/711 (30%)\n",
      "Train Epoch: 35 [496/711 (70%)]\tLoss: 0.589617\t Accuracy: 307/711 (43%)\n",
      "Train Epoch: 35 [656/711 (92%)]\tLoss: 0.571170\t Accuracy: 408/711 (57%)\n",
      "Train Epoch: 36 [16/711 (2%)]\tLoss: 0.546946\t Accuracy: 12/711 (2%)\n",
      "Train Epoch: 36 [176/711 (25%)]\tLoss: 0.595012\t Accuracy: 111/711 (16%)\n",
      "Train Epoch: 36 [336/711 (47%)]\tLoss: 0.638102\t Accuracy: 213/711 (30%)\n",
      "Train Epoch: 36 [496/711 (70%)]\tLoss: 0.605776\t Accuracy: 309/711 (43%)\n",
      "Train Epoch: 36 [656/711 (92%)]\tLoss: 0.650619\t Accuracy: 405/711 (57%)\n",
      "Train Epoch: 37 [16/711 (2%)]\tLoss: 0.617550\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 37 [176/711 (25%)]\tLoss: 0.519918\t Accuracy: 112/711 (16%)\n",
      "Train Epoch: 37 [336/711 (47%)]\tLoss: 0.599460\t Accuracy: 212/711 (30%)\n",
      "Train Epoch: 37 [496/711 (70%)]\tLoss: 0.717862\t Accuracy: 303/711 (43%)\n",
      "Train Epoch: 37 [656/711 (92%)]\tLoss: 0.731172\t Accuracy: 399/711 (56%)\n",
      "Train Epoch: 38 [16/711 (2%)]\tLoss: 0.752473\t Accuracy: 6/711 (1%)\n",
      "Train Epoch: 38 [176/711 (25%)]\tLoss: 0.572707\t Accuracy: 103/711 (14%)\n",
      "Train Epoch: 38 [336/711 (47%)]\tLoss: 0.621776\t Accuracy: 200/711 (28%)\n",
      "Train Epoch: 38 [496/711 (70%)]\tLoss: 0.606474\t Accuracy: 299/711 (42%)\n",
      "Train Epoch: 38 [656/711 (92%)]\tLoss: 0.552053\t Accuracy: 402/711 (57%)\n",
      "Train Epoch: 39 [16/711 (2%)]\tLoss: 0.611439\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 39 [176/711 (25%)]\tLoss: 0.629489\t Accuracy: 114/711 (16%)\n",
      "Train Epoch: 39 [336/711 (47%)]\tLoss: 0.598614\t Accuracy: 211/711 (30%)\n",
      "Train Epoch: 39 [496/711 (70%)]\tLoss: 0.555601\t Accuracy: 309/711 (43%)\n",
      "Train Epoch: 39 [656/711 (92%)]\tLoss: 0.469401\t Accuracy: 401/711 (56%)\n",
      "Train Epoch: 40 [16/711 (2%)]\tLoss: 0.520187\t Accuracy: 13/711 (2%)\n",
      "Train Epoch: 40 [176/711 (25%)]\tLoss: 0.570143\t Accuracy: 111/711 (16%)\n",
      "Train Epoch: 40 [336/711 (47%)]\tLoss: 0.673150\t Accuracy: 200/711 (28%)\n",
      "Train Epoch: 40 [496/711 (70%)]\tLoss: 0.480069\t Accuracy: 299/711 (42%)\n",
      "Train Epoch: 40 [656/711 (92%)]\tLoss: 0.630451\t Accuracy: 403/711 (57%)\n",
      "Train Epoch: 41 [16/711 (2%)]\tLoss: 0.718233\t Accuracy: 7/711 (1%)\n",
      "Train Epoch: 41 [176/711 (25%)]\tLoss: 0.664518\t Accuracy: 108/711 (15%)\n",
      "Train Epoch: 41 [336/711 (47%)]\tLoss: 0.596443\t Accuracy: 203/711 (29%)\n",
      "Train Epoch: 41 [496/711 (70%)]\tLoss: 0.683076\t Accuracy: 298/711 (42%)\n",
      "Train Epoch: 41 [656/711 (92%)]\tLoss: 0.454980\t Accuracy: 404/711 (57%)\n",
      "Train Epoch: 42 [16/711 (2%)]\tLoss: 0.529109\t Accuracy: 12/711 (2%)\n",
      "Train Epoch: 42 [176/711 (25%)]\tLoss: 0.600716\t Accuracy: 110/711 (15%)\n",
      "Train Epoch: 42 [336/711 (47%)]\tLoss: 0.539653\t Accuracy: 214/711 (30%)\n",
      "Train Epoch: 42 [496/711 (70%)]\tLoss: 0.694308\t Accuracy: 310/711 (44%)\n",
      "Train Epoch: 42 [656/711 (92%)]\tLoss: 0.652522\t Accuracy: 404/711 (57%)\n",
      "Train Epoch: 43 [16/711 (2%)]\tLoss: 0.582791\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 43 [176/711 (25%)]\tLoss: 0.660058\t Accuracy: 107/711 (15%)\n",
      "Train Epoch: 43 [336/711 (47%)]\tLoss: 0.542539\t Accuracy: 209/711 (29%)\n",
      "Train Epoch: 43 [496/711 (70%)]\tLoss: 0.596351\t Accuracy: 308/711 (43%)\n",
      "Train Epoch: 43 [656/711 (92%)]\tLoss: 0.682619\t Accuracy: 402/711 (57%)\n",
      "Train Epoch: 44 [16/711 (2%)]\tLoss: 0.578031\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 44 [176/711 (25%)]\tLoss: 0.571938\t Accuracy: 106/711 (15%)\n",
      "Train Epoch: 44 [336/711 (47%)]\tLoss: 0.609465\t Accuracy: 205/711 (29%)\n",
      "Train Epoch: 44 [496/711 (70%)]\tLoss: 0.649993\t Accuracy: 309/711 (43%)\n",
      "Train Epoch: 44 [656/711 (92%)]\tLoss: 0.752491\t Accuracy: 400/711 (56%)\n",
      "Train Epoch: 45 [16/711 (2%)]\tLoss: 0.621191\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 45 [176/711 (25%)]\tLoss: 0.706361\t Accuracy: 97/711 (14%)\n",
      "Train Epoch: 45 [336/711 (47%)]\tLoss: 0.550287\t Accuracy: 213/711 (30%)\n",
      "Train Epoch: 45 [496/711 (70%)]\tLoss: 0.652447\t Accuracy: 301/711 (42%)\n",
      "Train Epoch: 45 [656/711 (92%)]\tLoss: 0.576325\t Accuracy: 402/711 (57%)\n",
      "Train Epoch: 46 [16/711 (2%)]\tLoss: 0.612810\t Accuracy: 10/711 (1%)\n",
      "Train Epoch: 46 [176/711 (25%)]\tLoss: 0.579742\t Accuracy: 99/711 (14%)\n",
      "Train Epoch: 46 [336/711 (47%)]\tLoss: 0.724083\t Accuracy: 201/711 (28%)\n",
      "Train Epoch: 46 [496/711 (70%)]\tLoss: 0.668797\t Accuracy: 305/711 (43%)\n",
      "Train Epoch: 46 [656/711 (92%)]\tLoss: 0.504241\t Accuracy: 406/711 (57%)\n",
      "Train Epoch: 47 [16/711 (2%)]\tLoss: 0.519749\t Accuracy: 13/711 (2%)\n",
      "Train Epoch: 47 [176/711 (25%)]\tLoss: 0.598336\t Accuracy: 116/711 (16%)\n",
      "Train Epoch: 47 [336/711 (47%)]\tLoss: 0.628209\t Accuracy: 211/711 (30%)\n",
      "Train Epoch: 47 [496/711 (70%)]\tLoss: 0.575391\t Accuracy: 309/711 (43%)\n",
      "Train Epoch: 47 [656/711 (92%)]\tLoss: 0.633539\t Accuracy: 404/711 (57%)\n",
      "Train Epoch: 48 [16/711 (2%)]\tLoss: 0.687895\t Accuracy: 9/711 (1%)\n",
      "Train Epoch: 48 [176/711 (25%)]\tLoss: 0.640041\t Accuracy: 103/711 (14%)\n",
      "Train Epoch: 48 [336/711 (47%)]\tLoss: 0.595282\t Accuracy: 204/711 (29%)\n",
      "Train Epoch: 48 [496/711 (70%)]\tLoss: 0.538112\t Accuracy: 303/711 (43%)\n",
      "Train Epoch: 48 [656/711 (92%)]\tLoss: 0.604872\t Accuracy: 401/711 (56%)\n",
      "Train Epoch: 49 [16/711 (2%)]\tLoss: 0.606792\t Accuracy: 11/711 (2%)\n",
      "Train Epoch: 49 [176/711 (25%)]\tLoss: 0.592094\t Accuracy: 108/711 (15%)\n",
      "Train Epoch: 49 [336/711 (47%)]\tLoss: 0.597215\t Accuracy: 201/711 (28%)\n",
      "Train Epoch: 49 [496/711 (70%)]\tLoss: 0.653036\t Accuracy: 310/711 (44%)\n",
      "Train Epoch: 49 [656/711 (92%)]\tLoss: 0.671741\t Accuracy: 402/711 (57%)\n",
      "Train Epoch: 50 [16/711 (2%)]\tLoss: 0.641621\t Accuracy: 7/711 (1%)\n",
      "Train Epoch: 50 [176/711 (25%)]\tLoss: 0.564491\t Accuracy: 109/711 (15%)\n",
      "Train Epoch: 50 [336/711 (47%)]\tLoss: 0.567653\t Accuracy: 206/711 (29%)\n",
      "Train Epoch: 50 [496/711 (70%)]\tLoss: 0.625681\t Accuracy: 309/711 (43%)\n",
      "Train Epoch: 50 [656/711 (92%)]\tLoss: 0.622757\t Accuracy: 398/711 (56%)\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_accuracy = train(epoch)\n",
    "    test_accuracy = test()\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'accuracy')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(train_accuracies), color='blue', label='Training Accuracy')\n",
    "plt.plot(np.array(test_accuracies), color='red', label='Test Accuracy')\n",
    "# plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
