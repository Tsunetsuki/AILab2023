{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7fd499e7e950>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the data\n",
    "- Is there existing missing values \n",
    "- How many missing values and valuable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass   Age  SibSp  Parch  Fare     Sex Embarked  Survived\n",
       "61        1  38.0      0      0  80.0  female      NaN         1\n",
       "829       1  62.0      0      0  80.0  female      NaN         1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are existing invalid data in the dataset, so we need to remove them\n",
    "test = pd.read_csv('./data/titanic.csv')\n",
    "test = test[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\", \"Survived\"]]\n",
    "test_mean = test.mean()\n",
    "test = test.fillna(test_mean)\n",
    "rows_with_nan = test[test.isna().any(axis=1)]\n",
    "rows_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "889"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.dropna()\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "1. Write a custom dataset class for the titanic data (see the data folder on GitHub).\n",
    "2. Use only the features: \"Pclass\", \"Age\", \"SibSp\", \"Parch\", „Fare“, „Sex“, „Embarked“.\n",
    "3. Preprocess the features accordingly in that class (scaling, one-hot-encoding, etc) and\n",
    "4. split the data into train and validation data (80% and 20%). The constructor of that class\n",
    "should look like this:\n",
    "```\n",
    "titanic_train = TitanicDataSet('titanic.csv', train=True)\n",
    "titanic_val = TitanicDataSet('titanic.csv', train=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; \n",
    "# D_in is input dimension; \t10 features from Pclass/Age/SibSp/Parch/Fare/Sex[0\t1]/Embarked[2\t3\t4]\n",
    "# H is hidden dimension (Only one hidden layer, but containing 3 neurons.); \n",
    "# D_out is output dimension: 1 or 0 (Survived or not) 1 dimension for binary classification\n",
    "N, D_in, H, D_out = 16, 10, 3, 1\n",
    "lr = 0.001\n",
    "multi_neuron = 3\n",
    "num_hidden_layers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def TitanicDataSet(root_dir, train):\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    onehot_enc = OneHotEncoder()\n",
    "\n",
    "    titanic = pd.read_csv(root_dir)\n",
    "    # only need \"Pclass\", \"Age\", \"SibSp\", \"Parch\", „Fare“, „Sex“, „Embarked“\n",
    "    titanic = titanic[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\", \"Survived\"]]\n",
    "\n",
    "    # because i found NaN in 'Age' column, so filled it with mean value\n",
    "    mean_values = titanic.mean()\n",
    "    titanic = titanic.fillna(mean_values)\n",
    "    # remove the NaN data in the dataset, which is from Embarked column, two rows\n",
    "    titanic = titanic.dropna()\n",
    "    titanic = titanic.reset_index(drop=True) # reset the index, or combine_features will cause wrong index and length\n",
    "\n",
    "    # devide the data into categorical features and numerical features, and put the 'Survived' column into categorical features\n",
    "    categorical_features = titanic[titanic.select_dtypes(include=['object']).columns.tolist()]\n",
    "    numerical_features = titanic[titanic.select_dtypes(exclude=['object']).columns].drop('Survived', axis=1)\n",
    "    label_features = titanic['Survived']\n",
    "\n",
    "    # use one-hot encoding to transform categorical features to numerical features\n",
    "    numerical_features_arr = minmax_scaler.fit_transform(numerical_features)\n",
    "    categorical_features_arr = onehot_enc.fit_transform(categorical_features).toarray()\n",
    "\n",
    "    # combine the numerical features and categorical features\n",
    "    combined_features = pd.DataFrame(data=numerical_features_arr, columns=numerical_features.columns)\n",
    "    combined_features = pd.concat([combined_features, pd.DataFrame(data=categorical_features_arr)], axis=1)\n",
    "    combined_features = pd.concat([combined_features, label_features], axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    train_data, test_data = train_test_split(combined_features, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    # split the data into train and validation data (80% and 20%).\n",
    "    if train:\n",
    "        # return 80% of the data\n",
    "        return train_data\n",
    "    else: \n",
    "        # return 20% of the data\n",
    "        return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset len: 711\n",
      "val_dataset len: 178\n",
      "total_dataset len: 889\n"
     ]
    }
   ],
   "source": [
    "titanic_train = TitanicDataSet('./data/titanic.csv', train=True)\n",
    "titanic_val = TitanicDataSet('./data/titanic.csv', train=False)\n",
    "print('train_dataset len:', len(titanic_train))\n",
    "print('val_dataset len:', len(titanic_val))\n",
    "print('total_dataset len:', len(titanic_train) + len(titanic_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367921</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028213</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.396833</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367921</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.135753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.170646</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.091543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.367921</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.308872</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050610</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258608</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.067096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.409399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>711 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass       Age  SibSp     Parch      Fare    0    1    2    3    4  \\\n",
       "707     0.0  0.271174  0.000  0.000000  0.295806  1.0  0.0  0.0  0.0  1.0   \n",
       "239     1.0  0.367921  0.125  0.000000  0.028213  1.0  0.0  1.0  0.0  0.0   \n",
       "381     1.0  0.396833  0.000  0.000000  0.015469  0.0  1.0  0.0  0.0  1.0   \n",
       "791     1.0  0.367921  1.000  0.333333  0.135753  1.0  0.0  0.0  0.0  1.0   \n",
       "682     1.0  0.170646  0.625  0.333333  0.091543  0.0  1.0  0.0  0.0  1.0   \n",
       "..      ...       ...    ...       ...       ...  ...  ...  ...  ...  ...   \n",
       "106     1.0  0.367921  0.000  0.000000  0.015176  0.0  1.0  0.0  0.0  1.0   \n",
       "270     1.0  0.308872  0.000  0.000000  0.000000  0.0  1.0  0.0  0.0  1.0   \n",
       "860     0.0  0.597889  0.000  0.000000  0.050610  1.0  0.0  0.0  0.0  1.0   \n",
       "435     1.0  0.258608  0.250  0.333333  0.067096  1.0  0.0  0.0  0.0  1.0   \n",
       "102     1.0  0.409399  0.000  0.000000  0.016892  0.0  1.0  0.0  0.0  1.0   \n",
       "\n",
       "     Survived  \n",
       "707         1  \n",
       "239         0  \n",
       "381         0  \n",
       "791         0  \n",
       "682         0  \n",
       "..        ...  \n",
       "106         1  \n",
       "270         1  \n",
       "860         1  \n",
       "435         0  \n",
       "102         0  \n",
       "\n",
       "[711 rows x 11 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "Build a neural network with: \n",
    "1. [v] one hidden layer of size 3 that predicts the survival of the\n",
    "passengers. \n",
    "2. [v] Use a BCE loss (Hint: you need a sigmoid activation in the output layer).\n",
    "3. [v] Use a data loader to train in batches of size 16 and shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        features = torch.FloatTensor(sample[:-1])  # Exclude the 'Survived' column\n",
    "        label = torch.FloatTensor([sample['Survived']])  # 'Survived' column as label\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, num_hidden_layers):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(MultiLayerNet, self).__init__()\n",
    "        # the weight and bias of linear1 will be initialized \n",
    "        # you can access them by self.linear1.weight and self.linear1.bias\n",
    "        self.linear1 = nn.Linear(D_in, H) # this will create weight, bias for linear1\n",
    "        self.linear2 = nn.Linear(H, H) \n",
    "        self.linear2 = nn.Linear(H, H) \n",
    "        self.linear2 = nn.Linear(H, H) \n",
    "        self.linear3 = nn.Linear(H, D_out) # this will create weight, bias for linear3\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            h_relu = F.relu(self.linear2(h_relu))\n",
    "        y_pred = self.sigmoid(self.linear3(h_relu))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # the weight and bias of linear1 will be initialized \n",
    "        # you can access them by self.linear1.weight and self.linear1.bias\n",
    "        self.linear1 = nn.Linear(D_in, H) # this will create weight, bias for linear1\n",
    "        self.linear2 = nn.Linear(H, D_out) # this will create weight, bias for linear2\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        y_pred = self.sigmoid(self.linear2(h_relu))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = TwoLayerNet(D_in, H, D_out)  # H=3 for one hidden layer with 3 neurons\n",
    "multi_network = MultiLayerNet(D_in, multi_neuron, D_out, num_hidden_layers) \n",
    "\n",
    "# optimizer = optim.Adam(network.parameters(), lr)  # RMSProp + Momentum \n",
    "optimizer = torch.optim.SGD(network.parameters(), lr)\n",
    "multi_optimizer = torch.optim.SGD(multi_network.parameters(), lr)\n",
    "\n",
    "criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss\n",
    "multi_criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss\n",
    "\n",
    "n_epochs = 50 # You can adjust the number of epochs as needed\n",
    "log_interval = 10 # Print the training status every log_interval epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [] # Save the loss value of each training loop (epoch) of the neural network model during the training process\n",
    "train_counter = [] # Save the number of images for training so far\n",
    "test_losses = [] # Save the loss value of each test loop (epoch) of the neural network model during the training process\n",
    "test_counter = [i*len(titanic_train) for i in range(n_epochs+1)] # how many data for training so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(titanic_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=N, shuffle=True)\n",
    "test_dataset = CustomDataset(titanic_val)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=N, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_multi(epoch):\n",
    "    multi_network.train()\n",
    "    correct = 0\n",
    "    cur_count = 0 \n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        multi_optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward propagation\n",
    "        output = multi_network(data)\n",
    "        loss = multi_criterion(output, target)\n",
    "                \n",
    "        # Accuracy\n",
    "        pred = (output >= 0.5).float()  # survival_rate is the threshold\n",
    "        correct += (pred == target).sum().item()\n",
    "        cur_count += len(data)\n",
    "\n",
    "        # backword propagation\n",
    "        loss.backward()\n",
    "        multi_optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Muti Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                epoch, \n",
    "                cur_count, \n",
    "                len(train_dataloader.dataset),\n",
    "                100. * cur_count / len(train_dataloader.dataset), \n",
    "                loss.item(), \n",
    "                correct, len(train_dataloader.dataset),\n",
    "                100. * correct / len(train_dataloader.dataset))\n",
    "            )\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*16) + ((epoch-1)*len(train_dataloader.dataset)))\n",
    "            \n",
    "    return correct / len(train_dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    correct = 0\n",
    "    cur_count = 0 \n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward propagation\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "                \n",
    "        # Accuracy\n",
    "        pred = (output >= 0.5).float()  # survival_rate is the threshold\n",
    "        correct += (pred == target).sum().item()\n",
    "        cur_count += len(data)\n",
    "\n",
    "        # backword propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                epoch, \n",
    "                cur_count, \n",
    "                len(train_dataloader.dataset),\n",
    "                100. * cur_count / len(train_dataloader.dataset), \n",
    "                loss.item(), \n",
    "                correct, len(train_dataloader.dataset),\n",
    "                100. * correct / len(train_dataloader.dataset))\n",
    "            )\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*16) + ((epoch-1)*len(train_dataloader.dataset)))\n",
    "            \n",
    "    return correct / len(train_dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # forward propagation\n",
    "            output = network(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            # Accuracy\n",
    "            pred = (output >= 0.5).float()  # 0.5 is the threshold\n",
    "            correct += (pred == target).sum().item()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, \n",
    "        correct, \n",
    "        len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset))\n",
    "    )\n",
    "    return correct / len(test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multi():\n",
    "    multi_network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            # forward propagation\n",
    "            output = multi_network(data)\n",
    "            test_loss += multi_criterion(output, target).item()\n",
    "            # Accuracy\n",
    "            pred = (output >= 0.5).float()  # 0.5 is the threshold\n",
    "            correct += (pred == target).sum().item()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nMulti Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, \n",
    "        correct, \n",
    "        len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset))\n",
    "    )\n",
    "    return correct / len(test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multi Test set: Average loss: 0.0495, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 1 [16/711 (2%)]\tLoss: 0.780133\t Accuracy: 5/711 (1%)\n",
      "Muti Train Epoch: 1 [176/711 (25%)]\tLoss: 0.721133\t Accuracy: 70/711 (10%)\n",
      "Muti Train Epoch: 1 [336/711 (47%)]\tLoss: 0.762980\t Accuracy: 126/711 (18%)\n",
      "Muti Train Epoch: 1 [496/711 (70%)]\tLoss: 0.689679\t Accuracy: 191/711 (27%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muti Train Epoch: 1 [656/711 (92%)]\tLoss: 0.755899\t Accuracy: 257/711 (36%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0510, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 2 [16/711 (2%)]\tLoss: 0.749532\t Accuracy: 6/711 (1%)\n",
      "Muti Train Epoch: 2 [176/711 (25%)]\tLoss: 0.805049\t Accuracy: 67/711 (9%)\n",
      "Muti Train Epoch: 2 [336/711 (47%)]\tLoss: 0.680344\t Accuracy: 123/711 (17%)\n",
      "Muti Train Epoch: 2 [496/711 (70%)]\tLoss: 0.723828\t Accuracy: 181/711 (25%)\n",
      "Muti Train Epoch: 2 [656/711 (92%)]\tLoss: 0.751527\t Accuracy: 248/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0496, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 3 [16/711 (2%)]\tLoss: 0.736006\t Accuracy: 6/711 (1%)\n",
      "Muti Train Epoch: 3 [176/711 (25%)]\tLoss: 0.694065\t Accuracy: 69/711 (10%)\n",
      "Muti Train Epoch: 3 [336/711 (47%)]\tLoss: 0.765239\t Accuracy: 129/711 (18%)\n",
      "Muti Train Epoch: 3 [496/711 (70%)]\tLoss: 0.711456\t Accuracy: 199/711 (28%)\n",
      "Muti Train Epoch: 3 [656/711 (92%)]\tLoss: 0.791072\t Accuracy: 251/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0503, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 4 [16/711 (2%)]\tLoss: 0.702770\t Accuracy: 7/711 (1%)\n",
      "Muti Train Epoch: 4 [176/711 (25%)]\tLoss: 0.735669\t Accuracy: 73/711 (10%)\n",
      "Muti Train Epoch: 4 [336/711 (47%)]\tLoss: 0.764641\t Accuracy: 123/711 (17%)\n",
      "Muti Train Epoch: 4 [496/711 (70%)]\tLoss: 0.717542\t Accuracy: 187/711 (26%)\n",
      "Muti Train Epoch: 4 [656/711 (92%)]\tLoss: 0.706041\t Accuracy: 249/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0480, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 5 [16/711 (2%)]\tLoss: 0.747189\t Accuracy: 6/711 (1%)\n",
      "Muti Train Epoch: 5 [176/711 (25%)]\tLoss: 0.743853\t Accuracy: 69/711 (10%)\n",
      "Muti Train Epoch: 5 [336/711 (47%)]\tLoss: 0.736407\t Accuracy: 124/711 (17%)\n",
      "Muti Train Epoch: 5 [496/711 (70%)]\tLoss: 0.703348\t Accuracy: 185/711 (26%)\n",
      "Muti Train Epoch: 5 [656/711 (92%)]\tLoss: 0.770589\t Accuracy: 249/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0499, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 6 [16/711 (2%)]\tLoss: 0.807304\t Accuracy: 3/711 (0%)\n",
      "Muti Train Epoch: 6 [176/711 (25%)]\tLoss: 0.755786\t Accuracy: 58/711 (8%)\n",
      "Muti Train Epoch: 6 [336/711 (47%)]\tLoss: 0.722151\t Accuracy: 118/711 (17%)\n",
      "Muti Train Epoch: 6 [496/711 (70%)]\tLoss: 0.694241\t Accuracy: 185/711 (26%)\n",
      "Muti Train Epoch: 6 [656/711 (92%)]\tLoss: 0.705398\t Accuracy: 248/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0497, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 7 [16/711 (2%)]\tLoss: 0.681851\t Accuracy: 8/711 (1%)\n",
      "Muti Train Epoch: 7 [176/711 (25%)]\tLoss: 0.689548\t Accuracy: 68/711 (10%)\n",
      "Muti Train Epoch: 7 [336/711 (47%)]\tLoss: 0.802334\t Accuracy: 123/711 (17%)\n",
      "Muti Train Epoch: 7 [496/711 (70%)]\tLoss: 0.708801\t Accuracy: 190/711 (27%)\n",
      "Muti Train Epoch: 7 [656/711 (92%)]\tLoss: 0.765633\t Accuracy: 249/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0482, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 8 [16/711 (2%)]\tLoss: 0.675310\t Accuracy: 8/711 (1%)\n",
      "Muti Train Epoch: 8 [176/711 (25%)]\tLoss: 0.727066\t Accuracy: 74/711 (10%)\n",
      "Muti Train Epoch: 8 [336/711 (47%)]\tLoss: 0.692742\t Accuracy: 133/711 (19%)\n",
      "Muti Train Epoch: 8 [496/711 (70%)]\tLoss: 0.708022\t Accuracy: 194/711 (27%)\n",
      "Muti Train Epoch: 8 [656/711 (92%)]\tLoss: 0.737719\t Accuracy: 251/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0485, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 9 [16/711 (2%)]\tLoss: 0.719037\t Accuracy: 5/711 (1%)\n",
      "Muti Train Epoch: 9 [176/711 (25%)]\tLoss: 0.643195\t Accuracy: 65/711 (9%)\n",
      "Muti Train Epoch: 9 [336/711 (47%)]\tLoss: 0.671249\t Accuracy: 127/711 (18%)\n",
      "Muti Train Epoch: 9 [496/711 (70%)]\tLoss: 0.745185\t Accuracy: 186/711 (26%)\n",
      "Muti Train Epoch: 9 [656/711 (92%)]\tLoss: 0.704815\t Accuracy: 249/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0481, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 10 [16/711 (2%)]\tLoss: 0.742373\t Accuracy: 4/711 (1%)\n",
      "Muti Train Epoch: 10 [176/711 (25%)]\tLoss: 0.700213\t Accuracy: 62/711 (9%)\n",
      "Muti Train Epoch: 10 [336/711 (47%)]\tLoss: 0.653912\t Accuracy: 135/711 (19%)\n",
      "Muti Train Epoch: 10 [496/711 (70%)]\tLoss: 0.707664\t Accuracy: 193/711 (27%)\n",
      "Muti Train Epoch: 10 [656/711 (92%)]\tLoss: 0.717024\t Accuracy: 249/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0475, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 11 [16/711 (2%)]\tLoss: 0.747725\t Accuracy: 3/711 (0%)\n",
      "Muti Train Epoch: 11 [176/711 (25%)]\tLoss: 0.670439\t Accuracy: 59/711 (8%)\n",
      "Muti Train Epoch: 11 [336/711 (47%)]\tLoss: 0.709016\t Accuracy: 127/711 (18%)\n",
      "Muti Train Epoch: 11 [496/711 (70%)]\tLoss: 0.682425\t Accuracy: 189/711 (27%)\n",
      "Muti Train Epoch: 11 [656/711 (92%)]\tLoss: 0.727236\t Accuracy: 245/711 (34%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0471, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 12 [16/711 (2%)]\tLoss: 0.715530\t Accuracy: 4/711 (1%)\n",
      "Muti Train Epoch: 12 [176/711 (25%)]\tLoss: 0.754330\t Accuracy: 63/711 (9%)\n",
      "Muti Train Epoch: 12 [336/711 (47%)]\tLoss: 0.614553\t Accuracy: 128/711 (18%)\n",
      "Muti Train Epoch: 12 [496/711 (70%)]\tLoss: 0.632842\t Accuracy: 189/711 (27%)\n",
      "Muti Train Epoch: 12 [656/711 (92%)]\tLoss: 0.648715\t Accuracy: 248/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0469, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 13 [16/711 (2%)]\tLoss: 0.689490\t Accuracy: 7/711 (1%)\n",
      "Muti Train Epoch: 13 [176/711 (25%)]\tLoss: 0.660114\t Accuracy: 69/711 (10%)\n",
      "Muti Train Epoch: 13 [336/711 (47%)]\tLoss: 0.676155\t Accuracy: 120/711 (17%)\n",
      "Muti Train Epoch: 13 [496/711 (70%)]\tLoss: 0.698989\t Accuracy: 187/711 (26%)\n",
      "Muti Train Epoch: 13 [656/711 (92%)]\tLoss: 0.704643\t Accuracy: 248/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0463, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 14 [16/711 (2%)]\tLoss: 0.704659\t Accuracy: 6/711 (1%)\n",
      "Muti Train Epoch: 14 [176/711 (25%)]\tLoss: 0.656401\t Accuracy: 63/711 (9%)\n",
      "Muti Train Epoch: 14 [336/711 (47%)]\tLoss: 0.669321\t Accuracy: 124/711 (17%)\n",
      "Muti Train Epoch: 14 [496/711 (70%)]\tLoss: 0.676957\t Accuracy: 183/711 (26%)\n",
      "Muti Train Epoch: 14 [656/711 (92%)]\tLoss: 0.686093\t Accuracy: 253/711 (36%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0460, Accuracy: 69/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 15 [16/711 (2%)]\tLoss: 0.722326\t Accuracy: 3/711 (0%)\n",
      "Muti Train Epoch: 15 [176/711 (25%)]\tLoss: 0.747946\t Accuracy: 62/711 (9%)\n",
      "Muti Train Epoch: 15 [336/711 (47%)]\tLoss: 0.666614\t Accuracy: 121/711 (17%)\n",
      "Muti Train Epoch: 15 [496/711 (70%)]\tLoss: 0.710145\t Accuracy: 192/711 (27%)\n",
      "Muti Train Epoch: 15 [656/711 (92%)]\tLoss: 0.709403\t Accuracy: 248/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0458, Accuracy: 70/178 (39%)\n",
      "\n",
      "Muti Train Epoch: 16 [16/711 (2%)]\tLoss: 0.649049\t Accuracy: 7/711 (1%)\n",
      "Muti Train Epoch: 16 [176/711 (25%)]\tLoss: 0.731100\t Accuracy: 74/711 (10%)\n",
      "Muti Train Epoch: 16 [336/711 (47%)]\tLoss: 0.724547\t Accuracy: 125/711 (18%)\n",
      "Muti Train Epoch: 16 [496/711 (70%)]\tLoss: 0.688245\t Accuracy: 179/711 (25%)\n",
      "Muti Train Epoch: 16 [656/711 (92%)]\tLoss: 0.703005\t Accuracy: 246/711 (35%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0459, Accuracy: 72/178 (40%)\n",
      "\n",
      "Muti Train Epoch: 17 [16/711 (2%)]\tLoss: 0.713649\t Accuracy: 3/711 (0%)\n",
      "Muti Train Epoch: 17 [176/711 (25%)]\tLoss: 0.627856\t Accuracy: 71/711 (10%)\n",
      "Muti Train Epoch: 17 [336/711 (47%)]\tLoss: 0.740126\t Accuracy: 136/711 (19%)\n",
      "Muti Train Epoch: 17 [496/711 (70%)]\tLoss: 0.694908\t Accuracy: 201/711 (28%)\n",
      "Muti Train Epoch: 17 [656/711 (92%)]\tLoss: 0.640317\t Accuracy: 271/711 (38%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0457, Accuracy: 75/178 (42%)\n",
      "\n",
      "Muti Train Epoch: 18 [16/711 (2%)]\tLoss: 0.652012\t Accuracy: 9/711 (1%)\n",
      "Muti Train Epoch: 18 [176/711 (25%)]\tLoss: 0.676334\t Accuracy: 73/711 (10%)\n",
      "Muti Train Epoch: 18 [336/711 (47%)]\tLoss: 0.671637\t Accuracy: 146/711 (21%)\n",
      "Muti Train Epoch: 18 [496/711 (70%)]\tLoss: 0.682734\t Accuracy: 224/711 (32%)\n",
      "Muti Train Epoch: 18 [656/711 (92%)]\tLoss: 0.648186\t Accuracy: 293/711 (41%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0455, Accuracy: 87/178 (49%)\n",
      "\n",
      "Muti Train Epoch: 19 [16/711 (2%)]\tLoss: 0.642263\t Accuracy: 10/711 (1%)\n",
      "Muti Train Epoch: 19 [176/711 (25%)]\tLoss: 0.668435\t Accuracy: 89/711 (13%)\n",
      "Muti Train Epoch: 19 [336/711 (47%)]\tLoss: 0.700163\t Accuracy: 178/711 (25%)\n",
      "Muti Train Epoch: 19 [496/711 (70%)]\tLoss: 0.666473\t Accuracy: 269/711 (38%)\n",
      "Muti Train Epoch: 19 [656/711 (92%)]\tLoss: 0.656442\t Accuracy: 362/711 (51%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0452, Accuracy: 119/178 (67%)\n",
      "\n",
      "Muti Train Epoch: 20 [16/711 (2%)]\tLoss: 0.677527\t Accuracy: 10/711 (1%)\n",
      "Muti Train Epoch: 20 [176/711 (25%)]\tLoss: 0.665998\t Accuracy: 124/711 (17%)\n",
      "Muti Train Epoch: 20 [336/711 (47%)]\tLoss: 0.646932\t Accuracy: 241/711 (34%)\n",
      "Muti Train Epoch: 20 [496/711 (70%)]\tLoss: 0.645774\t Accuracy: 348/711 (49%)\n",
      "Muti Train Epoch: 20 [656/711 (92%)]\tLoss: 0.647158\t Accuracy: 457/711 (64%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0449, Accuracy: 133/178 (75%)\n",
      "\n",
      "Muti Train Epoch: 21 [16/711 (2%)]\tLoss: 0.683402\t Accuracy: 12/711 (2%)\n",
      "Muti Train Epoch: 21 [176/711 (25%)]\tLoss: 0.665706\t Accuracy: 121/711 (17%)\n",
      "Muti Train Epoch: 21 [336/711 (47%)]\tLoss: 0.688520\t Accuracy: 233/711 (33%)\n",
      "Muti Train Epoch: 21 [496/711 (70%)]\tLoss: 0.700085\t Accuracy: 346/711 (49%)\n",
      "Muti Train Epoch: 21 [656/711 (92%)]\tLoss: 0.676820\t Accuracy: 464/711 (65%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0444, Accuracy: 134/178 (75%)\n",
      "\n",
      "Muti Train Epoch: 22 [16/711 (2%)]\tLoss: 0.671346\t Accuracy: 10/711 (1%)\n",
      "Muti Train Epoch: 22 [176/711 (25%)]\tLoss: 0.660435\t Accuracy: 121/711 (17%)\n",
      "Muti Train Epoch: 22 [336/711 (47%)]\tLoss: 0.656570\t Accuracy: 237/711 (33%)\n",
      "Muti Train Epoch: 22 [496/711 (70%)]\tLoss: 0.675167\t Accuracy: 358/711 (50%)\n",
      "Muti Train Epoch: 22 [656/711 (92%)]\tLoss: 0.661105\t Accuracy: 487/711 (68%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0442, Accuracy: 137/178 (77%)\n",
      "\n",
      "Muti Train Epoch: 23 [16/711 (2%)]\tLoss: 0.639624\t Accuracy: 15/711 (2%)\n",
      "Muti Train Epoch: 23 [176/711 (25%)]\tLoss: 0.694772\t Accuracy: 137/711 (19%)\n",
      "Muti Train Epoch: 23 [336/711 (47%)]\tLoss: 0.625143\t Accuracy: 255/711 (36%)\n",
      "Muti Train Epoch: 23 [496/711 (70%)]\tLoss: 0.627521\t Accuracy: 366/711 (51%)\n",
      "Muti Train Epoch: 23 [656/711 (92%)]\tLoss: 0.632883\t Accuracy: 486/711 (68%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0444, Accuracy: 138/178 (78%)\n",
      "\n",
      "Muti Train Epoch: 24 [16/711 (2%)]\tLoss: 0.659560\t Accuracy: 12/711 (2%)\n",
      "Muti Train Epoch: 24 [176/711 (25%)]\tLoss: 0.683211\t Accuracy: 125/711 (18%)\n",
      "Muti Train Epoch: 24 [336/711 (47%)]\tLoss: 0.653157\t Accuracy: 253/711 (36%)\n",
      "Muti Train Epoch: 24 [496/711 (70%)]\tLoss: 0.660747\t Accuracy: 364/711 (51%)\n",
      "Muti Train Epoch: 24 [656/711 (92%)]\tLoss: 0.672284\t Accuracy: 483/711 (68%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0441, Accuracy: 138/178 (78%)\n",
      "\n",
      "Muti Train Epoch: 25 [16/711 (2%)]\tLoss: 0.642373\t Accuracy: 13/711 (2%)\n",
      "Muti Train Epoch: 25 [176/711 (25%)]\tLoss: 0.650752\t Accuracy: 136/711 (19%)\n",
      "Muti Train Epoch: 25 [336/711 (47%)]\tLoss: 0.671758\t Accuracy: 248/711 (35%)\n",
      "Muti Train Epoch: 25 [496/711 (70%)]\tLoss: 0.628230\t Accuracy: 362/711 (51%)\n",
      "Muti Train Epoch: 25 [656/711 (92%)]\tLoss: 0.633458\t Accuracy: 486/711 (68%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0436, Accuracy: 138/178 (78%)\n",
      "\n",
      "Muti Train Epoch: 26 [16/711 (2%)]\tLoss: 0.660506\t Accuracy: 12/711 (2%)\n",
      "Muti Train Epoch: 26 [176/711 (25%)]\tLoss: 0.678688\t Accuracy: 145/711 (20%)\n",
      "Muti Train Epoch: 26 [336/711 (47%)]\tLoss: 0.649042\t Accuracy: 251/711 (35%)\n",
      "Muti Train Epoch: 26 [496/711 (70%)]\tLoss: 0.657157\t Accuracy: 373/711 (52%)\n",
      "Muti Train Epoch: 26 [656/711 (92%)]\tLoss: 0.651439\t Accuracy: 491/711 (69%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0445, Accuracy: 138/178 (78%)\n",
      "\n",
      "Muti Train Epoch: 27 [16/711 (2%)]\tLoss: 0.648765\t Accuracy: 13/711 (2%)\n",
      "Muti Train Epoch: 27 [176/711 (25%)]\tLoss: 0.651406\t Accuracy: 136/711 (19%)\n",
      "Muti Train Epoch: 27 [336/711 (47%)]\tLoss: 0.649064\t Accuracy: 255/711 (36%)\n",
      "Muti Train Epoch: 27 [496/711 (70%)]\tLoss: 0.610441\t Accuracy: 375/711 (53%)\n",
      "Muti Train Epoch: 27 [656/711 (92%)]\tLoss: 0.671915\t Accuracy: 492/711 (69%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0437, Accuracy: 139/178 (78%)\n",
      "\n",
      "Muti Train Epoch: 28 [16/711 (2%)]\tLoss: 0.622227\t Accuracy: 13/711 (2%)\n",
      "Muti Train Epoch: 28 [176/711 (25%)]\tLoss: 0.627402\t Accuracy: 128/711 (18%)\n",
      "Muti Train Epoch: 28 [336/711 (47%)]\tLoss: 0.633806\t Accuracy: 251/711 (35%)\n",
      "Muti Train Epoch: 28 [496/711 (70%)]\tLoss: 0.665286\t Accuracy: 380/711 (53%)\n",
      "Muti Train Epoch: 28 [656/711 (92%)]\tLoss: 0.614362\t Accuracy: 507/711 (71%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0436, Accuracy: 142/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 29 [16/711 (2%)]\tLoss: 0.675535\t Accuracy: 10/711 (1%)\n",
      "Muti Train Epoch: 29 [176/711 (25%)]\tLoss: 0.660761\t Accuracy: 131/711 (18%)\n",
      "Muti Train Epoch: 29 [336/711 (47%)]\tLoss: 0.635373\t Accuracy: 262/711 (37%)\n",
      "Muti Train Epoch: 29 [496/711 (70%)]\tLoss: 0.626287\t Accuracy: 397/711 (56%)\n",
      "Muti Train Epoch: 29 [656/711 (92%)]\tLoss: 0.654949\t Accuracy: 513/711 (72%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0433, Accuracy: 142/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 30 [16/711 (2%)]\tLoss: 0.622797\t Accuracy: 15/711 (2%)\n",
      "Muti Train Epoch: 30 [176/711 (25%)]\tLoss: 0.631486\t Accuracy: 138/711 (19%)\n",
      "Muti Train Epoch: 30 [336/711 (47%)]\tLoss: 0.634098\t Accuracy: 268/711 (38%)\n",
      "Muti Train Epoch: 30 [496/711 (70%)]\tLoss: 0.631000\t Accuracy: 393/711 (55%)\n",
      "Muti Train Epoch: 30 [656/711 (92%)]\tLoss: 0.658655\t Accuracy: 518/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0426, Accuracy: 142/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 31 [16/711 (2%)]\tLoss: 0.613109\t Accuracy: 14/711 (2%)\n",
      "Muti Train Epoch: 31 [176/711 (25%)]\tLoss: 0.633505\t Accuracy: 139/711 (20%)\n",
      "Muti Train Epoch: 31 [336/711 (47%)]\tLoss: 0.627681\t Accuracy: 264/711 (37%)\n",
      "Muti Train Epoch: 31 [496/711 (70%)]\tLoss: 0.639114\t Accuracy: 392/711 (55%)\n",
      "Muti Train Epoch: 31 [656/711 (92%)]\tLoss: 0.658311\t Accuracy: 519/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0424, Accuracy: 142/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 32 [16/711 (2%)]\tLoss: 0.664231\t Accuracy: 10/711 (1%)\n",
      "Muti Train Epoch: 32 [176/711 (25%)]\tLoss: 0.650247\t Accuracy: 133/711 (19%)\n",
      "Muti Train Epoch: 32 [336/711 (47%)]\tLoss: 0.621922\t Accuracy: 251/711 (35%)\n",
      "Muti Train Epoch: 32 [496/711 (70%)]\tLoss: 0.651973\t Accuracy: 375/711 (53%)\n",
      "Muti Train Epoch: 32 [656/711 (92%)]\tLoss: 0.619534\t Accuracy: 508/711 (71%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0423, Accuracy: 142/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 33 [16/711 (2%)]\tLoss: 0.623969\t Accuracy: 13/711 (2%)\n",
      "Muti Train Epoch: 33 [176/711 (25%)]\tLoss: 0.615354\t Accuracy: 139/711 (20%)\n",
      "Muti Train Epoch: 33 [336/711 (47%)]\tLoss: 0.633338\t Accuracy: 262/711 (37%)\n",
      "Muti Train Epoch: 33 [496/711 (70%)]\tLoss: 0.645675\t Accuracy: 385/711 (54%)\n",
      "Muti Train Epoch: 33 [656/711 (92%)]\tLoss: 0.613615\t Accuracy: 508/711 (71%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0426, Accuracy: 142/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 34 [16/711 (2%)]\tLoss: 0.621560\t Accuracy: 13/711 (2%)\n",
      "Muti Train Epoch: 34 [176/711 (25%)]\tLoss: 0.656513\t Accuracy: 131/711 (18%)\n",
      "Muti Train Epoch: 34 [336/711 (47%)]\tLoss: 0.608353\t Accuracy: 263/711 (37%)\n",
      "Muti Train Epoch: 34 [496/711 (70%)]\tLoss: 0.636646\t Accuracy: 390/711 (55%)\n",
      "Muti Train Epoch: 34 [656/711 (92%)]\tLoss: 0.640608\t Accuracy: 515/711 (72%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0427, Accuracy: 142/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 35 [16/711 (2%)]\tLoss: 0.594040\t Accuracy: 15/711 (2%)\n",
      "Muti Train Epoch: 35 [176/711 (25%)]\tLoss: 0.614895\t Accuracy: 133/711 (19%)\n",
      "Muti Train Epoch: 35 [336/711 (47%)]\tLoss: 0.662341\t Accuracy: 257/711 (36%)\n",
      "Muti Train Epoch: 35 [496/711 (70%)]\tLoss: 0.683626\t Accuracy: 382/711 (54%)\n",
      "Muti Train Epoch: 35 [656/711 (92%)]\tLoss: 0.592680\t Accuracy: 513/711 (72%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0417, Accuracy: 142/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 36 [16/711 (2%)]\tLoss: 0.617185\t Accuracy: 12/711 (2%)\n",
      "Muti Train Epoch: 36 [176/711 (25%)]\tLoss: 0.597306\t Accuracy: 144/711 (20%)\n",
      "Muti Train Epoch: 36 [336/711 (47%)]\tLoss: 0.633891\t Accuracy: 274/711 (39%)\n",
      "Muti Train Epoch: 36 [496/711 (70%)]\tLoss: 0.674419\t Accuracy: 388/711 (55%)\n",
      "Muti Train Epoch: 36 [656/711 (92%)]\tLoss: 0.587811\t Accuracy: 517/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0417, Accuracy: 142/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 37 [16/711 (2%)]\tLoss: 0.600350\t Accuracy: 14/711 (2%)\n",
      "Muti Train Epoch: 37 [176/711 (25%)]\tLoss: 0.612287\t Accuracy: 139/711 (20%)\n",
      "Muti Train Epoch: 37 [336/711 (47%)]\tLoss: 0.642723\t Accuracy: 264/711 (37%)\n",
      "Muti Train Epoch: 37 [496/711 (70%)]\tLoss: 0.610980\t Accuracy: 390/711 (55%)\n",
      "Muti Train Epoch: 37 [656/711 (92%)]\tLoss: 0.638579\t Accuracy: 515/711 (72%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0415, Accuracy: 141/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 38 [16/711 (2%)]\tLoss: 0.646582\t Accuracy: 12/711 (2%)\n",
      "Muti Train Epoch: 38 [176/711 (25%)]\tLoss: 0.576721\t Accuracy: 146/711 (21%)\n",
      "Muti Train Epoch: 38 [336/711 (47%)]\tLoss: 0.656534\t Accuracy: 271/711 (38%)\n",
      "Muti Train Epoch: 38 [496/711 (70%)]\tLoss: 0.643079\t Accuracy: 397/711 (56%)\n",
      "Muti Train Epoch: 38 [656/711 (92%)]\tLoss: 0.669473\t Accuracy: 520/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0424, Accuracy: 141/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 39 [16/711 (2%)]\tLoss: 0.616043\t Accuracy: 13/711 (2%)\n",
      "Muti Train Epoch: 39 [176/711 (25%)]\tLoss: 0.605453\t Accuracy: 145/711 (20%)\n",
      "Muti Train Epoch: 39 [336/711 (47%)]\tLoss: 0.652593\t Accuracy: 270/711 (38%)\n",
      "Muti Train Epoch: 39 [496/711 (70%)]\tLoss: 0.605995\t Accuracy: 399/711 (56%)\n",
      "Muti Train Epoch: 39 [656/711 (92%)]\tLoss: 0.629781\t Accuracy: 518/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0411, Accuracy: 141/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 40 [16/711 (2%)]\tLoss: 0.618126\t Accuracy: 13/711 (2%)\n",
      "Muti Train Epoch: 40 [176/711 (25%)]\tLoss: 0.592732\t Accuracy: 145/711 (20%)\n",
      "Muti Train Epoch: 40 [336/711 (47%)]\tLoss: 0.597901\t Accuracy: 284/711 (40%)\n",
      "Muti Train Epoch: 40 [496/711 (70%)]\tLoss: 0.590945\t Accuracy: 403/711 (57%)\n",
      "Muti Train Epoch: 40 [656/711 (92%)]\tLoss: 0.642555\t Accuracy: 523/711 (74%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0409, Accuracy: 141/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 41 [16/711 (2%)]\tLoss: 0.637378\t Accuracy: 11/711 (2%)\n",
      "Muti Train Epoch: 41 [176/711 (25%)]\tLoss: 0.572991\t Accuracy: 144/711 (20%)\n",
      "Muti Train Epoch: 41 [336/711 (47%)]\tLoss: 0.641032\t Accuracy: 267/711 (38%)\n",
      "Muti Train Epoch: 41 [496/711 (70%)]\tLoss: 0.599593\t Accuracy: 403/711 (57%)\n",
      "Muti Train Epoch: 41 [656/711 (92%)]\tLoss: 0.679554\t Accuracy: 525/711 (74%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0408, Accuracy: 141/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 42 [16/711 (2%)]\tLoss: 0.593611\t Accuracy: 13/711 (2%)\n",
      "Muti Train Epoch: 42 [176/711 (25%)]\tLoss: 0.653148\t Accuracy: 133/711 (19%)\n",
      "Muti Train Epoch: 42 [336/711 (47%)]\tLoss: 0.575700\t Accuracy: 267/711 (38%)\n",
      "Muti Train Epoch: 42 [496/711 (70%)]\tLoss: 0.606740\t Accuracy: 399/711 (56%)\n",
      "Muti Train Epoch: 42 [656/711 (92%)]\tLoss: 0.570475\t Accuracy: 521/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0407, Accuracy: 141/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 43 [16/711 (2%)]\tLoss: 0.626941\t Accuracy: 11/711 (2%)\n",
      "Muti Train Epoch: 43 [176/711 (25%)]\tLoss: 0.680263\t Accuracy: 130/711 (18%)\n",
      "Muti Train Epoch: 43 [336/711 (47%)]\tLoss: 0.655064\t Accuracy: 253/711 (36%)\n",
      "Muti Train Epoch: 43 [496/711 (70%)]\tLoss: 0.613259\t Accuracy: 384/711 (54%)\n",
      "Muti Train Epoch: 43 [656/711 (92%)]\tLoss: 0.553687\t Accuracy: 522/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0408, Accuracy: 141/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 44 [16/711 (2%)]\tLoss: 0.686414\t Accuracy: 9/711 (1%)\n",
      "Muti Train Epoch: 44 [176/711 (25%)]\tLoss: 0.638872\t Accuracy: 144/711 (20%)\n",
      "Muti Train Epoch: 44 [336/711 (47%)]\tLoss: 0.594989\t Accuracy: 271/711 (38%)\n",
      "Muti Train Epoch: 44 [496/711 (70%)]\tLoss: 0.600078\t Accuracy: 395/711 (56%)\n",
      "Muti Train Epoch: 44 [656/711 (92%)]\tLoss: 0.626377\t Accuracy: 520/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0406, Accuracy: 141/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 45 [16/711 (2%)]\tLoss: 0.555654\t Accuracy: 14/711 (2%)\n",
      "Muti Train Epoch: 45 [176/711 (25%)]\tLoss: 0.555158\t Accuracy: 142/711 (20%)\n",
      "Muti Train Epoch: 45 [336/711 (47%)]\tLoss: 0.620895\t Accuracy: 260/711 (37%)\n",
      "Muti Train Epoch: 45 [496/711 (70%)]\tLoss: 0.532045\t Accuracy: 396/711 (56%)\n",
      "Muti Train Epoch: 45 [656/711 (92%)]\tLoss: 0.646219\t Accuracy: 520/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0411, Accuracy: 141/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 46 [16/711 (2%)]\tLoss: 0.554551\t Accuracy: 15/711 (2%)\n",
      "Muti Train Epoch: 46 [176/711 (25%)]\tLoss: 0.575527\t Accuracy: 145/711 (20%)\n",
      "Muti Train Epoch: 46 [336/711 (47%)]\tLoss: 0.610883\t Accuracy: 274/711 (39%)\n",
      "Muti Train Epoch: 46 [496/711 (70%)]\tLoss: 0.610403\t Accuracy: 399/711 (56%)\n",
      "Muti Train Epoch: 46 [656/711 (92%)]\tLoss: 0.629142\t Accuracy: 528/711 (74%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0410, Accuracy: 141/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 47 [16/711 (2%)]\tLoss: 0.568640\t Accuracy: 15/711 (2%)\n",
      "Muti Train Epoch: 47 [176/711 (25%)]\tLoss: 0.626957\t Accuracy: 139/711 (20%)\n",
      "Muti Train Epoch: 47 [336/711 (47%)]\tLoss: 0.652089\t Accuracy: 266/711 (37%)\n",
      "Muti Train Epoch: 47 [496/711 (70%)]\tLoss: 0.598880\t Accuracy: 387/711 (54%)\n",
      "Muti Train Epoch: 47 [656/711 (92%)]\tLoss: 0.575080\t Accuracy: 517/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0404, Accuracy: 140/178 (79%)\n",
      "\n",
      "Muti Train Epoch: 48 [16/711 (2%)]\tLoss: 0.615018\t Accuracy: 13/711 (2%)\n",
      "Muti Train Epoch: 48 [176/711 (25%)]\tLoss: 0.663165\t Accuracy: 137/711 (19%)\n",
      "Muti Train Epoch: 48 [336/711 (47%)]\tLoss: 0.555052\t Accuracy: 265/711 (37%)\n",
      "Muti Train Epoch: 48 [496/711 (70%)]\tLoss: 0.619228\t Accuracy: 398/711 (56%)\n",
      "Muti Train Epoch: 48 [656/711 (92%)]\tLoss: 0.588838\t Accuracy: 519/711 (73%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0402, Accuracy: 143/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 49 [16/711 (2%)]\tLoss: 0.602553\t Accuracy: 14/711 (2%)\n",
      "Muti Train Epoch: 49 [176/711 (25%)]\tLoss: 0.543145\t Accuracy: 140/711 (20%)\n",
      "Muti Train Epoch: 49 [336/711 (47%)]\tLoss: 0.644984\t Accuracy: 264/711 (37%)\n",
      "Muti Train Epoch: 49 [496/711 (70%)]\tLoss: 0.563310\t Accuracy: 393/711 (55%)\n",
      "Muti Train Epoch: 49 [656/711 (92%)]\tLoss: 0.621150\t Accuracy: 514/711 (72%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0406, Accuracy: 142/178 (80%)\n",
      "\n",
      "Muti Train Epoch: 50 [16/711 (2%)]\tLoss: 0.568726\t Accuracy: 13/711 (2%)\n",
      "Muti Train Epoch: 50 [176/711 (25%)]\tLoss: 0.576017\t Accuracy: 136/711 (19%)\n",
      "Muti Train Epoch: 50 [336/711 (47%)]\tLoss: 0.575273\t Accuracy: 270/711 (38%)\n",
      "Muti Train Epoch: 50 [496/711 (70%)]\tLoss: 0.611666\t Accuracy: 389/711 (55%)\n",
      "Muti Train Epoch: 50 [656/711 (92%)]\tLoss: 0.520199\t Accuracy: 510/711 (72%)\n",
      "\n",
      "Multi Test set: Average loss: 0.0401, Accuracy: 141/178 (79%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test()\n",
    "test_multi()\n",
    "\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "multi_train_accuracy_list = []\n",
    "multi_test_accuracy_list = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # train_accuracy_list.append(train(epoch))\n",
    "    # test_accuracy_list.append(test())\n",
    "\n",
    "    multi_train_accuracy_list.append(train_multi(epoch))\n",
    "    multi_test_accuracy_list.append(test_multi())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_accuracy_list, color='blue')\n",
    "plt.plot(test_accuracy_list, color='red')\n",
    "plt.plot(multi_train_accuracy_list, color='orange')\n",
    "plt.plot(multi_test_accuracy_list, color='green')\n",
    "\n",
    "plt.legend(['Train Loss', 'Test Loss', 'Mutli Train Loss', 'Mutli Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen (counter)')\n",
    "plt.ylabel('negative log likelihood loss (losses)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_neuron = 3\n",
    "num_hidden_layers = 0\n",
    "\n",
    "for i in range(multi_network): \n",
    "    network = TwoLayerNet(D_in, H, D_out)  # H=3 for one hidden layer with 3 neurons\n",
    "    multi_network = MultiLayerNet(D_in, multi_neuron, D_out, num_hidden_layers) \n",
    "\n",
    "    # optimizer = optim.Adam(network.parameters(), lr)  # RMSProp + Momentum \n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr)\n",
    "    multi_optimizer = torch.optim.SGD(multi_network.parameters(), lr)\n",
    "\n",
    "    criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss\n",
    "    multi_criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss\n",
    "\n",
    "    n_epochs = 50 # You can adjust the number of epochs as needed\n",
    "    log_interval = 10 # Print the training status every log_interval epoch\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
