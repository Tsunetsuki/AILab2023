{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on: https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "Go there for input optimizations and practical use tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816a791dbf7a455490461fa039888ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Le\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4334efbebac14727be4a4e25941c6e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd78610fb0ba471e9dbae64fef7a2ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ec9525d8094a6d8bc5d394efd78300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because BERT is a pretrained model that expects input data in a specific format, we will need:\n",
    "\n",
    "    A special token, [SEP], to mark the end of a sentence, or the separation between two sentences\n",
    "    A special token, [CLS], at the beginning of our text. This token is used for classification tasks, but BERT expects it no matter what your application is.\n",
    "    Tokens that conform with the fixed vocabulary used in BERT\n",
    "    The Token IDs for the tokens, from BERTâ€™s tokenizer\n",
    "    Mask IDs to indicate which elements in the sequence are tokens and which are padding elements\n",
    "    Segment IDs used to distinguish different sentences\n",
    "    Positional Embeddings used to show token position within the sequence\n",
    "The transformers interface can do all of this, this example is the manual version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the BERT Tokenizer, the WordPiece model generated a vocabulary that contains all English characters plus the ~30,000 most common words and subwords found in the English language corpus the model is trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "       \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only needed when 2 sentences are passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144c24df3909477190db98b979f04f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 219,648 unique values just to represent our one sentence!\n",
    " Calculated by multiplying these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 22\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing range of values for given layer and token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 5\n",
    "layer_i = 5\n",
    "vec = hidden_states[layer_i][batch_i][token_i]\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation of values:\n",
    "\n",
    "Current dimensions:\n",
    "\n",
    "[# layers, # batches, # tokens, # features]\n",
    "\n",
    "Desired dimensions:\n",
    "\n",
    "[# tokens, # layers, # features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Type of hidden_states:  <class 'tuple'>\n",
      "Tensor shape for each layer:  torch.Size([1, 22, 768])\n"
     ]
    }
   ],
   "source": [
    "# `hidden_states` is a Python list.\n",
    "print('      Type of hidden_states: ', type(hidden_states))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining layers to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 22, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 13, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These hidden states should now be usable embeddings\n",
    "\n",
    "Use examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof that embeddings are contextual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "  print (i, token_str)\n",
    "\n",
    "# bank is 6, 10, 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    tensor([ 3.3596, -2.9805, -1.5421,  0.7065,  2.0031])\n",
      "bank robber   tensor([ 2.7359, -2.5577, -1.3094,  0.6797,  1.6633])\n",
      "river bank    tensor([ 1.5266, -0.8895, -0.5152, -0.9298,  2.8334])\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs_sum[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs_sum[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs_sum[19][:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.94\n",
      "Vector similarity for *different* meanings:  0.69\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
