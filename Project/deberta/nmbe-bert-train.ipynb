{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start, the env setup \n",
    "1. use conda to create a new env\n",
    "```bash\n",
    "    conda create -n nmbe python=3.7\n",
    "    conda activate nmbe\n",
    "```\n",
    "\n",
    "2. install the following packages\n",
    "```bash\n",
    "    pip install torch==1.10.0\n",
    "    pip install sacremoses==0.0.41\n",
    "    pip install transformers\n",
    "```\n",
    "```bash if using cuda\n",
    "    pip install torch==1.10.0.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    pip install sacremoses==0.0.41\n",
    "    pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # About this notebook\n",
    " - Deberta-base starter code\n",
    " - pip wheels is [here](https://www.kaggle.com/yasufuminakama/nbme-pip-wheels)\n",
    " - Inference notebook is [here](https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-inference)\n",
    "\n",
    " If this notebook is helpful, feel free to upvote :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "root = '.'\n",
    "OUTPUT_DIR = root + '/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG (Classifier Free Guidance?)\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    competition='NBME'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model=\"bert-base-uncased\"\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=5\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=12\n",
    "    fc_dropout=0.2\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    # possible to simply delete folds that are already trained\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.13.3\n",
      "transformers.__version__: 4.16.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import ast\n",
    "import copy\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from torch.nn import Parameter\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.system('pip uninstall -y transformers')\n",
    "os.system('python -m pip install --no-index --find-links=./input/nbme-pip-wheels transformers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import (AutoConfig, AutoModel, AutoTokenizer,\n",
    "                          get_cosine_schedule_with_warmup,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = \"cpu\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incompatible models lead to NaN gradients, this is the case for CUDA version of this model with 0 workers in DataLoaders\n",
    "use_scaler = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train_bert'):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed) if torch.cuda.is_available() else torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (14300, 6)\n",
      "features.shape: (143, 3)\n",
      "patient_notes.shape: (42146, 3)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/train.csv')\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval) # ['mom'] -> [mom]\n",
    "train['location'] = train['location'].apply(ast.literal_eval) # ['0 1'] -> [0, 1]\n",
    "features = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/features.csv')\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "features = preprocess_features(features)\n",
    "patient_notes = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "#display(train.head())\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "#display(features.head())\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "#display(patient_notes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "#display(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "#display(train['annotation_length'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=CFG.n_fold) # for loop for 5 rounds, val_index is a list of index of validation data, others are index of training data\n",
    "groups = train['pn_num'].values # make sure same patient does not appear in train & valid at the same time \n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)): # Fold.split(x, y, groups), each fold has same distribution of y\n",
    "    train.loc[val_index, 'fold'] = int(n) # add fold column, and fill with fold number which this row belongs to evaluation\n",
    "train['fold'] = train['fold'].astype(int) # validation data is len(train)/5 = 2860, each fold 1~5 have 2860 testing data and 11440 training data \n",
    "# display(train.groupby('fold').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    # display(train.groupby('fold').size())\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    # display(train.groupby('fold').size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42146 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42146/42146 [00:48<00:00, 876.43it/s]\n",
      "pn_history max(lengths): 312\n",
      "pn_history max(lengths): 312\n",
      "100%|██████████| 143/143 [00:00<00:00, 5727.57it/s]\n",
      "feature_text max(lengths): 29\n",
      "feature_text max(lengths): 29\n",
      "max_len: 344\n",
      "max_len: 344\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "for text_col in ['pn_history']:\n",
    "    pn_history_lengths = []\n",
    "    tk0 = tqdm(patient_notes[text_col].fillna(\"\").values, total=len(patient_notes)) # tqdm: progress bar \n",
    "    for text in tk0: # tk0 is a list of text which is text_col column of patient_notes \n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids']) # turn the sentence to index: len([1360, 12, 180, ...])\n",
    "        pn_history_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(pn_history_lengths)}')\n",
    "\n",
    "for text_col in ['feature_text']:\n",
    "    features_lengths = []\n",
    "    tk0 = tqdm(features[text_col].fillna(\"\").values, total=len(features))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids']) # 'Intermittent' -> ['In', 'term',...] -> [1360, 12, 180, ...]\n",
    "        features_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(features_lengths)}')\n",
    "\n",
    "CFG.max_len = max(pn_history_lengths) + max(features_lengths) + 3 # cls 開始 & sep 病例結果 & sep 特徵結尾 \n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text, feature_text):\n",
    "    inputs = cfg.tokenizer(text, feature_text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CFG.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items(): # k = \n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def create_label(cfg, text, annotation_length, location_list):\n",
    "    encoded = cfg.tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=CFG.max_len,\n",
    "                            padding=\"max_length\",\n",
    "                            return_offsets_mapping=True)\n",
    "    offset_mapping = encoded['offset_mapping'] # index of each token: (0, 0) (0, 1) (1, 2) (2, 3) (3, 4) ...\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0] # list all None token index\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1 # filled all None token index with = -1; [-1, 0, 0, ..., -1, -1, -1];  0 is no features, -1 is None token  \n",
    "    if annotation_length != 0:\n",
    "        for location in location_list: # location = '237 242;261 268' \n",
    "            for loc in [s.split() for s in location.split(';')]: # [['237', '242'], ['261', '268']] => loc = ['237', '242']\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]): # offset_mapping[idx] = (0, 0); start < 0 -> start_idx = 0 \n",
    "                        start_idx = idx - 1 # find start_ans is in the range of offset_mapping \n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]): \n",
    "                        end_idx = idx + 1 # label[start_idx:end_idx], the last token is not included, so end_idx + 1 \n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1 # each is token [-1, 0, 0, ..., 1, 1, ..., -1, -1, -1]; -1 is None token, 0 is no features, 1 is features\n",
    "    return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, \n",
    "                               self.pn_historys[item], \n",
    "                               self.feature_texts[item])\n",
    "        label = create_label(self.cfg, \n",
    "                             self.pn_historys[item], \n",
    "                             self.annotation_lengths[item], \n",
    "                             self.locations[item])\n",
    "        return inputs, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helpler functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    if use_scaler:\n",
    "        # standard growth factor: 2.0\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)  \n",
    "    losses = AverageMeter() # calculate the average loss of each batch, \n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        if torch.cuda.is_available():\n",
    "            with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "                y_preds = model(inputs) # [12, 458, 1]; batch is 12, max_len is 458 \n",
    "        else:\n",
    "            y_preds = model(inputs)\n",
    "        \n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean() # choose the loss of label != -1, and calculate the mean of all none -1 loss\n",
    "        \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        \n",
    "        losses.update(loss.item(), batch_size) # loss=0.1, batch_size=12, losses.avg = (0.1*12 + 0.2*12 ...)/(12*num_batch)\n",
    "\n",
    "        scaler.scale(loss).backward() if use_scaler else loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm) # constrain the gradient to be less than max_grad_norm, to avoid gradient explosion\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1 # global_step is including the epoch\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0 and use_scaler :\n",
    "             scaler.step(optimizer)\n",
    "             scaler.update()\n",
    "             optimizer.zero_grad()\n",
    "             global_step += 1\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        end = time.time()\n",
    "        #if step % CFG.print_freq == 0 or step == (len(train_loader)-1) and torch.cuda.is_available():\n",
    "        print('Epoch: [{0}][{1}/{2}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                'Grad: {grad_norm:.4f}  '\n",
    "                'LR: {lr:.8f}  '\n",
    "                .format(epoch+1, step, len(train_loader), \n",
    "                        remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                        loss=losses,\n",
    "                        grad_norm=grad_norm,\n",
    "                        lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        # if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "        print('EVAL: [{0}/{1}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                .format(step, len(valid_loader),\n",
    "                        loss=losses,\n",
    "                        remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True) # if fold = 0, then fold_index = 1, 2, 3, 4 are training data\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_texts = valid_folds['pn_history'].values\n",
    "    valid_labels = create_labels_for_scoring(valid_folds)\n",
    "    print(valid_labels)\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    # num_workers set to 0 because of an error in this specific torch version for windows\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    best_score = 0.\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n",
    "        \n",
    "        # scoring\n",
    "        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(valid_labels, preds)\n",
    "        print('valid labels, preds', valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else torch.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[622, 631]], [[633, 652]], [], [[76, 84], [171, 180]], [[254, 270]], [], [[389, 396]], [[284, 303]], [], [[85, 99], [126, 138], [126, 131], [143, 151]], [[64, 75], [187, 209]], [[0, 5]], [[6, 7]], [[483, 500]], [[455, 480]], [[177, 191]], [[124, 135]], [[701, 716]], [], [[555, 563]], [], [], [[41, 55]], [], [[8, 14]], [[15, 16]], [[905, 938], [905, 929]], [[879, 900]], [[267, 282], [172, 187]], [], [], [], [[413, 427]], [], [], [[79, 91]], [[103, 128]], [[22, 26]], [[45, 46]], [[666, 691]], [[636, 664]], [[384, 398]], [[120, 132]], [[359, 374]], [], [[571, 579]], [[376, 379]], [], [[133, 147]], [[154, 178], [199, 213]], [[88, 93]], [[94, 95]], [[621, 627], [632, 644]], [[594, 600], [606, 619]], [[282, 297]], [[51, 63], [155, 163]], [], [], [[473, 481], [528, 536], [832, 840]], [], [], [[64, 78], [64, 69], [83, 95]], [[37, 50]], [[13, 18]], [[19, 20]], [[511, 517], [522, 524]], [[484, 490], [495, 509]], [[240, 254]], [[27, 35]], [[220, 235]], [[277, 283], [310, 325]], [[441, 452]], [[214, 217]], [], [[36, 47]], [[48, 73]], [[5, 10]], [[11, 12]], [[689, 695], [716, 722]], [[732, 753]], [[288, 302]], [[93, 114]], [[327, 342]], [], [[418, 425], [800, 807]], [[283, 286]], [], [[41, 53]], [[63, 76]], [[19, 23]], [[23, 24]], [], [], [], [[37, 58], [160, 169]], [], [], [[497, 504]], [[285, 288], [673, 676]], [], [[98, 117], [98, 103], [123, 129]], [[16, 25]], [[0, 6]], [[7, 11]], [[677, 686]], [[655, 675]], [], [[51, 63], [202, 209], [389, 397]], [[530, 541]], [], [[187, 194], [359, 367], [480, 487]], [[522, 525]], [], [[64, 76]], [[77, 100]], [[24, 29]], [[30, 31]], [], [[566, 581]], [], [[155, 163]], [], [[346, 352], [389, 406]], [[300, 307], [476, 484]], [[233, 236]], [[262, 268], [273, 281]], [[34, 48]], [[49, 61], [167, 180]], [[0, 12]], [], [[372, 409], [659, 669]], [[671, 694]], [], [[38, 46], [96, 104], [190, 197]], [[259, 270]], [[535, 541], [604, 616], [535, 541], [629, 650]], [[444, 452]], [[251, 254]], [], [[48, 77]], [[26, 37], [105, 129]], [[0, 5]], [[6, 7]], [[693, 702]], [[667, 690]], [[274, 295]], [[47, 55], [251, 258]], [], [], [[323, 331], [436, 444]], [[300, 303]], [], [[16, 30], [56, 70]], [[71, 93]], [[4, 9]], [[10, 11]], [], [[651, 677]], [[224, 238]], [[153, 165], [182, 201]], [], [], [[560, 572]], [[244, 247]], [[249, 272], [274, 297]], [[50, 64], [110, 123]], [[65, 73]], [[0, 5]], [[6, 7]], [[883, 900]], [[862, 881]], [[425, 460]], [[105, 113], [350, 358]], [[189, 200]], [[678, 680], [735, 753]], [[616, 624], [831, 839]], [[494, 497]], [], [[46, 60], [232, 261], [264, 287]], [[65, 88]], [[23, 28]], [[29, 30]], [[590, 596], [603, 605]], [[566, 588]], [[198, 212]], [[91, 108]], [], [], [[348, 354]], [[217, 224]], [], [[25, 39], [25, 30], [44, 50]], [[52, 81], [61, 81]], [[0, 5]], [], [[675, 689]], [[641, 669]], [[312, 326]], [], [[331, 346]], [], [[459, 466], [712, 719]], [[165, 168]], [], [[37, 55]], [[25, 33]], [[0, 5]], [], [], [[597, 625]], [[186, 200]], [[79, 102], [128, 140]], [], [[267, 273], [353, 357], [367, 373]], [[520, 533]], [], [], [[37, 51]], [[59, 77]], [[0, 9]], [[10, 11]], [[629, 653]], [[655, 681]], [[183, 197]], [[39, 47], [99, 107]], [], [[452, 458], [478, 501], [452, 476]], [[692, 699]], [[203, 206]], [[264, 270], [281, 294]], [[48, 60]], [[61, 82]], [[5, 10]], [[11, 12]], [[728, 734], [740, 757]], [[695, 726]], [[314, 322], [333, 335], [354, 359]], [[172, 195], [196, 215]], [[511, 526]], [], [[645, 650], [660, 667]], [[489, 508]], [], [[82, 99]], [[122, 136]], [[26, 37]], [[38, 42]], [[845, 847], [855, 861]], [[814, 824], [832, 838]], [[274, 282]], [[149, 158], [327, 335]], [[309, 325]], [], [[435, 443]], [[284, 303]], [[528, 536], [549, 555], [528, 536], [564, 577]], [[69, 83], [260, 272]], [[124, 147]], [[19, 24]], [[25, 29]], [[206, 218]], [], [[529, 563]], [], [[0, 6]], [], [[265, 306]], [[82, 114]], [[219, 229]], [], [[445, 460]], [[42, 55]], [[7, 8]], [], [], [[854, 887]], [[416, 424]], [[22, 27]], [[406, 424]], [[295, 329]], [[168, 171]], [[461, 467], [518, 525], [461, 467], [527, 536]], [[437, 455]], [[809, 836]], [[122, 145]], [[28, 29]], [], [], [[790, 823], [753, 768]], [[220, 248]], [[8, 15]], [[209, 228]], [[374, 416]], [[61, 65], [129, 132]], [], [[253, 269]], [[545, 560]], [[151, 165]], [[20, 26]], [], [], [[516, 553]], [[340, 357]], [[0, 4]], [[340, 348], [367, 375], [377, 386]], [[195, 230], [421, 468]], [[45, 53]], [[637, 643], [673, 682], [637, 643], [684, 691]], [[275, 286]], [[614, 629]], [[49, 53], [55, 61]], [[23, 24]], [], [[277, 291], [568, 578]], [[753, 785]], [[328, 336]], [[10, 16]], [[328, 336], [351, 359]], [], [[70, 84]], [[523, 541]], [[294, 305]], [], [[87, 106]], [[17, 23]], [[822, 824], [907, 924]], [[534, 547], [549, 571]], [[717, 751]], [[317, 329], [348, 356], [422, 441], [451, 459], [470, 486]], [[0, 4]], [[385, 393], [404, 412]], [[422, 446], [470, 486]], [[22, 30]], [[822, 824], [889, 905]], [], [[800, 811], [812, 815]], [[45, 59]], [[4, 5]], [], [], [[843, 864]], [[281, 309]], [[27, 38]], [[315, 320], [354, 367]], [], [], [], [[435, 462]], [[758, 769]], [[73, 101]], [[39, 45]], [[368, 392]], [[333, 344]], [[668, 697]], [[277, 285]], [[13, 17]], [], [[195, 240]], [[37, 62]], [[413, 419], [458, 465], [413, 419], [469, 478]], [], [[644, 659]], [[67, 96]], [[18, 19]], [], [[517, 528]], [[657, 685]], [[475, 489], [499, 509]], [[2, 9]], [], [[342, 349], [363, 379], [381, 393], [403, 418]], [[151, 187]], [], [], [[585, 618]], [[96, 116]], [[14, 20]], [], [], [], [[231, 239], [251, 276], [382, 390], [391, 406]], [[5, 10]], [[278, 293], [278, 280], [295, 301]], [[345, 377], [391, 406]], [], [[554, 562], [585, 602], [554, 562], [585, 592], [603, 607], [554, 562], [608, 615]], [[534, 552]], [[831, 846]], [[42, 62]], [[11, 17]], [], [], [[515, 527]], [[341, 349], [351, 365]], [[0, 5]], [], [], [[36, 71]], [], [[220, 238]], [[497, 514]], [[76, 86]], [[6, 7]], [], [[285, 296]], [], [[260, 268]], [[0, 7]], [], [[101, 137]], [[19, 22]], [], [[298, 316]], [[433, 452]], [[26, 48]], [[12, 13]], [], [[476, 497]], [[807, 826]], [[351, 359]], [[5, 10]], [[351, 359], [378, 386]], [[239, 286]], [], [], [[423, 434]], [[575, 612]], [[33, 59]], [[11, 12]], [], [], [[540, 560], [581, 593]], [[269, 277]], [[18, 22]], [[288, 308], [288, 295], [312, 318]], [[181, 208]], [[83, 101]], [[383, 397]], [[249, 267]], [[418, 433], [464, 488]], [[70, 82], [97, 101]], [[23, 24]], [], [], [[667, 702]], [[102, 124]], [[5, 9]], [], [[458, 500]], [[57, 65]], [], [[225, 237], [246, 283]], [[391, 401]], [[61, 93]], [[10, 11]], [], [], [[509, 544]], [[329, 353]], [[0, 5]], [], [[202, 242]], [], [[355, 361], [405, 428]], [], [[583, 598]], [[30, 70]], [[6, 7]], [], [], [[791, 822]], [], [[20, 25]], [], [[488, 545]], [[67, 89]], [], [], [[345, 362], [388, 399]], [[91, 111]], [[26, 27]], [], [[387, 398]], [], [[292, 316]], [[0, 5]], [[292, 300], [331, 344]], [[224, 252]], [[28, 46]], [[400, 406], [432, 457]], [[359, 377]], [[459, 474]], [[47, 58], [70, 85]], [[6, 7]], [], [], [[503, 541]], [[347, 355], [370, 389]], [[0, 11]], [[347, 369]], [[407, 454]], [[45, 60]], [], [], [[584, 599]], [[45, 49], [62, 85]], [[12, 18]], [], [], [], [[360, 387], [393, 405], [418, 424]], [[33, 38]], [[407, 424]], [], [[92, 117]], [[442, 444], [480, 487], [442, 444], [489, 501], [442, 444], [503, 518]], [[260, 268]], [[772, 787]], [[182, 204]], [[39, 45]], [[584, 630]], [], [[537, 541]], [[729, 744]], [[174, 189]], [[79, 96]], [], [], [[25, 26]], [], [], [[202, 213]], [[136, 171]], [[118, 130]], [[265, 287]], [], [[20, 22]], [[134, 160]], [], [[372, 375]], [[305, 320], [325, 335], [922, 937]], [[524, 539]], [[72, 89]], [[645, 672]], [], [], [], [[112, 132]], [[507, 518]], [[162, 186], [187, 209], [258, 296]], [[90, 110]], [[447, 459]], [], [[24, 30]], [[76, 102]], [], [], [[557, 584]], [[481, 496]], [[57, 74]], [], [], [[34, 40]], [], [], [[277, 288]], [[146, 222], [232, 263]], [], [[293, 301]], [], [[24, 28]], [[268, 286], [291, 303], [309, 316]], [[735, 749]], [[902, 905]], [[868, 883]], [[334, 349], [342, 349], [364, 387]], [[80, 87], [98, 108], [70, 87]], [], [], [[0, 2], [25, 31]], [], [], [[410, 421]], [[130, 163], [165, 188], [193, 214]], [[52, 69]], [[426, 434], [459, 468], [475, 481]], [], [[19, 24]], [], [], [], [[466, 481], [499, 511]], [[559, 574]], [[49, 66]], [[607, 618], [623, 627], [633, 653]], [], [[26, 32]], [], [], [[328, 339]], [[95, 114], [141, 173], [189, 222], [223, 263]], [[67, 87]], [[311, 323]], [], [[21, 25]], [[394, 423], [394, 415], [424, 440], [394, 410], [442, 453], [394, 410], [454, 467]], [], [], [], [], [[68, 85], [107, 124]], [], [], [[0, 2], [31, 37]], [], [[317, 337]], [[234, 245], [589, 600]], [[126, 157], [162, 178], [179, 207]], [[98, 106]], [[250, 258]], [], [[20, 25]], [[99, 117], [99, 109], [118, 124], [99, 109], [124, 135]], [], [], [[559, 569]], [[178, 185], [186, 193], [195, 224]], [[12, 38]], [], [], [[6, 7]], [], [], [], [], [[39, 48]], [], [], [[0, 5]], [], [[544, 573]], [[512, 515]], [[782, 816]], [], [[36, 53]], [[633, 706]], [], [[30, 31]], [], [], [[183, 193]], [[81, 122]], [[54, 74]], [[153, 159]], [[161, 177]], [[24, 26]], [], [], [[52, 55]], [[460, 475]], [[545, 560]], [[18, 34]], [], [[244, 286]], [], [], [], [], [[83, 104], [109, 141], [143, 174], [177, 187], [234, 238], [177, 187], [212, 229]], [[0, 14]], [], [], [], [[78, 84], [99, 118], [78, 84], [99, 104], [133, 151], [78, 84], [99, 104], [153, 171]], [], [], [[346, 356], [472, 487]], [[327, 345], [370, 384]], [[16, 32]], [], [], [[10, 11]], [], [], [[430, 441]], [[174, 181], [188, 211], [213, 221], [228, 240], [242, 272]], [[34, 40]], [[443, 451]], [], [[5, 9]], [[50, 72], [80, 87], [50, 64], [73, 87], [50, 64], [91, 103]], [[569, 572], [586, 596]], [], [[390, 405]], [], [[23, 40]], [], [], [[7, 8]], [], [], [[222, 233]], [[189, 203], [199, 216]], [[9, 22]], [[235, 243], [264, 276]], [], [[0, 6]], [[45, 64], [45, 54], [66, 76]], [], [[559, 562]], [[529, 539]], [[198, 213], [315, 330]], [[22, 39]], [], [], [[6, 7]], [], [[162, 174]], [[292, 303]], [[86, 94], [96, 121], [125, 160]], [], [[305, 313]], [], [[0, 5]], [[126, 193]], [], [], [[232, 262]], [], [[150, 193], [195, 212]], [], [], [[7, 11]], [], [[214, 230]], [], [], [], [[19, 27], [83, 124]], [], [[0, 6]], [[681, 706]], [], [[835, 838]], [[799, 814]], [], [[74, 102]], [[286, 317]], [], [[0, 3], [27, 33]], [], [[209, 247]], [[395, 406]], [[131, 152], [157, 202]], [[103, 106], [117, 124]], [], [], [[21, 26]], [], [], [], [[397, 412]], [[241, 256]], [[46, 63], [65, 92]], [], [], [[7, 13]], [], [], [], [[134, 150]], [[28, 45], [93, 104]], [], [], [[0, 6]], [[55, 81], [55, 65], [74, 95], [55, 65], [74, 81], [97, 111], [55, 65], [74, 81], [113, 126]], [], [], [[642, 657]], [], [[13, 29], [164, 191], [164, 181], [193, 206]], [], [], [[7, 8]], [], [], [[275, 286]], [[208, 227]], [[30, 51], [140, 162]], [], [[291, 314], [316, 329], [331, 345]], [[0, 6]], [[103, 147], [148, 178]], [], [], [[559, 574]], [[249, 264]], [[84, 101]], [], [], [[32, 38]], [], [], [[213, 224]], [], [[59, 83]], [[229, 237]], [], [[20, 22]], [[376, 402]], [], [[509, 512]], [[477, 491]], [[454, 469]], [[21, 49], [58, 84], [376, 382], [407, 424]], [], [], [[10, 11]], [], [[338, 354]], [[154, 165]], [], [[12, 20]], [[167, 175]], [[192, 205], [206, 212]], [[0, 9]], [[569, 579], [580, 609], [569, 587], [614, 627], [569, 587], [628, 665]], [], [], [[914, 929]], [[378, 393]], [[43, 65], [94, 102], [113, 127], [103, 127]], [], [], [[27, 28], [0, 3]], [], [], [[409, 420]], [[136, 192], [231, 240], [246, 251], [260, 269], [275, 280], [290, 315]], [[67, 83], [67, 70], [76, 83]], [[436, 448]], [], [[15, 26]], [], [], [[455, 458]], [[414, 429]], [[228, 243]], [[33, 50]], [], [], [[23, 27]], [], [[714, 730]], [[202, 213]], [[63, 95], [96, 120]], [[51, 61]], [[215, 223]], [], [[0, 2]], [[57, 79]], [[561, 591]], [[776, 779]], [], [], [[36, 53], [151, 170]], [], [], [[6, 7]], [], [], [[349, 360]], [[172, 235], [262, 280], [237, 260]], [[18, 35]], [[318, 326], [365, 373]], [], [[0, 2]], [[922, 945]], [[69, 84]], [[503, 526]], [[664, 670]], [[61, 68]], [[579, 597]], [[202, 280]], [[798, 812]], [[25, 26]], [[85, 103]], [[393, 433]], [], [], [[292, 335]], [[134, 142]], [[13, 24]], [[604, 639]], [[232, 259]], [[310, 322]], [[386, 401]], [[197, 216]], [], [], [[730, 746]], [[6, 7]], [[70, 90]], [], [], [], [[349, 378]], [[436, 442]], [[0, 5]], [[624, 657]], [[64, 82]], [[274, 294]], [[451, 458]], [[42, 49]], [[255, 263], [573, 581]], [], [[538, 560]], [[0, 2], [20, 24]], [], [], [], [], [[131, 139], [163, 181]], [[248, 254]], [[14, 19]], [[582, 606]], [[12, 45]], [[389, 400]], [[544, 550]], [[104, 111]], [], [], [], [[6, 7]], [[46, 62]], [], [[395, 414]], [[67, 81], [83, 102]], [[140, 186]], [[252, 261]], [[0, 5]], [[896, 920]], [[69, 84]], [[557, 576]], [[459, 465], [708, 714]], [], [], [[133, 146]], [[843, 862]], [[0, 3], [29, 38]], [[50, 65]], [], [[578, 592]], [[95, 107]], [[272, 302], [192, 202], [236, 254]], [[467, 473]], [], [[658, 681]], [[43, 58]], [[465, 476]], [[636, 642]], [[110, 122]], [], [[65, 77], [160, 179], [193, 208]], [[726, 744]], [[12, 16]], [[26, 39]], [], [], [], [], [[256, 262]], [[0, 11]], [[903, 930]], [[22, 37]], [[259, 271]], [[637, 643]], [[181, 188]], [[310, 330], [826, 834]], [[74, 88], [93, 111]], [[768, 784]], [[6, 7]], [[60, 72]], [[133, 165]], [[503, 509], [536, 547]], [[167, 179]], [[449, 501]], [[53, 59]], [[0, 5]], [[489, 520]], [[105, 132]], [[175, 194]], [[324, 339], [528, 537]], [[89, 104]], [], [[25, 34], [50, 85]], [], [[4, 5]], [[6, 21]], [], [[210, 247]], [], [[276, 300]], [], [[0, 4]], [[668, 694]], [[102, 117], [195, 199], [229, 243]], [[345, 358]], [[479, 491], [598, 604]], [], [[297, 310]], [[152, 171]], [[727, 731], [747, 756]], [[0, 3], [31, 32]], [[118, 130]], [], [[380, 403]], [], [], [[286, 292]], [[24, 30]], [[798, 818]], [[68, 83]], [[525, 536]], [[782, 788]], [[52, 67]], [[578, 596]], [[205, 227], [254, 276]], [[923, 933]], [[6, 7]], [[228, 241]], [[492, 523]], [], [[84, 98]], [[320, 382]], [[470, 476]], [[0, 5]], [[536, 561]], [[21, 46]], [], [], [[111, 130]], [], [[390, 404]], [[614, 629]], [[6, 7]], [[52, 72]], [[270, 317]], [], [[96, 108]], [[202, 206], [212, 243]], [], [[0, 5]], [[801, 833]], [[144, 148], [170, 188]], [[434, 451]], [[666, 672]], [[261, 268]], [], [[235, 246]], [], [[29, 33], [119, 120]], [[205, 222]], [], [], [], [[301, 353]], [[371, 377]], [[17, 28], [111, 117]], [], [], [[151, 169]], [[331, 342], [401, 407]], [[60, 75]], [], [], [], [[5, 6]], [[40, 58]], [], [], [[77, 89]], [[108, 144]], [[172, 178]], [[0, 4]], [[873, 904]], [[45, 60]], [[335, 350], [352, 358]], [[680, 686]], [], [[424, 448]], [[171, 198]], [[780, 802]], [[10, 14]], [[74, 87]], [[92, 108]], [], [[223, 237]], [[246, 293]], [[413, 419]], [[4, 9]], [[692, 713]], [], [[487, 509]], [[937, 943]], [], [[294, 322]], [[66, 93]], [[773, 792]], [[7, 8]], [[25, 37]], [], [[497, 509], [539, 553]], [[149, 168]], [[118, 147]], [[443, 449]], [[0, 6]], [], [[48, 63]], [[334, 349]], [[459, 472], [714, 720]], [], [[312, 320]], [], [[541, 558]], [[22, 23]], [[69, 87]], [], [[370, 384]], [], [], [[428, 433]], [[19, 21]], [], [[14, 34]], [[308, 319]], [[380, 386]], [[109, 116]], [[154, 178]], [[68, 93]], [], [[5, 6]], [[43, 57]], [], [], [], [], [[299, 305]], [[0, 4]], [[594, 607]], [[65, 80]], [], [[539, 555]], [[196, 203]], [], [[91, 110], [131, 155]], [[630, 643]], [[6, 7]], [[49, 63]], [], [], [[116, 128]], [[220, 248]], [], [[0, 5]], [[433, 456], [546, 569]], [[91, 114]], [[142, 148], [327, 343]], [], [[175, 182]], [[120, 128]], [[69, 90]], [[597, 610]], [[35, 39]], [[56, 68]], [], [[381, 400]], [[245, 257]], [], [[307, 313]], [[10, 15]], [[812, 833]], [[124, 153]], [[400, 422]], [[733, 739]], [[116, 123]], [[491, 510]], [[177, 192]], [[889, 906]], [[21, 25]], [[83, 96]], [[567, 592]], [], [], [[258, 320]], [[345, 351]], [[13, 20]], [[494, 513], [494, 500], [515, 526], [494, 500], [554, 579], [494, 500], [581, 593], [494, 500], [632, 653]], [[49, 60]], [[443, 488]], [], [], [[494, 500], [528, 539], [494, 500], [541, 552]], [[233, 258]], [[29, 30]], [[210, 228]], [[17, 24]], [[365, 379], [365, 367], [393, 416]], [[86, 97], [142, 149]], [[218, 243]], [], [[365, 367], [434, 450], [365, 367], [452, 476]], [], [[326, 345]], [[27, 33]], [[347, 358]], [[15, 22]], [[615, 621], [642, 654]], [[47, 58], [415, 426], [553, 560]], [[195, 223], [195, 204], [269, 288]], [], [], [[615, 621], [629, 640]], [[455, 477]], [[19, 20]], [[530, 546]], [[15, 19]], [[423, 429], [439, 451]], [[52, 63], [95, 106]], [[779, 793], [821, 833], [756, 769]], [], [[376, 385], [542, 564], [542, 554], [568, 574]], [], [[324, 350]], [[11, 12]], [[256, 274]], [[5, 10]], [[323, 325], [375, 390], [323, 325], [375, 382], [395, 399], [323, 325], [401, 413], [323, 325], [415, 431], [323, 325], [433, 459]], [[12, 23]], [[148, 165], [179, 202]], [[289, 306]], [[323, 325], [461, 468], [469, 478], [323, 325], [480, 497], [499, 514]], [], [[41, 67]], [[6, 7]], [], [[0, 5]], [[512, 518], [531, 542]], [[33, 44], [105, 112]], [], [[243, 267]], [[389, 410], [389, 395], [412, 428], [389, 395], [430, 454]], [[512, 518], [554, 570]], [[277, 302], [307, 337]], [[6, 7]], [[588, 604]], [[0, 5]], [[251, 257], [270, 282], [251, 257], [289, 301], [251, 257], [303, 316]], [[17, 28]], [], [], [], [[320, 337]], [[141, 167]], [[11, 12]], [[118, 136]], [[5, 10]], [[798, 806], [829, 841], [798, 806], [886, 898], [798, 806], [900, 908]], [[48, 55], [761, 772]], [], [[433, 451]], [[798, 806], [941, 950]], [[798, 806], [910, 923]], [[410, 418]], [[25, 31]], [[375, 397]], [[13, 24]], [[534, 553], [581, 611]], [[76, 87], [265, 272]], [[102, 111], [128, 153]], [], [], [[385, 400]], [[283, 308]], [[20, 26]], [[332, 350]], [[15, 19]], [[492, 498], [525, 537], [492, 498], [598, 606], [492, 498], [616, 632], [492, 498], [608, 612], [621, 632], [492, 498], [634, 649], [492, 498], [634, 644], [650, 654], [492, 498], [634, 644], [658, 663]], [[43, 54]], [[232, 257], [275, 289], [896, 932]], [], [], [[492, 498], [576, 587], [492, 498], [576, 582], [591, 595]], [[373, 398], [426, 445]], [[14, 15]], [[322, 340], [446, 464]], [[8, 13]], [[247, 253], [278, 288], [300, 312]], [[18, 29]], [], [[228, 243]], [], [[247, 253], [261, 272]], [[158, 177]], [[11, 12]], [[210, 223]], [[5, 10]], [[456, 470], [456, 458], [472, 484], [544, 563], [576, 583]], [[4, 15], [40, 51], [145, 156], [810, 821]], [], [[428, 454]], [[397, 417]], [[381, 395]], [[317, 342]], [[25, 26]], [[353, 366]], [[22, 24]], [[346, 360], [346, 348], [362, 374], [346, 348], [411, 427], [809, 814], [826, 833], [768, 770]], [[20, 31], [131, 142], [480, 487]], [], [[502, 522]], [[346, 348], [392, 409]], [[346, 348], [431, 442]], [], [[7, 8]], [[461, 472]], [[0, 6]], [[507, 526], [507, 513], [528, 554], [507, 513], [577, 606]], [[50, 61]], [[391, 428], [391, 412], [430, 441]], [], [], [[507, 513], [611, 628]], [[166, 191]], [[25, 31]], [[305, 321]], [[13, 20]], [[551, 557], [567, 583], [551, 557], [562, 566], [572, 583], [551, 557], [597, 609], [551, 557], [632, 640], [551, 557], [645, 657]], [[72, 83], [88, 95], [112, 119], [328, 335], [429, 436]], [[242, 270], [309, 322]], [], [], [], [[520, 545]], [[25, 31]], [[497, 508]], [[13, 24]], [[482, 488], [525, 537], [482, 488], [563, 568], [580, 587]], [[16, 27]], [[872, 888], [908, 932]], [[784, 801]], [], [], [[201, 226]], [[5, 11]], [[233, 251]], [[0, 4]], [[354, 376], [354, 360], [423, 432], [442, 447]], [[13, 25], [136, 143]], [[136, 149], [170, 177]], [], [[354, 360], [378, 383], [354, 360], [405, 421], [354, 360], [423, 441], [354, 360], [385, 403]], [[354, 360], [423, 432], [461, 467]], [[320, 348]], [[7, 8]], [], [[0, 6]], [[548, 562], [548, 554], [593, 601], [548, 554], [593, 596], [602, 606], [548, 554], [608, 616], [548, 554], [618, 630], [548, 554], [632, 639]], [[56, 67], [147, 158], [406, 417]], [[256, 267], [304, 329]], [], [[466, 493]], [[548, 554], [575, 586], [548, 554], [575, 581], [587, 591]], [], [[21, 22]], [[442, 460]], [[17, 21]], [[119, 121], [136, 149], [284, 298]], [[9, 16]], [], [[192, 212]], [], [], [[92, 117]], [[3, 4]], [[264, 282]], [[0, 2]], [], [[62, 73], [210, 221], [299, 310]], [[434, 487]], [], [], [], [[362, 384]], [[27, 33]], [[341, 357]], [[15, 26]], [[297, 317]], [[29, 35]], [[924, 945]], [], [[78, 90]], [[216, 290]], [], [[773, 779], [798, 806]], [[571, 579], [580, 586], [591, 599]], [], [], [], [], [[166, 174], [189, 195]], [[496, 519]], [], [[383, 388], [398, 404], [417, 461]], [[17, 28]], [[48, 63]], [[12, 13]], [], [[240, 243], [300, 321]], [[30, 42]], [], [], [], [[245, 251], [300, 321]], [[77, 112]], [], [[118, 149]], [], [], [[276, 299]], [], [[471, 520]], [[6, 8]], [[307, 327]], [[31, 32]], [], [[557, 560], [569, 572]], [[398, 453], [186, 198], [360, 372]], [], [], [[849, 855], [877, 893]], [[557, 567]], [[485, 520]], [[205, 241]], [], [[557, 560], [574, 590]], [[592, 603], [608, 620]], [], [], [[796, 805]], [[17, 28]], [[348, 359]], [[27, 33]], [], [[451, 454], [517, 536]], [[95, 107], [335, 347]], [[122, 320]], [[543, 552]], [[711, 730]], [[456, 457], [517, 536]], [], [], [], [[459, 475], [517, 536]], [[509, 515], [481, 496]], [[403, 418]], [], [], [[15, 26]], [[113, 124], [113, 148], [169, 183]], [[14, 20], [10, 20]], [], [[283, 287], [300, 319], [265, 319]], [[99, 111], [29, 111]], [], [[513, 519], [553, 563]], [[860, 876]], [[283, 287], [321, 327]], [[226, 258], [222, 258]], [], [], [[283, 287], [354, 375]], [[260, 299], [380, 393], [380, 393]], [[329, 352], [260, 299], [329, 352]], [], [[781, 791]], [[7, 13], [7, 9]], [], [[12, 18]], [], [[103, 150]], [[68, 80]], [[238, 249], [331, 411]], [[550, 563]], [], [[103, 129], [152, 158]], [], [], [], [], [[163, 174], [179, 185]], [], [], [], [], [[44, 47], [53, 59]], [[19, 25]], [], [[344, 350], [366, 388]], [[33, 43], [110, 121], [272, 283], [351, 361], [430, 440]], [], [], [[568, 584]], [], [[68, 92]], [], [], [], [], [], [], [], [[11, 18]], [[64, 81]], [[19, 20]], [], [[223, 237], [331, 338]], [[51, 63]], [[340, 351], [398, 413], [418, 425], [457, 467], [340, 351], [398, 413], [436, 467]], [[489, 495], [549, 559]], [[802, 804], [818, 823]], [[223, 244]], [[94, 167]], [[291, 325]], [], [], [[262, 273], [283, 289]], [[373, 388]], [], [[611, 619], [623, 647]], [[13, 18]], [[82, 91]], [[11, 17]], [], [[253, 257], [285, 300]], [[27, 39]], [[923, 950], [167, 191], [908, 950], [182, 191]], [], [[478, 490], [504, 522]], [[253, 264]], [[93, 123]], [[321, 350]], [], [[253, 257], [266, 283]], [[313, 319]], [[224, 247]], [], [[617, 674]], [[0, 4]], [[80, 97]], [[6, 12]], [[741, 789]], [], [[62, 79]], [], [[299, 305], [310, 320], [460, 466], [508, 518]], [[661, 667], [689, 697]], [], [[99, 144]], [], [], [], [[236, 247], [258, 264]], [[171, 184], [188, 217]], [], [[835, 866], [801, 816]], [[0, 2]], [], [[21, 27]], [], [[144, 163]], [[53, 76]], [], [], [[625, 627], [633, 638]], [[144, 154], [218, 224]], [[274, 297]], [], [[107, 128]], [[144, 154], [165, 183]], [[185, 209], [211, 216]], [[242, 266]], [], [[349, 361], [375, 393]], [[12, 14]], [[40, 72]], [[6, 12]], [], [[196, 221]], [[26, 38]], [[452, 521]], [], [[653, 655], [665, 669]], [[196, 217], [242, 248]], [[77, 101]], [[291, 337]], [], [[196, 217], [223, 240]], [[196, 217], [254, 268]], [[165, 194]], [], [[751, 774]], [[0, 2]], [[57, 74]], [[6, 12]], [], [[348, 362]], [[230, 267], [44, 56]], [], [], [[840, 846], [864, 869]], [[348, 353], [364, 370]], [[88, 97], [113, 124], [81, 97], [128, 136], [143, 165]], [[301, 331]], [[199, 217]], [[348, 353], [372, 391]], [[403, 416], [393, 396], [409, 416]], [], [], [[471, 480]], [[0, 5]], [[132, 152]], [[0, 2], [23, 29]], [[578, 592], [593, 597], [610, 625]], [[351, 355], [362, 374], [378, 383], [424, 452], [453, 459]], [[68, 80], [362, 374], [542, 554]], [[286, 297], [303, 313], [337, 349]], [[723, 736]], [[919, 921], [940, 948]], [[351, 355], [362, 374], [378, 383], [385, 394], [479, 487]], [[132, 168], [173, 221]], [], [], [[351, 355], [362, 374], [378, 383], [404, 422]], [[465, 474], [519, 525]], [], [], [[664, 672], [700, 714]], [[16, 22]], [[109, 135]], [[14, 15]], [[472, 474], [484, 497]], [], [], [], [], [[472, 480]], [], [[144, 176]], [], [[181, 201]], [], [[62, 71]], [[52, 60], [85, 98]], [], [], [[8, 10]], [[144, 161], [167, 176], [144, 176]], [[3, 4], [0, 4]], [], [[58, 77], [5, 17], [58, 77]], [[32, 56], [5, 17], [32, 56]], [[539, 576], [517, 576]], [], [[638, 640], [652, 657]], [[95, 104]], [[204, 227], [192, 227]], [], [], [], [[79, 93], [110, 121], [5, 17], [79, 93], [5, 17], [110, 121]], [], [], [[390, 450]], [[0, 2], [0, 4]], [], [[27, 33]], [], [[224, 235], [252, 255]], [[95, 107]], [], [], [[507, 515], [532, 554]], [[224, 235], [288, 294]], [], [], [], [[224, 235], [257, 273]], [[275, 286]], [[300, 318]], [], [[140, 198]], [[15, 26]], [], [[18, 19]], [[885, 908]], [[513, 534]], [[161, 187]], [[35, 51], [62, 70], [89, 93], [112, 116], [129, 149]], [[573, 579], [647, 657]], [[771, 773], [783, 799]], [[513, 518], [536, 544]], [[276, 336]], [], [[351, 355], [366, 379]], [], [[555, 567]], [], [], [[419, 431], [439, 453]], [[13, 17]], [[56, 71]], [[2, 3]], [], [[84, 95], [110, 113]], [[96, 108]], [[235, 246], [350, 382], [383, 424]], [], [[839, 841], [858, 863]], [[84, 95], [133, 139]], [[186, 217], [186, 208], [235, 246]], [], [[457, 471]], [[84, 95], [115, 131]], [[145, 156], [145, 152], [167, 173]], [[276, 299]], [], [[530, 544], [558, 574], [827, 837]], [[0, 2]], [[52, 78]], [[10, 11]], [], [], [[38, 50], [117, 129]], [], [], [[397, 413]], [[181, 196], [218, 224]], [], [], [[160, 180]], [[181, 196], [226, 250]], [], [[181, 216]], [[252, 259], [264, 285]], [], [[2, 4]], [[373, 382]], [[7, 8]], [[0, 6]], [[391, 422]], [[164, 201]], [[501, 525]], [[33, 43]], [[97, 112]], [], [], [[269, 298]], [[128, 133], [177, 183]], [[392, 397]], [[2, 3]], [[0, 2]], [[378, 386], [406, 416]], [[163, 194]], [[17, 40]], [[59, 69], [112, 122]], [[201, 218]], [[529, 535]], [], [[270, 294]], [[138, 143]], [[470, 480]], [[23, 24]], [[16, 22]], [[399, 415]], [[193, 213]], [[37, 60], [739, 762]], [[88, 98]], [[79, 84]], [[519, 525], [612, 615]], [], [[231, 236], [256, 265], [274, 296]], [[150, 155], [160, 168], [349, 353]], [[448, 453]], [[44, 48]], [[15, 26]], [[394, 413]], [[263, 286]], [[80, 103], [601, 624]], [[133, 143], [171, 175], [191, 193], [215, 220]], [[122, 132]], [[481, 483], [533, 552]], [], [[344, 369]], [[164, 169], [177, 189]], [[259, 264]], [[19, 20]], [[13, 18]], [[246, 257]], [[86, 116]], [[500, 523]], [[67, 77]], [[37, 42]], [[361, 363], [397, 400]], [], [[187, 202], [214, 223]], [[150, 155], [160, 168]], [[379, 395]], [[20, 24]], [[14, 19]], [[424, 460]], [[189, 226], [228, 244], [258, 273]], [[681, 704]], [[58, 68], [93, 103]], [[69, 87]], [], [[934, 945]], [], [[161, 166], [189, 193]], [[268, 284]], [[28, 32]], [[16, 27]], [[286, 296]], [[169, 196]], [[585, 608]], [[51, 61]], [[67, 82]], [[308, 314], [334, 337]], [[540, 553], [563, 576]], [], [[149, 154], [155, 163]], [[346, 362]], [[21, 25]], [[15, 20]], [[330, 341]], [[121, 140]], [[490, 513]], [[42, 52]], [[75, 84]], [], [], [[218, 250]], [[99, 104]], [[377, 382]], [[17, 21]], [[12, 16]], [[402, 426]], [[134, 162], [298, 302], [315, 329]], [[529, 552]], [[72, 82]], [[40, 48], [243, 260], [269, 283]], [], [], [[177, 194], [206, 223]], [[106, 111], [112, 120], [333, 339]], [[475, 484]], [[23, 27]], [[15, 22]], [], [[203, 227]], [[59, 83], [603, 627]], [[121, 131]], [[161, 170]], [[489, 495], [527, 530]], [], [[387, 418]], [[178, 183], [184, 192]], [[316, 331]], [[20, 24]], [[15, 19]], [[264, 275]], [[64, 68], [126, 147]], [[523, 546]], [[32, 42]], [[43, 58]], [[379, 385], [393, 396]], [], [[197, 221]], [], [], [[6, 7]], [[0, 5]], [[553, 563], [578, 595]], [[205, 233], [317, 319], [332, 360]], [[626, 649]], [[28, 38], [103, 107], [131, 136]], [[41, 56]], [[384, 390]], [], [[267, 292]], [[176, 181], [183, 191]], [[222, 227]], [[6, 7]], [[0, 5]], [[188, 204], [228, 242]], [[114, 123], [132, 150], [153, 162], [173, 184]], [], [[14, 24], [70, 80]], [[25, 31]], [], [], [], [[82, 87], [114, 118]], [[472, 484]], [[22, 23]], [[16, 21]], [[460, 471], [507, 517]], [[254, 275]], [[698, 721]], [[57, 67], [157, 161], [194, 199], [548, 558]], [[74, 88]], [[592, 618]], [], [[345, 362], [421, 433]], [[151, 156], [241, 247], [248, 253]], [], [[6, 7]], [[0, 5]], [[247, 258], [276, 288]], [[104, 133]], [[552, 575]], [[26, 36]], [[146, 165]], [], [[775, 788]], [[187, 225]], [[38, 43]], [[345, 355]], [[22, 23]], [[16, 21]], [[174, 185]], [[49, 58], [272, 297]], [[503, 526]], [[67, 77]], [[114, 129]], [[360, 366], [389, 392]], [], [], [[42, 47]], [], [[11, 12]], [[5, 10]], [[435, 462]], [[154, 182]], [[736, 759]], [[35, 45]], [[69, 79]], [[247, 253], [267, 270]], [], [], [[100, 105]], [[211, 232]], [[6, 7]], [[0, 5]], [[193, 210]], [[101, 115], [130, 148]], [[485, 508]], [[20, 30]], [[35, 40]], [[316, 326]], [], [], [[71, 76]], [[155, 171]], [[23, 27]], [[16, 22]], [[374, 397]], [[107, 136]], [[614, 637]], [[67, 77]], [[84, 93]], [], [], [[198, 230]], [], [[419, 428]], [[6, 10]], [[0, 5]], [[73, 84]], [[183, 212]], [[534, 558]], [[24, 34]], [[40, 57]], [[321, 327], [359, 378]], [[295, 308]], [[219, 234], [246, 255]], [[143, 148]], [[0, 2], [20, 21]], [[506, 517]], [[89, 117], [122, 138], [368, 402]], [[330, 358]], [[940, 950]], [[556, 563]], [[630, 641]], [[15, 20]], [[61, 72], [280, 291], [530, 540]], [[6, 7]], [[383, 394]], [[43, 55], [92, 98], [114, 119], [173, 208]], [], [[896, 905], [909, 912]], [[364, 371]], [[412, 434]], [[0, 5]], [[69, 77], [403, 411]], [[12, 13]], [], [[18, 34], [66, 72], [91, 97], [54, 57], [120, 126], [54, 57], [131, 168]], [], [], [], [[269, 308]], [], [[39, 46]], [[7, 8]], [[458, 469]], [[36, 49], [178, 192], [601, 611], [225, 265], [613, 639]], [[583, 599]], [], [[426, 434]], [[527, 536], [543, 562]], [[0, 6]], [[50, 58]], [[29, 30]], [[707, 720]], [[53, 70], [47, 52], [63, 70], [163, 191]], [[214, 217], [232, 244]], [[511, 531]], [[697, 705]], [[381, 403], [434, 445]], [[17, 28]], [[84, 92]], [[12, 17]], [[687, 703]], [[32, 54], [56, 67], [292, 329], [338, 356]], [], [], [[387, 392]], [[427, 462]], [[0, 11]], [[186, 194], [714, 722]], [[6, 7]], [[414, 425]], [[36, 49], [216, 229], [231, 261]], [], [], [], [[622, 642], [655, 672]], [[0, 5]], [[24, 32], [164, 172], [440, 448]], [[51, 57]], [[592, 603], [599, 603], [605, 617]], [[104, 116], [243, 260]], [[144, 164]], [[759, 761], [767, 777]], [[583, 590]], [], [[45, 50]], [[130, 138], [196, 204], [623, 631]], [[6, 7]], [[331, 348]], [[33, 46], [118, 148], [154, 174]], [], [[797, 825]], [[246, 254]], [], [[0, 5]], [[24, 29]], [[27, 33]], [], [], [[196, 212]], [[450, 466]], [], [], [[20, 26]], [[101, 109]], [[12, 13]], [[315, 326], [322, 336]], [[28, 45], [151, 172], [260, 282]], [], [[916, 941]], [], [[528, 550]], [[0, 11]], [[138, 146]], [[0, 2]], [[342, 361]], [[44, 57], [77, 114]], [[59, 75]], [], [], [[274, 285]], [[21, 23]], [[25, 32], [374, 382]], [[0, 3], [50, 56]], [], [[151, 173], [182, 208]], [], [[271, 287], [853, 873]], [], [[311, 328], [387, 409], [636, 658]], [[19, 30]], [[125, 133]], [[11, 12]], [[293, 304]], [[27, 44], [17, 22], [37, 44], [62, 69], [92, 101], [245, 268]], [[120, 140]], [], [], [[443, 474]], [[6, 10]], [[102, 110], [313, 321]], [[6, 7]], [[274, 295]], [[36, 53], [135, 158]], [], [[599, 624]], [[236, 241]], [[325, 356], [368, 382]], [[0, 5]], [[60, 68], [304, 312]], [[11, 12]], [[410, 421]], [[32, 48], [76, 90]], [[814, 826]], [[754, 772]], [[251, 260]], [[355, 373]], [[5, 10]], [[51, 59], [192, 200], [434, 442]], [[24, 25]], [[553, 569]], [[44, 68], [113, 154], [186, 214], [244, 277]], [[160, 180]], [[362, 371], [384, 397]], [[643, 651]], [[426, 444], [525, 536], [458, 476], [497, 512]], [[18, 23]], [], [[6, 7]], [[436, 447]], [[21, 34], [59, 80], [112, 118], [170, 196]], [], [], [], [[487, 507]], [[0, 5]], [[37, 45]], [[11, 12]], [[251, 263]], [[17, 40], [59, 72], [74, 91], [113, 148]], [[150, 167]], [[668, 681]], [], [[569, 594]], [[5, 10]], [[45, 53]], [[7, 12]], [[376, 387], [544, 566]], [[27, 42], [73, 97], [115, 133], [200, 227], [208, 211], [250, 283]], [[135, 165]], [[741, 775]], [], [], [[0, 6]], [[56, 64]], [[364, 386]], [[41, 77]], [[14, 15]], [[392, 399], [447, 503]], [[113, 129]], [[8, 13]], [[86, 111]], [], [[41, 50]], [], [[215, 227], [243, 271]], [[315, 330], [335, 340], [558, 568]], [[529, 553]], [[392, 437]], [[891, 894], [905, 915]], [[146, 160]], [[574, 591], [642, 659]], [[32, 40], [162, 182]], [], [[137, 164]], [[6, 7]], [[357, 431]], [], [[0, 5]], [[56, 81]], [], [[19, 31]], [], [], [[104, 121]], [], [[227, 287], [296, 336]], [[767, 793]], [[83, 98]], [[461, 467], [477, 504]], [[35, 43]], [[366, 384]], [[90, 101], [108, 120]], [[0, 2], [21, 27]], [[386, 395], [407, 421]], [], [[15, 20]], [[153, 178]], [], [[28, 40], [82, 101]], [], [], [[348, 364]], [[337, 346]], [[400, 421]], [[784, 794], [744, 750]], [[200, 223]], [[464, 470], [503, 505]], [[44, 60], [62, 81], [251, 288]], [[469, 487]], [[190, 218]], [[6, 7]], [[360, 368], [380, 394]], [[140, 159]], [[0, 5]], [[71, 96]], [], [[49, 61]], [], [], [], [[313, 329]], [[373, 394]], [], [[98, 113]], [[411, 417], [425, 449]], [[24, 40], [261, 280]], [], [[107, 115], [123, 131]], [[7, 8]], [], [], [[0, 6]], [[45, 52], [71, 85]], [], [[30, 41]], [], [[264, 285]], [[340, 356]], [[320, 338]], [], [], [], [[406, 415]], [[45, 61], [45, 52], [90, 104]], [[260, 271]], [[171, 191]], [[41, 42]], [[282, 296]], [], [[38, 40]], [[93, 120]], [], [[81, 91]], [], [], [[390, 403]], [[217, 230]], [[301, 315]], [[622, 625], [630, 640]], [[142, 169]], [[374, 388]], [[4, 20], [56, 72], [93, 103], [122, 136]], [[403, 425]], [[31, 42], [53, 68]], [], [[186, 247]], [], [[0, 6]], [[78, 104]], [], [[29, 38]], [], [], [[257, 268]], [[293, 302]], [[156, 181]], [[592, 613]], [[106, 133]], [[427, 433], [490, 507]], [[13, 29]], [[262, 280]], [[52, 71]], [[6, 12]], [[322, 365]], [], [[0, 5]], [[83, 108]], [], [[22, 34]], [], [], [[312, 320]], [[251, 260], [282, 310]], [[370, 391]], [[820, 840]], [], [], [[38, 46], [83, 93], [113, 131], [133, 173]], [[364, 382]], [[86, 93], [131, 149]], [[0, 4], [22, 27]], [[516, 552]], [], [[16, 21]], [[191, 198], [204, 218]], [], [[73, 93]], [], [], [[427, 437]], [[384, 397]], [[466, 511]], [[801, 807], [823, 838]], [[251, 266]], [], [[56, 72], [114, 130], [191, 198], [223, 237], [275, 315]], [[402, 420]], [[64, 94]], [[5, 10]], [], [[210, 226]], [[0, 4]], [[36, 61], [133, 169]], [], [[20, 32]], [[236, 305]], [[565, 605]], [], [[509, 528]], [], [], [], [], [], [[360, 378]], [[295, 316]], [[7, 8]], [], [], [[0, 6]], [], [], [[51, 70]], [[264, 281]], [], [[226, 259]], [[444, 460]], [[408, 439]], [[845, 859]], [[165, 181]], [[486, 508]], [[33, 49], [148, 163]], [], [[88, 116]], [[5, 11]], [], [], [[0, 4]], [], [], [], [], [], [[372, 379], [390, 407]], [], [], [], [], [[449, 473]], [[49, 65]], [[405, 426]], [[44, 82]], [[8, 9]], [[315, 375]], [], [[5, 7]], [], [], [[39, 51]], [], [], [[210, 220]], [], [[290, 310]], [[920, 945]], [[182, 205]], [], [[14, 30], [137, 168]], [[179, 196]], [[68, 88]], [[6, 7]], [[295, 356]], [], [[0, 5]], [], [], [[41, 50]], [], [[390, 445]], [[55, 62], [167, 177]], [[129, 145]], [[265, 290]], [], [[112, 127]], [[235, 255]], [[24, 40]], [], [[80, 115]], [[8, 14]], [[433, 460]], [], [[2, 7]], [[271, 296]], [], [], [], [[325, 337], [349, 366]], [[699, 706]], [[678, 694]], [], [[933, 947]], [], [], [[36, 55], [197, 261], [271, 281], [301, 319]], [], [[29, 58]], [[19, 24]], [], [], [[13, 18]], [], [], [[98, 106]], [], [], [], [], [[320, 378]], [[596, 617]], [[198, 214]], [[256, 262], [288, 304]], [[81, 97], [159, 176], [226, 250]], [[348, 370]], [[144, 171]], [[7, 8]], [[238, 261]], [], [[4, 6]], [[59, 88]], [], [[45, 57]], [], [], [[193, 207]], [[278, 294]], [[227, 233], [247, 261]], [[504, 510], [532, 559]], [[93, 109]], [], [[25, 44]], [[125, 136]], [[91, 116]], [[7, 8]], [], [], [[9, 14]], [], [], [[41, 60]], [], [[192, 198], [213, 225]], [], [], [], [[341, 357]], [], [], [[21, 38], [62, 89]], [[305, 323]], [[70, 89], [123, 135]], [[6, 7]], [], [], [[0, 5]], [[154, 179]], [], [[43, 57]], [], [], [[287, 303]], [[240, 256]], [], [], [[202, 226]], [[342, 351]], [[22, 38]], [[503, 525]], [[220, 236]], [[11, 12]], [[317, 371]], [[69, 88]], [[5, 10]], [[103, 125]], [], [[49, 61]], [], [[774, 780], [827, 840]], [[632, 640]], [[574, 587]], [[259, 281]], [[931, 950]], [[146, 160]], [[448, 468]], [[24, 40], [103, 110], [127, 141], [183, 213]], [[232, 236], [251, 273], [232, 246], [259, 273]], [[0, 6]], [[32, 38], [155, 161]], [], [[51, 60]], [], [[115, 123]], [], [[90, 96]], [[418, 433]], [], [], [], [[7, 13]], [[77, 88]], [[472, 478], [500, 513]], [[397, 406]], [[128, 135], [150, 157], [162, 173], [140, 149], [150, 157], [162, 173]], [[0, 5]], [[39, 48], [296, 305]], [[365, 369]], [[12, 20], [50, 61]], [], [[271, 279]], [[407, 418]], [[261, 266]], [], [], [], [[672, 678], [683, 692], [635, 637], [683, 692]], [[6, 7]], [[371, 389]], [[407, 413], [435, 447]], [[315, 320]], [[182, 191], [214, 232], [195, 202], [214, 232]], [[0, 5]], [[149, 158]], [], [], [], [[351, 360]], [], [], [], [], [], [], [[6, 12]], [[234, 252]], [], [], [[264, 271], [304, 327], [273, 282], [304, 327]], [[0, 5]], [[119, 128]], [], [[83, 91], [99, 107]], [], [[452, 453]], [], [[450, 451]], [[485, 495], [500, 511]], [[800, 826]], [], [[763, 778], [769, 778], [755, 761]], [[6, 7]], [[415, 426]], [[648, 671]], [[428, 444]], [[354, 363], [374, 389]], [[35, 41]], [[181, 190]], [[424, 428]], [], [[444, 453]], [[277, 285]], [], [[266, 272]], [[458, 468]], [[781, 799]], [], [[749, 762], [741, 747], [753, 762]], [[42, 48]], [[491, 502]], [], [[580, 589]], [], [[0, 11]], [[81, 88]], [], [[69, 77], [101, 109]], [], [[292, 300]], [], [[283, 290]], [[302, 317], [340, 342]], [], [], [[498, 512], [469, 471], [505, 512]], [[12, 18]], [], [], [], [[274, 280], [294, 309], [286, 309]], [[0, 4]], [[55, 64]], [[103, 121]], [[212, 233]], [[250, 264]], [[341, 348]], [], [], [[82, 99], [141, 151], [153, 168]], [[431, 469]], [], [[686, 692], [706, 715], [694, 715]], [[5, 6]], [], [[380, 386], [404, 417], [668, 684]], [[123, 139]], [[332, 353]], [[8, 13]], [[32, 37], [152, 161]], [[111, 121]], [], [], [[273, 281]], [], [[265, 271]], [[61, 87], [98, 109], [127, 137]], [], [], [[554, 556], [581, 590], [572, 575], [581, 590]], [[14, 15]], [[287, 307]], [], [], [[260, 283], [299, 308]], [[0, 5]], [[37, 46], [208, 217], [413, 422]], [], [[21, 23], [144, 154]], [], [[451, 459]], [], [[440, 446]], [], [], [], [[677, 680], [683, 692], [671, 675], [683, 692]], [[6, 7]], [], [[486, 492], [534, 546]], [[403, 412]], [[257, 266], [275, 289], [267, 289]], [[0, 4]], [[21, 26]], [], [[66, 74], [95, 114]], [[291, 295], [335, 339]], [[163, 169]], [], [[152, 158]], [[536, 551], [561, 575]], [], [], [[644, 662], [616, 630], [653, 662]], [[5, 6]], [[175, 186]], [[809, 825]], [[390, 397]], [], [[26, 31]], [[67, 76]], [], [[47, 54], [144, 157]], [[365, 379], [385, 402]], [[248, 257]], [], [[240, 246]], [[481, 491], [496, 511]], [], [], [[740, 742], [750, 759], [744, 747], [750, 759]], [[32, 33], [0, 2]], [[227, 238]], [[418, 420], [438, 451]], [[453, 466]], [[632, 641], [659, 668], [646, 668]], [[0, 5]], [[32, 41]], [], [[17, 25], [180, 199]], [], [[320, 326]], [], [[309, 315]], [], [], [], [[745, 760], [741, 743], [750, 760]], [[6, 12]], [[448, 459]], [], [[544, 560]], [[250, 275]], [[2, 13]], [[79, 88]], [], [[56, 64], [169, 183]], [], [[382, 390], [396, 403]], [], [], [[457, 467]], [], [], [[699, 708], [725, 733], [714, 733]], [[14, 20]], [[277, 287], [311, 328]], [], [[339, 348]], [[236, 258], [236, 250], [259, 268]], [[5, 10]], [[32, 41]], [], [], [], [[120, 128]], [], [[133, 139]], [[304, 314], [319, 331]], [], [], [[402, 408], [413, 421], [398, 400], [413, 421]], [[11, 12]], [[179, 189], [193, 205]], [], [], [[179, 188], [195, 206], [167, 174], [195, 206]], [[0, 4]], [[55, 64], [83, 121]], [], [[40, 48], [246, 263]], [], [[314, 321], [352, 357]], [], [], [], [], [], [], [[5, 11]], [[430, 441]], [], [], [[190, 197], [244, 257], [199, 208], [244, 257]], [[35, 39]], [[74, 79]], [[455, 465]], [], [], [[292, 301], [346, 354]], [], [[284, 290]], [[426, 441], [443, 453]], [[607, 634]], [], [[723, 744], [719, 721], [735, 744]], [[40, 41]], [[467, 478]], [[497, 546]], [[307, 324]], [], [[4, 8]], [[35, 44]], [], [[13, 20], [50, 67]], [], [], [], [[115, 121]], [], [], [], [[137, 158]], [], [], [], [], [], [[6, 11]], [[31, 40]], [], [[86, 94], [102, 114]], [[158, 167]], [[297, 306]], [], [[289, 295]], [], [], [], [[486, 502], [478, 484], [493, 502]], [[12, 13]], [[337, 355]], [[618, 624], [673, 686]], [[169, 185]], [[473, 480], [493, 503], [462, 471], [493, 503]], [[16, 21]], [[183, 192]], [], [[53, 55], [225, 240], [53, 55], [207, 214]], [], [[318, 325]], [[377, 379], [419, 425]], [[306, 312]], [], [], [], [[641, 647], [651, 660], [637, 639], [651, 660]], [[0, 2], [22, 28]], [[270, 281]], [[340, 356]], [[358, 367]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/953] Elapsed 0m 2s (remain 46m 19s) Loss: 0.8264(0.8264) Grad: 5.9640  LR: 0.00002000  \n",
      "Epoch: [1][1/953] Elapsed 0m 6s (remain 51m 13s) Loss: 0.6305(0.7284) Grad: 4.8752  LR: 0.00002000  \n",
      "Epoch: [1][2/953] Elapsed 0m 10s (remain 53m 0s) Loss: 0.4815(0.6461) Grad: 4.0724  LR: 0.00002000  \n",
      "Epoch: [1][3/953] Elapsed 0m 13s (remain 54m 12s) Loss: 0.3620(0.5751) Grad: 3.2824  LR: 0.00002000  \n",
      "Epoch: [1][4/953] Elapsed 0m 17s (remain 54m 54s) Loss: 0.2763(0.5153) Grad: 2.5638  LR: 0.00002000  \n",
      "Epoch: [1][5/953] Elapsed 0m 21s (remain 55m 30s) Loss: 0.1969(0.4623) Grad: 1.8668  LR: 0.00002000  \n",
      "Epoch: [1][6/953] Elapsed 0m 24s (remain 55m 40s) Loss: 0.1624(0.4194) Grad: 1.2727  LR: 0.00002000  \n",
      "Epoch: [1][7/953] Elapsed 0m 28s (remain 55m 36s) Loss: 0.1121(0.3810) Grad: 0.7669  LR: 0.00002000  \n",
      "Epoch: [1][8/953] Elapsed 0m 32s (remain 56m 3s) Loss: 0.0991(0.3497) Grad: 0.5053  LR: 0.00002000  \n",
      "Epoch: [1][9/953] Elapsed 0m 35s (remain 55m 55s) Loss: 0.0948(0.3242) Grad: 0.4042  LR: 0.00002000  \n",
      "Epoch: [1][10/953] Elapsed 0m 39s (remain 55m 42s) Loss: 0.0790(0.3019) Grad: 0.2840  LR: 0.00002000  \n",
      "Epoch: [1][11/953] Elapsed 0m 42s (remain 55m 43s) Loss: 0.0451(0.2805) Grad: 0.2905  LR: 0.00002000  \n",
      "Epoch: [1][12/953] Elapsed 0m 46s (remain 55m 38s) Loss: 0.0933(0.2661) Grad: 0.1722  LR: 0.00002000  \n",
      "Epoch: [1][13/953] Elapsed 0m 49s (remain 55m 29s) Loss: 0.0748(0.2524) Grad: 0.1226  LR: 0.00002000  \n",
      "Epoch: [1][14/953] Elapsed 0m 53s (remain 55m 23s) Loss: 0.0590(0.2395) Grad: 0.1017  LR: 0.00002000  \n",
      "Epoch: [1][15/953] Elapsed 0m 56s (remain 55m 19s) Loss: 0.0530(0.2279) Grad: 0.0923  LR: 0.00002000  \n",
      "Epoch: [1][16/953] Elapsed 1m 0s (remain 55m 19s) Loss: 0.0737(0.2188) Grad: 0.1404  LR: 0.00002000  \n",
      "Epoch: [1][17/953] Elapsed 1m 3s (remain 55m 15s) Loss: 0.0683(0.2104) Grad: 0.1305  LR: 0.00002000  \n",
      "Epoch: [1][18/953] Elapsed 1m 7s (remain 55m 17s) Loss: 0.0773(0.2034) Grad: 0.1434  LR: 0.00002000  \n",
      "Epoch: [1][19/953] Elapsed 1m 10s (remain 55m 11s) Loss: 0.0846(0.1975) Grad: 0.1594  LR: 0.00002000  \n",
      "Epoch: [1][20/953] Elapsed 1m 14s (remain 55m 15s) Loss: 0.0840(0.1921) Grad: 0.1589  LR: 0.00002000  \n",
      "Epoch: [1][21/953] Elapsed 1m 18s (remain 55m 11s) Loss: 0.0796(0.1870) Grad: 0.1048  LR: 0.00002000  \n",
      "Epoch: [1][22/953] Elapsed 1m 21s (remain 55m 6s) Loss: 0.0407(0.1806) Grad: 0.2561  LR: 0.00002000  \n",
      "Epoch: [1][23/953] Elapsed 1m 25s (remain 55m 3s) Loss: 0.0647(0.1758) Grad: 0.1913  LR: 0.00002000  \n",
      "Epoch: [1][24/953] Elapsed 1m 28s (remain 55m 1s) Loss: 0.0706(0.1716) Grad: 0.2051  LR: 0.00002000  \n",
      "Epoch: [1][25/953] Elapsed 1m 32s (remain 54m 58s) Loss: 0.0491(0.1669) Grad: 0.3399  LR: 0.00002000  \n",
      "Epoch: [1][26/953] Elapsed 1m 36s (remain 54m 56s) Loss: 0.0766(0.1635) Grad: 0.1115  LR: 0.00002000  \n",
      "Epoch: [1][27/953] Elapsed 1m 39s (remain 54m 54s) Loss: 0.0633(0.1599) Grad: 0.1301  LR: 0.00002000  \n",
      "Epoch: [1][28/953] Elapsed 1m 43s (remain 54m 49s) Loss: 0.0794(0.1572) Grad: 0.1292  LR: 0.00002000  \n",
      "Epoch: [1][29/953] Elapsed 1m 46s (remain 54m 45s) Loss: 0.0695(0.1542) Grad: 0.0874  LR: 0.00002000  \n",
      "Epoch: [1][30/953] Elapsed 1m 50s (remain 54m 39s) Loss: 0.0588(0.1512) Grad: 0.1016  LR: 0.00002000  \n",
      "Epoch: [1][31/953] Elapsed 1m 53s (remain 54m 34s) Loss: 0.0475(0.1479) Grad: 0.1411  LR: 0.00002000  \n",
      "Epoch: [1][32/953] Elapsed 1m 57s (remain 54m 32s) Loss: 0.0580(0.1452) Grad: 0.0849  LR: 0.00002000  \n",
      "Epoch: [1][33/953] Elapsed 2m 1s (remain 54m 33s) Loss: 0.0979(0.1438) Grad: 0.2756  LR: 0.00002000  \n",
      "Epoch: [1][34/953] Elapsed 2m 4s (remain 54m 30s) Loss: 0.0805(0.1420) Grad: 0.1292  LR: 0.00002000  \n",
      "Epoch: [1][35/953] Elapsed 2m 8s (remain 54m 28s) Loss: 0.0972(0.1408) Grad: 0.2067  LR: 0.00002000  \n",
      "Epoch: [1][36/953] Elapsed 2m 12s (remain 54m 31s) Loss: 0.0602(0.1386) Grad: 0.1393  LR: 0.00002000  \n",
      "Epoch: [1][37/953] Elapsed 2m 15s (remain 54m 31s) Loss: 0.0696(0.1368) Grad: 0.1398  LR: 0.00002000  \n",
      "Epoch: [1][38/953] Elapsed 2m 19s (remain 54m 30s) Loss: 0.0640(0.1349) Grad: 0.2032  LR: 0.00002000  \n",
      "Epoch: [1][39/953] Elapsed 2m 23s (remain 54m 26s) Loss: 0.1171(0.1345) Grad: 0.2304  LR: 0.00002000  \n",
      "Epoch: [1][40/953] Elapsed 2m 26s (remain 54m 20s) Loss: 0.0787(0.1331) Grad: 0.1956  LR: 0.00002000  \n",
      "Epoch: [1][41/953] Elapsed 2m 30s (remain 54m 17s) Loss: 0.0625(0.1314) Grad: 0.2968  LR: 0.00002000  \n",
      "Epoch: [1][42/953] Elapsed 2m 33s (remain 54m 12s) Loss: 0.0784(0.1302) Grad: 0.1796  LR: 0.00002000  \n",
      "Epoch: [1][43/953] Elapsed 2m 37s (remain 54m 7s) Loss: 0.0936(0.1293) Grad: 0.1042  LR: 0.00002000  \n",
      "Epoch: [1][44/953] Elapsed 2m 40s (remain 54m 3s) Loss: 0.0838(0.1283) Grad: 0.1176  LR: 0.00002000  \n",
      "Epoch: [1][45/953] Elapsed 2m 44s (remain 53m 57s) Loss: 0.0512(0.1267) Grad: 0.2623  LR: 0.00002000  \n",
      "Epoch: [1][46/953] Elapsed 2m 47s (remain 53m 53s) Loss: 0.0737(0.1255) Grad: 0.1128  LR: 0.00002000  \n",
      "Epoch: [1][47/953] Elapsed 2m 51s (remain 53m 47s) Loss: 0.0655(0.1243) Grad: 0.1114  LR: 0.00001999  \n",
      "Epoch: [1][48/953] Elapsed 2m 54s (remain 53m 43s) Loss: 0.0520(0.1228) Grad: 0.1462  LR: 0.00001999  \n",
      "Epoch: [1][49/953] Elapsed 2m 58s (remain 53m 39s) Loss: 0.0858(0.1221) Grad: 0.1634  LR: 0.00001999  \n",
      "Epoch: [1][50/953] Elapsed 3m 1s (remain 53m 34s) Loss: 0.0991(0.1216) Grad: 0.2214  LR: 0.00001999  \n",
      "Epoch: [1][51/953] Elapsed 3m 5s (remain 53m 31s) Loss: 0.0959(0.1211) Grad: 0.2010  LR: 0.00001999  \n",
      "Epoch: [1][52/953] Elapsed 3m 9s (remain 53m 31s) Loss: 0.0534(0.1198) Grad: 0.1937  LR: 0.00001999  \n",
      "Epoch: [1][53/953] Elapsed 3m 12s (remain 53m 31s) Loss: 0.0504(0.1186) Grad: 0.2366  LR: 0.00001999  \n",
      "Epoch: [1][54/953] Elapsed 3m 16s (remain 53m 28s) Loss: 0.0828(0.1179) Grad: 0.1222  LR: 0.00001999  \n",
      "Epoch: [1][55/953] Elapsed 3m 19s (remain 53m 23s) Loss: 0.0794(0.1172) Grad: 0.1362  LR: 0.00001999  \n",
      "Epoch: [1][56/953] Elapsed 3m 23s (remain 53m 18s) Loss: 0.0809(0.1166) Grad: 0.1616  LR: 0.00001999  \n",
      "Epoch: [1][57/953] Elapsed 3m 26s (remain 53m 12s) Loss: 0.1466(0.1171) Grad: 0.3804  LR: 0.00001999  \n",
      "Epoch: [1][58/953] Elapsed 3m 30s (remain 53m 7s) Loss: 0.0984(0.1168) Grad: 0.1658  LR: 0.00001999  \n",
      "Epoch: [1][59/953] Elapsed 3m 33s (remain 53m 3s) Loss: 0.0852(0.1163) Grad: 0.2324  LR: 0.00001999  \n",
      "Epoch: [1][60/953] Elapsed 3m 37s (remain 53m 1s) Loss: 0.0623(0.1154) Grad: 0.4909  LR: 0.00001999  \n",
      "Epoch: [1][61/953] Elapsed 3m 41s (remain 52m 59s) Loss: 0.0846(0.1149) Grad: 0.3504  LR: 0.00001999  \n",
      "Epoch: [1][62/953] Elapsed 3m 44s (remain 52m 57s) Loss: 0.0990(0.1146) Grad: 0.1700  LR: 0.00001999  \n",
      "Epoch: [1][63/953] Elapsed 3m 48s (remain 52m 57s) Loss: 0.0545(0.1137) Grad: 0.3859  LR: 0.00001999  \n",
      "Epoch: [1][64/953] Elapsed 3m 52s (remain 52m 54s) Loss: 0.0658(0.1129) Grad: 0.2081  LR: 0.00001999  \n",
      "Epoch: [1][65/953] Elapsed 3m 56s (remain 52m 53s) Loss: 0.0815(0.1125) Grad: 0.1229  LR: 0.00001999  \n",
      "Epoch: [1][66/953] Elapsed 3m 59s (remain 52m 51s) Loss: 0.0707(0.1118) Grad: 0.1282  LR: 0.00001999  \n",
      "Epoch: [1][67/953] Elapsed 4m 3s (remain 52m 46s) Loss: 0.0533(0.1110) Grad: 0.1617  LR: 0.00001999  \n",
      "Epoch: [1][68/953] Elapsed 4m 6s (remain 52m 42s) Loss: 0.0491(0.1101) Grad: 0.1623  LR: 0.00001999  \n",
      "Epoch: [1][69/953] Elapsed 4m 10s (remain 52m 37s) Loss: 0.0801(0.1097) Grad: 0.2342  LR: 0.00001999  \n",
      "Epoch: [1][70/953] Elapsed 4m 13s (remain 52m 33s) Loss: 0.0649(0.1090) Grad: 0.1601  LR: 0.00001999  \n",
      "Epoch: [1][71/953] Elapsed 4m 17s (remain 52m 29s) Loss: 0.0873(0.1087) Grad: 0.1874  LR: 0.00001999  \n",
      "Epoch: [1][72/953] Elapsed 4m 20s (remain 52m 25s) Loss: 0.0659(0.1081) Grad: 0.1606  LR: 0.00001999  \n",
      "Epoch: [1][73/953] Elapsed 4m 24s (remain 52m 21s) Loss: 0.0841(0.1078) Grad: 0.1629  LR: 0.00001999  \n",
      "Epoch: [1][74/953] Elapsed 4m 27s (remain 52m 16s) Loss: 0.0686(0.1073) Grad: 0.2354  LR: 0.00001999  \n",
      "Epoch: [1][75/953] Elapsed 4m 31s (remain 52m 12s) Loss: 0.0870(0.1070) Grad: 0.1641  LR: 0.00001999  \n",
      "Epoch: [1][76/953] Elapsed 4m 34s (remain 52m 8s) Loss: 0.0917(0.1068) Grad: 0.1741  LR: 0.00001999  \n",
      "Epoch: [1][77/953] Elapsed 4m 38s (remain 52m 4s) Loss: 0.0833(0.1065) Grad: 0.3601  LR: 0.00001999  \n",
      "Epoch: [1][78/953] Elapsed 4m 42s (remain 51m 59s) Loss: 0.0888(0.1063) Grad: 0.2144  LR: 0.00001999  \n",
      "Epoch: [1][79/953] Elapsed 4m 45s (remain 51m 57s) Loss: 0.0875(0.1061) Grad: 0.4718  LR: 0.00001999  \n",
      "Epoch: [1][80/953] Elapsed 4m 49s (remain 51m 52s) Loss: 0.1245(0.1063) Grad: 0.4453  LR: 0.00001999  \n",
      "Epoch: [1][81/953] Elapsed 4m 52s (remain 51m 47s) Loss: 0.0869(0.1061) Grad: 0.5478  LR: 0.00001999  \n",
      "Epoch: [1][82/953] Elapsed 4m 56s (remain 51m 42s) Loss: 0.0681(0.1056) Grad: 0.7113  LR: 0.00001999  \n",
      "Epoch: [1][83/953] Elapsed 4m 59s (remain 51m 37s) Loss: 0.0691(0.1052) Grad: 0.5807  LR: 0.00001998  \n",
      "Epoch: [1][84/953] Elapsed 5m 2s (remain 51m 32s) Loss: 0.0848(0.1049) Grad: 0.1836  LR: 0.00001998  \n",
      "Epoch: [1][85/953] Elapsed 5m 6s (remain 51m 27s) Loss: 0.0869(0.1047) Grad: 0.1527  LR: 0.00001998  \n",
      "Epoch: [1][86/953] Elapsed 5m 9s (remain 51m 22s) Loss: 0.0772(0.1044) Grad: 0.4485  LR: 0.00001998  \n",
      "Epoch: [1][87/953] Elapsed 5m 13s (remain 51m 20s) Loss: 0.0547(0.1038) Grad: 0.1944  LR: 0.00001998  \n",
      "Epoch: [1][88/953] Elapsed 5m 17s (remain 51m 18s) Loss: 0.0533(0.1033) Grad: 0.1380  LR: 0.00001998  \n",
      "Epoch: [1][89/953] Elapsed 5m 20s (remain 51m 16s) Loss: 0.0528(0.1027) Grad: 0.1392  LR: 0.00001998  \n",
      "Epoch: [1][90/953] Elapsed 5m 24s (remain 51m 12s) Loss: 0.0910(0.1026) Grad: 0.2470  LR: 0.00001998  \n",
      "Epoch: [1][91/953] Elapsed 5m 27s (remain 51m 7s) Loss: 0.0782(0.1023) Grad: 0.1181  LR: 0.00001998  \n",
      "Epoch: [1][92/953] Elapsed 5m 31s (remain 51m 4s) Loss: 0.0723(0.1020) Grad: 0.1375  LR: 0.00001998  \n",
      "Epoch: [1][93/953] Elapsed 5m 35s (remain 51m 1s) Loss: 0.0682(0.1016) Grad: 0.1363  LR: 0.00001998  \n",
      "Epoch: [1][94/953] Elapsed 5m 38s (remain 50m 58s) Loss: 0.1162(0.1018) Grad: 0.2987  LR: 0.00001998  \n",
      "Epoch: [1][95/953] Elapsed 5m 42s (remain 50m 55s) Loss: 0.0727(0.1015) Grad: 0.2234  LR: 0.00001998  \n",
      "Epoch: [1][96/953] Elapsed 5m 45s (remain 50m 52s) Loss: 0.1048(0.1015) Grad: 0.1571  LR: 0.00001998  \n",
      "Epoch: [1][97/953] Elapsed 5m 49s (remain 50m 49s) Loss: 0.0701(0.1012) Grad: 0.2872  LR: 0.00001998  \n",
      "Epoch: [1][98/953] Elapsed 5m 53s (remain 50m 47s) Loss: 0.0570(0.1008) Grad: 0.3522  LR: 0.00001998  \n",
      "Epoch: [1][99/953] Elapsed 5m 56s (remain 50m 44s) Loss: 0.0675(0.1004) Grad: 0.3024  LR: 0.00001998  \n",
      "Epoch: [1][100/953] Elapsed 6m 0s (remain 50m 43s) Loss: 0.0592(0.1000) Grad: 0.3045  LR: 0.00001998  \n",
      "Epoch: [1][101/953] Elapsed 6m 4s (remain 50m 40s) Loss: 0.0896(0.0999) Grad: 0.3156  LR: 0.00001998  \n",
      "Epoch: [1][102/953] Elapsed 6m 8s (remain 50m 39s) Loss: 0.0540(0.0995) Grad: 0.2382  LR: 0.00001998  \n",
      "Epoch: [1][103/953] Elapsed 6m 11s (remain 50m 35s) Loss: 0.0714(0.0992) Grad: 0.2619  LR: 0.00001998  \n",
      "Epoch: [1][104/953] Elapsed 6m 15s (remain 50m 31s) Loss: 0.1018(0.0992) Grad: 0.4296  LR: 0.00001998  \n",
      "Epoch: [1][105/953] Elapsed 6m 19s (remain 50m 28s) Loss: 0.0748(0.0990) Grad: 0.1862  LR: 0.00001998  \n",
      "Epoch: [1][106/953] Elapsed 6m 22s (remain 50m 25s) Loss: 0.0780(0.0988) Grad: 0.2939  LR: 0.00001998  \n",
      "Epoch: [1][107/953] Elapsed 6m 26s (remain 50m 22s) Loss: 0.0732(0.0986) Grad: 0.2920  LR: 0.00001997  \n",
      "Epoch: [1][108/953] Elapsed 6m 30s (remain 50m 20s) Loss: 0.0544(0.0981) Grad: 0.4681  LR: 0.00001997  \n",
      "Epoch: [1][109/953] Elapsed 6m 33s (remain 50m 17s) Loss: 0.0498(0.0977) Grad: 0.4053  LR: 0.00001997  \n",
      "Epoch: [1][110/953] Elapsed 6m 37s (remain 50m 14s) Loss: 0.0676(0.0974) Grad: 0.1394  LR: 0.00001997  \n",
      "Epoch: [1][111/953] Elapsed 6m 40s (remain 50m 10s) Loss: 0.0944(0.0974) Grad: 0.2195  LR: 0.00001997  \n",
      "Epoch: [1][112/953] Elapsed 6m 44s (remain 50m 6s) Loss: 0.0508(0.0970) Grad: 0.1257  LR: 0.00001997  \n",
      "Epoch: [1][113/953] Elapsed 6m 47s (remain 50m 2s) Loss: 0.0773(0.0968) Grad: 0.1539  LR: 0.00001997  \n",
      "Epoch: [1][114/953] Elapsed 6m 51s (remain 50m 1s) Loss: 0.0923(0.0968) Grad: 0.2384  LR: 0.00001997  \n",
      "Epoch: [1][115/953] Elapsed 6m 55s (remain 49m 59s) Loss: 0.0557(0.0964) Grad: 0.0895  LR: 0.00001997  \n",
      "Epoch: [1][116/953] Elapsed 6m 59s (remain 49m 57s) Loss: 0.0615(0.0961) Grad: 0.1184  LR: 0.00001997  \n",
      "Epoch: [1][117/953] Elapsed 7m 3s (remain 49m 53s) Loss: 0.0563(0.0958) Grad: 0.1459  LR: 0.00001997  \n",
      "Epoch: [1][118/953] Elapsed 7m 6s (remain 49m 50s) Loss: 0.0536(0.0954) Grad: 0.1295  LR: 0.00001997  \n",
      "Epoch: [1][119/953] Elapsed 7m 10s (remain 49m 46s) Loss: 0.0732(0.0953) Grad: 0.1607  LR: 0.00001997  \n",
      "Epoch: [1][120/953] Elapsed 7m 13s (remain 49m 42s) Loss: 0.0604(0.0950) Grad: 0.1371  LR: 0.00001997  \n",
      "Epoch: [1][121/953] Elapsed 7m 17s (remain 49m 39s) Loss: 0.0593(0.0947) Grad: 0.1466  LR: 0.00001997  \n",
      "Epoch: [1][122/953] Elapsed 7m 20s (remain 49m 35s) Loss: 0.0731(0.0945) Grad: 0.1019  LR: 0.00001997  \n",
      "Epoch: [1][123/953] Elapsed 7m 24s (remain 49m 30s) Loss: 0.0583(0.0942) Grad: 0.1758  LR: 0.00001997  \n",
      "Epoch: [1][124/953] Elapsed 7m 27s (remain 49m 27s) Loss: 0.0881(0.0942) Grad: 0.1374  LR: 0.00001997  \n",
      "Epoch: [1][125/953] Elapsed 7m 31s (remain 49m 24s) Loss: 0.0822(0.0941) Grad: 0.1218  LR: 0.00001997  \n",
      "Epoch: [1][126/953] Elapsed 7m 35s (remain 49m 22s) Loss: 0.0688(0.0939) Grad: 0.1688  LR: 0.00001996  \n",
      "Epoch: [1][127/953] Elapsed 7m 39s (remain 49m 19s) Loss: 0.0635(0.0936) Grad: 0.2272  LR: 0.00001996  \n",
      "Epoch: [1][128/953] Elapsed 7m 42s (remain 49m 16s) Loss: 0.0895(0.0936) Grad: 0.2071  LR: 0.00001996  \n",
      "Epoch: [1][129/953] Elapsed 7m 46s (remain 49m 12s) Loss: 0.1200(0.0938) Grad: 0.2375  LR: 0.00001996  \n",
      "Epoch: [1][130/953] Elapsed 7m 49s (remain 49m 8s) Loss: 0.1136(0.0940) Grad: 0.2876  LR: 0.00001996  \n",
      "Epoch: [1][131/953] Elapsed 7m 53s (remain 49m 4s) Loss: 0.0549(0.0937) Grad: 0.4839  LR: 0.00001996  \n",
      "Epoch: [1][132/953] Elapsed 7m 56s (remain 49m 0s) Loss: 0.0633(0.0934) Grad: 0.2944  LR: 0.00001996  \n",
      "Epoch: [1][133/953] Elapsed 8m 0s (remain 48m 56s) Loss: 0.0997(0.0935) Grad: 0.1435  LR: 0.00001996  \n",
      "Epoch: [1][134/953] Elapsed 8m 4s (remain 48m 53s) Loss: 0.1119(0.0936) Grad: 0.1415  LR: 0.00001996  \n",
      "Epoch: [1][135/953] Elapsed 8m 7s (remain 48m 50s) Loss: 0.0907(0.0936) Grad: 0.1322  LR: 0.00001996  \n",
      "Epoch: [1][136/953] Elapsed 8m 11s (remain 48m 48s) Loss: 0.0644(0.0934) Grad: 0.2720  LR: 0.00001996  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = create_labels_for_scoring(oof_df)\n",
    "        print(labels)\n",
    "        predictions = oof_df[[i for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        print(preds)\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "        \n",
    "    if CFG.wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
