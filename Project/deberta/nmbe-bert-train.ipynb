{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook uses the same code as nmbe-deberta-train.ipynb except for the backbone model (bert-base-uncased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start, the env setup \n",
    "1. use conda to create a new env\n",
    "```bash\n",
    "    conda create -n nmbe python=3.7\n",
    "    conda activate nmbe\n",
    "```\n",
    "\n",
    "2. install the following packages\n",
    "```bash\n",
    "    pip install torch==1.10.0\n",
    "    pip install sacremoses==0.0.41\n",
    "    pip install transformers\n",
    "```\n",
    "```bash if using cuda\n",
    "    pip install torch==1.10.0.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    pip install sacremoses==0.0.41\n",
    "    pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # About this notebook\n",
    " - Deberta-base starter code\n",
    " - pip wheels is [here](https://www.kaggle.com/yasufuminakama/nbme-pip-wheels)\n",
    " - Inference notebook is [here](https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-inference)\n",
    "\n",
    " If this notebook is helpful, feel free to upvote :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "root = '.'\n",
    "OUTPUT_DIR = root + '/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG (Classifier Free Guidance?)\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    competition='NBME'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=True\n",
    "    apex=True\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model=\"bert-base-uncased\"\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=5\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=12\n",
    "    fc_dropout=0.2\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    # possible to simply delete folds that are already trained\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.13.3\n",
      "transformers.__version__: 4.16.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import ast\n",
    "import copy\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from torch.nn import Parameter\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.system('pip uninstall -y transformers')\n",
    "os.system('python -m pip install --no-index --find-links=./input/nbme-pip-wheels transformers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import (AutoConfig, AutoModel, AutoTokenizer,\n",
    "                          get_cosine_schedule_with_warmup,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = \"cpu\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incompatible models lead to NaN gradients, this is the case for CUDA version of this model with 0 workers in DataLoaders\n",
    "use_scaler = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    # print('span_micro preds', preds)\n",
    "    # print('span_micro truths', truths)\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        #print('bin preds: ', bin_preds)\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "        #print('bin truths: ', bin_truths)\n",
    "    return micro_f1(bin_preds, bin_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    #print('inside score true: ', y_true)\n",
    "    #print('inside score pred: ', y_pred)\n",
    "    score = span_micro_f1(y_pred, y_true)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train_bert'):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed) if torch.cuda.is_available() else torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (14300, 6)\n",
      "features.shape: (143, 3)\n",
      "patient_notes.shape: (42146, 3)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/train.csv')\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval) # ['mom'] -> [mom]\n",
    "train['location'] = train['location'].apply(ast.literal_eval) # ['0 1'] -> [0, 1]\n",
    "features = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/features.csv')\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "features = preprocess_features(features)\n",
    "patient_notes = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "#display(train.head())\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "#display(features.head())\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "#display(patient_notes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "#display(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "#display(train['annotation_length'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=CFG.n_fold) # for loop for 5 rounds, val_index is a list of index of validation data, others are index of training data\n",
    "groups = train['pn_num'].values # make sure same patient does not appear in train & valid at the same time \n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)): # Fold.split(x, y, groups), each fold has same distribution of y\n",
    "    train.loc[val_index, 'fold'] = int(n) # add fold column, and fill with fold number which this row belongs to evaluation\n",
    "train['fold'] = train['fold'].astype(int) # validation data is len(train)/5 = 2860, each fold 1~5 have 2860 testing data and 11440 training data \n",
    "# display(train.groupby('fold').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    # display(train.groupby('fold').size())\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    # display(train.groupby('fold').size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42146 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42146/42146 [00:49<00:00, 845.49it/s] \n",
      "pn_history max(lengths): 312\n",
      "pn_history max(lengths): 312\n",
      "pn_history max(lengths): 312\n",
      "pn_history max(lengths): 312\n",
      "100%|██████████| 143/143 [00:00<00:00, 5310.66it/s]\n",
      "feature_text max(lengths): 29\n",
      "feature_text max(lengths): 29\n",
      "feature_text max(lengths): 29\n",
      "feature_text max(lengths): 29\n",
      "max_len: 344\n",
      "max_len: 344\n",
      "max_len: 344\n",
      "max_len: 344\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "for text_col in ['pn_history']:\n",
    "    pn_history_lengths = []\n",
    "    tk0 = tqdm(patient_notes[text_col].fillna(\"\").values, total=len(patient_notes)) # tqdm: progress bar \n",
    "    for text in tk0: # tk0 is a list of text which is text_col column of patient_notes \n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids']) # turn the sentence to index: len([1360, 12, 180, ...])\n",
    "        pn_history_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(pn_history_lengths)}')\n",
    "\n",
    "for text_col in ['feature_text']:\n",
    "    features_lengths = []\n",
    "    tk0 = tqdm(features[text_col].fillna(\"\").values, total=len(features))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids']) # 'Intermittent' -> ['In', 'term',...] -> [1360, 12, 180, ...]\n",
    "        features_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(features_lengths)}')\n",
    "\n",
    "CFG.max_len = max(pn_history_lengths) + max(features_lengths) + 3 # cls 開始 & sep 病例結果 & sep 特徵結尾 \n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text, feature_text):\n",
    "    inputs = cfg.tokenizer(text, feature_text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CFG.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items(): # k = \n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def create_label(cfg, text, annotation_length, location_list):\n",
    "    encoded = cfg.tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=CFG.max_len,\n",
    "                            padding=\"max_length\",\n",
    "                            return_offsets_mapping=True)\n",
    "    offset_mapping = encoded['offset_mapping'] # index of each token: (0, 0) (0, 1) (1, 2) (2, 3) (3, 4) ...\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0] # list all None token index\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1 # filled all None token index with = -1; [-1, 0, 0, ..., -1, -1, -1];  0 is no features, -1 is None token  \n",
    "    if annotation_length != 0:\n",
    "        for location in location_list: # location = '237 242;261 268' \n",
    "            for loc in [s.split() for s in location.split(';')]: # [['237', '242'], ['261', '268']] => loc = ['237', '242']\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]): # offset_mapping[idx] = (0, 0); start < 0 -> start_idx = 0 \n",
    "                        start_idx = idx - 1 # find start_ans is in the range of offset_mapping \n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]): \n",
    "                        end_idx = idx + 1 # label[start_idx:end_idx], the last token is not included, so end_idx + 1 \n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1 # each is token [-1, 0, 0, ..., 1, 1, ..., -1, -1, -1]; -1 is None token, 0 is no features, 1 is features\n",
    "    return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, \n",
    "                               self.pn_historys[item], \n",
    "                               self.feature_texts[item])\n",
    "        label = create_label(self.cfg, \n",
    "                             self.pn_historys[item], \n",
    "                             self.annotation_lengths[item], \n",
    "                             self.locations[item])\n",
    "        return inputs, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helpler functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    if use_scaler:\n",
    "        # standard growth factor: 2.0\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)  \n",
    "    losses = AverageMeter() # calculate the average loss of each batch, \n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        if torch.cuda.is_available():\n",
    "            with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "                y_preds = model(inputs) # [12, 458, 1]; batch is 12, max_len is 458 \n",
    "        else:\n",
    "            y_preds = model(inputs)\n",
    "        \n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean() # choose the loss of label != -1, and calculate the mean of all none -1 loss\n",
    "        \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        \n",
    "        losses.update(loss.item(), batch_size) # loss=0.1, batch_size=12, losses.avg = (0.1*12 + 0.2*12 ...)/(12*num_batch)\n",
    "\n",
    "        scaler.scale(loss).backward() if use_scaler else loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm) # constrain the gradient to be less than max_grad_norm, to avoid gradient explosion\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1 # global_step is including the epoch\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0 and use_scaler :\n",
    "             scaler.step(optimizer)\n",
    "             scaler.update()\n",
    "             optimizer.zero_grad()\n",
    "             global_step += 1\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        end = time.time()\n",
    "        #if step % CFG.print_freq == 0 or step == (len(train_loader)-1) and torch.cuda.is_available():\n",
    "        print('Epoch: [{0}][{1}/{2}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                'Grad: {grad_norm:.4f}  '\n",
    "                'LR: {lr:.8f}  '\n",
    "                .format(epoch+1, step, len(train_loader), \n",
    "                        remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                        loss=losses,\n",
    "                        grad_norm=grad_norm,\n",
    "                        lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            #print('y_preds: ', y_preds)\n",
    "            # loss seems fine\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        # TODO preds are empty\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        # if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "        print('EVAL: [{0}/{1}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                .format(step, len(valid_loader),\n",
    "                        loss=losses,\n",
    "                        remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "        \n",
    "    predictions = np.concatenate(preds)\n",
    "    # print('concat_preds: ', predictions)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True) # if fold = 0, then fold_index = 1, 2, 3, 4 are training data\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_texts = valid_folds['pn_history'].values\n",
    "    valid_labels = create_labels_for_scoring(valid_folds)\n",
    "    #print(valid_labels)\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    # num_workers set to 0 because of an error in this specific torch version for windows\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    best_score = 0.\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "        #avg_loss = 0.0\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n",
    "        \n",
    "        # scoring\n",
    "        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(valid_labels, preds)\n",
    "        # these have the same format\n",
    "        # print('valid labels: ', valid_labels)\n",
    "        #print('preds: ', preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "    # ERROR here:\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else torch.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "========== fold: 0 training ==========\n",
      "========== fold: 0 training ==========\n",
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/66] Elapsed 0m 0s (remain 0m 58s) Loss: 0.6847(0.6847) Grad: 5.3736  LR: 0.00002000  \n",
      "Epoch: [1][1/66] Elapsed 0m 1s (remain 0m 40s) Loss: 0.5244(0.6045) Grad: 4.3105  LR: 0.00001999  \n",
      "Epoch: [1][2/66] Elapsed 0m 1s (remain 0m 33s) Loss: 0.3804(0.5298) Grad: 3.5659  LR: 0.00001997  \n",
      "Epoch: [1][3/66] Elapsed 0m 1s (remain 0m 30s) Loss: 0.2828(0.4681) Grad: 2.7973  LR: 0.00001995  \n",
      "Epoch: [1][4/66] Elapsed 0m 2s (remain 0m 28s) Loss: 0.1718(0.4088) Grad: 2.1460  LR: 0.00001993  \n",
      "Epoch: [1][5/66] Elapsed 0m 2s (remain 0m 26s) Loss: 0.1380(0.3637) Grad: 1.3681  LR: 0.00001990  \n",
      "Epoch: [1][6/66] Elapsed 0m 3s (remain 0m 25s) Loss: 0.1218(0.3291) Grad: 0.8080  LR: 0.00001986  \n",
      "Epoch: [1][7/66] Elapsed 0m 3s (remain 0m 24s) Loss: 0.0707(0.2968) Grad: 0.7019  LR: 0.00001982  \n",
      "Epoch: [1][8/66] Elapsed 0m 3s (remain 0m 23s) Loss: 0.0935(0.2742) Grad: 0.3294  LR: 0.00001977  \n",
      "Epoch: [1][9/66] Elapsed 0m 4s (remain 0m 22s) Loss: 0.0773(0.2545) Grad: 0.1932  LR: 0.00001972  \n",
      "Epoch: [1][10/66] Elapsed 0m 4s (remain 0m 21s) Loss: 0.0941(0.2399) Grad: 0.1307  LR: 0.00001966  \n",
      "Epoch: [1][11/66] Elapsed 0m 4s (remain 0m 21s) Loss: 0.0270(0.2222) Grad: 0.2660  LR: 0.00001959  \n",
      "Epoch: [1][12/66] Elapsed 0m 5s (remain 0m 20s) Loss: 0.0640(0.2100) Grad: 0.0927  LR: 0.00001953  \n",
      "Epoch: [1][13/66] Elapsed 0m 5s (remain 0m 19s) Loss: 0.0782(0.2006) Grad: 0.1698  LR: 0.00001945  \n",
      "Epoch: [1][14/66] Elapsed 0m 5s (remain 0m 19s) Loss: 0.1000(0.1939) Grad: 0.9074  LR: 0.00001937  \n",
      "Epoch: [1][15/66] Elapsed 0m 6s (remain 0m 18s) Loss: 0.0460(0.1847) Grad: 0.0903  LR: 0.00001928  \n",
      "Epoch: [1][16/66] Elapsed 0m 6s (remain 0m 18s) Loss: 0.1409(0.1821) Grad: 0.5163  LR: 0.00001919  \n",
      "Epoch: [1][17/66] Elapsed 0m 6s (remain 0m 17s) Loss: 0.0969(0.1774) Grad: 0.2343  LR: 0.00001910  \n",
      "Epoch: [1][18/66] Elapsed 0m 7s (remain 0m 17s) Loss: 0.0768(0.1721) Grad: 0.0892  LR: 0.00001899  \n",
      "Epoch: [1][19/66] Elapsed 0m 7s (remain 0m 16s) Loss: 0.0914(0.1680) Grad: 0.1125  LR: 0.00001889  \n",
      "Epoch: [1][20/66] Elapsed 0m 7s (remain 0m 16s) Loss: 0.1068(0.1651) Grad: 0.2225  LR: 0.00001878  \n",
      "Epoch: [1][21/66] Elapsed 0m 8s (remain 0m 16s) Loss: 0.1178(0.1630) Grad: 0.1715  LR: 0.00001866  \n",
      "Epoch: [1][22/66] Elapsed 0m 8s (remain 0m 15s) Loss: 0.1120(0.1608) Grad: 0.2840  LR: 0.00001854  \n",
      "Epoch: [1][23/66] Elapsed 0m 8s (remain 0m 15s) Loss: 0.1248(0.1593) Grad: 0.3222  LR: 0.00001841  \n",
      "Epoch: [1][24/66] Elapsed 0m 9s (remain 0m 14s) Loss: 0.0577(0.1552) Grad: 0.7632  LR: 0.00001828  \n",
      "Epoch: [1][25/66] Elapsed 0m 9s (remain 0m 14s) Loss: 0.0644(0.1517) Grad: 0.4279  LR: 0.00001815  \n",
      "Epoch: [1][26/66] Elapsed 0m 9s (remain 0m 14s) Loss: 0.0932(0.1495) Grad: 0.1139  LR: 0.00001801  \n",
      "Epoch: [1][27/66] Elapsed 0m 10s (remain 0m 13s) Loss: 0.1056(0.1480) Grad: 0.3813  LR: 0.00001786  \n",
      "Epoch: [1][28/66] Elapsed 0m 10s (remain 0m 13s) Loss: 0.0971(0.1462) Grad: 0.1981  LR: 0.00001771  \n",
      "Epoch: [1][29/66] Elapsed 0m 10s (remain 0m 12s) Loss: 0.0492(0.1430) Grad: 0.1068  LR: 0.00001756  \n",
      "Epoch: [1][30/66] Elapsed 0m 11s (remain 0m 12s) Loss: 0.0727(0.1407) Grad: 0.0812  LR: 0.00001740  \n",
      "Epoch: [1][31/66] Elapsed 0m 11s (remain 0m 12s) Loss: 0.0494(0.1379) Grad: 0.0821  LR: 0.00001724  \n",
      "Epoch: [1][32/66] Elapsed 0m 11s (remain 0m 11s) Loss: 0.0398(0.1349) Grad: 0.1352  LR: 0.00001707  \n",
      "Epoch: [1][33/66] Elapsed 0m 12s (remain 0m 11s) Loss: 0.0791(0.1332) Grad: 0.1087  LR: 0.00001690  \n",
      "Epoch: [1][34/66] Elapsed 0m 12s (remain 0m 11s) Loss: 0.0810(0.1318) Grad: 0.1338  LR: 0.00001673  \n",
      "Epoch: [1][35/66] Elapsed 0m 12s (remain 0m 10s) Loss: 0.0890(0.1306) Grad: 0.1503  LR: 0.00001655  \n",
      "Epoch: [1][36/66] Elapsed 0m 13s (remain 0m 10s) Loss: 0.0623(0.1287) Grad: 0.0634  LR: 0.00001637  \n",
      "Epoch: [1][37/66] Elapsed 0m 13s (remain 0m 10s) Loss: 0.0925(0.1278) Grad: 0.1541  LR: 0.00001618  \n",
      "Epoch: [1][38/66] Elapsed 0m 14s (remain 0m 9s) Loss: 0.0792(0.1265) Grad: 0.0787  LR: 0.00001599  \n",
      "Epoch: [1][39/66] Elapsed 0m 14s (remain 0m 9s) Loss: 0.0968(0.1258) Grad: 0.1550  LR: 0.00001580  \n",
      "Epoch: [1][40/66] Elapsed 0m 14s (remain 0m 8s) Loss: 0.0509(0.1240) Grad: 0.1979  LR: 0.00001561  \n",
      "Epoch: [1][41/66] Elapsed 0m 15s (remain 0m 8s) Loss: 0.0771(0.1228) Grad: 0.1017  LR: 0.00001541  \n",
      "Epoch: [1][42/66] Elapsed 0m 15s (remain 0m 8s) Loss: 0.0916(0.1221) Grad: 0.0831  LR: 0.00001520  \n",
      "Epoch: [1][43/66] Elapsed 0m 15s (remain 0m 7s) Loss: 0.0759(0.1211) Grad: 0.1388  LR: 0.00001500  \n",
      "Epoch: [1][44/66] Elapsed 0m 16s (remain 0m 7s) Loss: 0.0897(0.1204) Grad: 0.1012  LR: 0.00001479  \n",
      "Epoch: [1][45/66] Elapsed 0m 16s (remain 0m 7s) Loss: 0.0572(0.1190) Grad: 0.2878  LR: 0.00001458  \n",
      "Epoch: [1][46/66] Elapsed 0m 16s (remain 0m 6s) Loss: 0.0861(0.1183) Grad: 0.1278  LR: 0.00001437  \n",
      "Epoch: [1][47/66] Elapsed 0m 17s (remain 0m 6s) Loss: 0.0630(0.1171) Grad: 0.2409  LR: 0.00001415  \n",
      "Epoch: [1][48/66] Elapsed 0m 17s (remain 0m 6s) Loss: 0.0620(0.1160) Grad: 0.2422  LR: 0.00001394  \n",
      "Epoch: [1][49/66] Elapsed 0m 17s (remain 0m 5s) Loss: 0.0458(0.1146) Grad: 0.3111  LR: 0.00001372  \n",
      "Epoch: [1][50/66] Elapsed 0m 18s (remain 0m 5s) Loss: 0.0807(0.1139) Grad: 0.0899  LR: 0.00001349  \n",
      "Epoch: [1][51/66] Elapsed 0m 18s (remain 0m 4s) Loss: 0.0618(0.1129) Grad: 0.1503  LR: 0.00001327  \n",
      "Epoch: [1][52/66] Elapsed 0m 18s (remain 0m 4s) Loss: 0.1636(0.1139) Grad: 0.5134  LR: 0.00001304  \n",
      "Epoch: [1][53/66] Elapsed 0m 19s (remain 0m 4s) Loss: 0.0747(0.1132) Grad: 0.0843  LR: 0.00001282  \n",
      "Epoch: [1][54/66] Elapsed 0m 19s (remain 0m 3s) Loss: 0.0714(0.1124) Grad: 0.1055  LR: 0.00001259  \n",
      "Epoch: [1][55/66] Elapsed 0m 19s (remain 0m 3s) Loss: 0.0736(0.1117) Grad: 0.0876  LR: 0.00001236  \n",
      "Epoch: [1][56/66] Elapsed 0m 20s (remain 0m 3s) Loss: 0.0914(0.1114) Grad: 0.0853  LR: 0.00001213  \n",
      "Epoch: [1][57/66] Elapsed 0m 20s (remain 0m 2s) Loss: 0.0705(0.1107) Grad: 0.1131  LR: 0.00001189  \n",
      "Epoch: [1][58/66] Elapsed 0m 20s (remain 0m 2s) Loss: 0.0564(0.1097) Grad: 0.1762  LR: 0.00001166  \n",
      "Epoch: [1][59/66] Elapsed 0m 21s (remain 0m 2s) Loss: 0.0843(0.1093) Grad: 0.1044  LR: 0.00001142  \n",
      "Epoch: [1][60/66] Elapsed 0m 21s (remain 0m 1s) Loss: 0.0593(0.1085) Grad: 0.1721  LR: 0.00001119  \n",
      "Epoch: [1][61/66] Elapsed 0m 21s (remain 0m 1s) Loss: 0.0947(0.1083) Grad: 0.1011  LR: 0.00001095  \n",
      "Epoch: [1][62/66] Elapsed 0m 22s (remain 0m 1s) Loss: 0.1080(0.1083) Grad: 0.1992  LR: 0.00001071  \n",
      "Epoch: [1][63/66] Elapsed 0m 22s (remain 0m 0s) Loss: 0.1265(0.1085) Grad: 0.2439  LR: 0.00001048  \n",
      "Epoch: [1][64/66] Elapsed 0m 22s (remain 0m 0s) Loss: 0.0688(0.1079) Grad: 0.1642  LR: 0.00001024  \n",
      "Epoch: [1][65/66] Elapsed 0m 23s (remain 0m 0s) Loss: 0.0846(0.1076) Grad: 0.1092  LR: 0.00001000  \n",
      "EVAL: [0/17] Elapsed 0m 0s (remain 0m 4s) Loss: 0.0793(0.0793) \n",
      "EVAL: [1/17] Elapsed 0m 0s (remain 0m 4s) Loss: 0.1484(0.1139) \n",
      "EVAL: [2/17] Elapsed 0m 0s (remain 0m 3s) Loss: 0.0403(0.0894) \n",
      "EVAL: [3/17] Elapsed 0m 1s (remain 0m 3s) Loss: 0.0516(0.0799) \n",
      "EVAL: [4/17] Elapsed 0m 1s (remain 0m 3s) Loss: 0.1066(0.0852) \n",
      "EVAL: [5/17] Elapsed 0m 1s (remain 0m 2s) Loss: 0.0920(0.0864) \n",
      "EVAL: [6/17] Elapsed 0m 1s (remain 0m 2s) Loss: 0.0474(0.0808) \n",
      "EVAL: [7/17] Elapsed 0m 2s (remain 0m 2s) Loss: 0.0981(0.0830) \n",
      "EVAL: [8/17] Elapsed 0m 2s (remain 0m 2s) Loss: 0.0703(0.0816) \n",
      "EVAL: [9/17] Elapsed 0m 2s (remain 0m 1s) Loss: 0.0794(0.0813) \n",
      "EVAL: [10/17] Elapsed 0m 2s (remain 0m 1s) Loss: 0.0705(0.0804) \n",
      "EVAL: [11/17] Elapsed 0m 3s (remain 0m 1s) Loss: 0.0803(0.0804) \n",
      "EVAL: [12/17] Elapsed 0m 3s (remain 0m 1s) Loss: 0.0532(0.0783) \n",
      "EVAL: [13/17] Elapsed 0m 3s (remain 0m 0s) Loss: 0.0865(0.0788) \n",
      "EVAL: [14/17] Elapsed 0m 3s (remain 0m 0s) Loss: 0.0804(0.0790) \n",
      "EVAL: [15/17] Elapsed 0m 3s (remain 0m 0s) Loss: 0.0673(0.0782) \n",
      "EVAL: [16/17] Elapsed 0m 4s (remain 0m 0s) Loss: 0.0647(0.0774) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.1076  avg_val_loss: 0.0774  time: 28s\n",
      "Epoch 1 - avg_train_loss: 0.1076  avg_val_loss: 0.0774  time: 28s\n",
      "Epoch 1 - avg_train_loss: 0.1076  avg_val_loss: 0.0774  time: 28s\n",
      "Epoch 1 - avg_train_loss: 0.1076  avg_val_loss: 0.0774  time: 28s\n",
      "Epoch 1 - Score: 0.0000\n",
      "Epoch 1 - Score: 0.0000\n",
      "Epoch 1 - Score: 0.0000\n",
      "Epoch 1 - Score: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/66] Elapsed 0m 0s (remain 0m 22s) Loss: 0.0791(0.0791) Grad: 0.1475  LR: 0.00000976  \n",
      "Epoch: [2][1/66] Elapsed 0m 0s (remain 0m 21s) Loss: 0.0691(0.0741) Grad: 0.2014  LR: 0.00000952  \n",
      "Epoch: [2][2/66] Elapsed 0m 1s (remain 0m 21s) Loss: 0.0823(0.0769) Grad: 0.1482  LR: 0.00000929  \n",
      "Epoch: [2][3/66] Elapsed 0m 1s (remain 0m 21s) Loss: 0.0574(0.0720) Grad: 0.2889  LR: 0.00000905  \n",
      "Epoch: [2][4/66] Elapsed 0m 1s (remain 0m 20s) Loss: 0.0803(0.0737) Grad: 0.1522  LR: 0.00000881  \n",
      "Epoch: [2][5/66] Elapsed 0m 2s (remain 0m 20s) Loss: 0.1028(0.0785) Grad: 0.1012  LR: 0.00000858  \n",
      "Epoch: [2][6/66] Elapsed 0m 2s (remain 0m 20s) Loss: 0.0768(0.0783) Grad: 0.1536  LR: 0.00000834  \n",
      "Epoch: [2][7/66] Elapsed 0m 2s (remain 0m 19s) Loss: 0.0676(0.0769) Grad: 0.1924  LR: 0.00000811  \n",
      "Epoch: [2][8/66] Elapsed 0m 3s (remain 0m 19s) Loss: 0.1025(0.0798) Grad: 0.0963  LR: 0.00000787  \n",
      "Epoch: [2][9/66] Elapsed 0m 3s (remain 0m 19s) Loss: 0.0617(0.0780) Grad: 0.2274  LR: 0.00000764  \n",
      "Epoch: [2][10/66] Elapsed 0m 3s (remain 0m 19s) Loss: 0.1121(0.0811) Grad: 0.1322  LR: 0.00000741  \n",
      "Epoch: [2][11/66] Elapsed 0m 4s (remain 0m 18s) Loss: 0.0911(0.0819) Grad: 0.0841  LR: 0.00000718  \n",
      "Epoch: [2][12/66] Elapsed 0m 4s (remain 0m 18s) Loss: 0.0745(0.0813) Grad: 0.1289  LR: 0.00000696  \n",
      "Epoch: [2][13/66] Elapsed 0m 4s (remain 0m 18s) Loss: 0.0755(0.0809) Grad: 0.1274  LR: 0.00000673  \n",
      "Epoch: [2][14/66] Elapsed 0m 5s (remain 0m 17s) Loss: 0.0686(0.0801) Grad: 0.1579  LR: 0.00000651  \n",
      "Epoch: [2][15/66] Elapsed 0m 5s (remain 0m 17s) Loss: 0.1040(0.0816) Grad: 0.1057  LR: 0.00000628  \n",
      "Epoch: [2][16/66] Elapsed 0m 5s (remain 0m 17s) Loss: 0.1246(0.0841) Grad: 0.2474  LR: 0.00000606  \n",
      "Epoch: [2][17/66] Elapsed 0m 6s (remain 0m 16s) Loss: 0.0723(0.0835) Grad: 0.1390  LR: 0.00000585  \n",
      "Epoch: [2][18/66] Elapsed 0m 6s (remain 0m 16s) Loss: 0.0774(0.0831) Grad: 0.1286  LR: 0.00000563  \n",
      "Epoch: [2][19/66] Elapsed 0m 6s (remain 0m 15s) Loss: 0.0812(0.0830) Grad: 0.1055  LR: 0.00000542  \n",
      "Epoch: [2][20/66] Elapsed 0m 7s (remain 0m 15s) Loss: 0.0678(0.0823) Grad: 0.1700  LR: 0.00000521  \n",
      "Epoch: [2][21/66] Elapsed 0m 7s (remain 0m 15s) Loss: 0.0854(0.0825) Grad: 0.0984  LR: 0.00000500  \n",
      "Epoch: [2][22/66] Elapsed 0m 7s (remain 0m 14s) Loss: 0.0944(0.0830) Grad: 0.0839  LR: 0.00000480  \n",
      "Epoch: [2][23/66] Elapsed 0m 8s (remain 0m 14s) Loss: 0.0869(0.0831) Grad: 0.0935  LR: 0.00000459  \n",
      "Epoch: [2][24/66] Elapsed 0m 8s (remain 0m 14s) Loss: 0.1195(0.0846) Grad: 0.1628  LR: 0.00000439  \n",
      "Epoch: [2][25/66] Elapsed 0m 8s (remain 0m 13s) Loss: 0.0752(0.0842) Grad: 0.1334  LR: 0.00000420  \n",
      "Epoch: [2][26/66] Elapsed 0m 9s (remain 0m 13s) Loss: 0.0650(0.0835) Grad: 0.2047  LR: 0.00000401  \n",
      "Epoch: [2][27/66] Elapsed 0m 9s (remain 0m 13s) Loss: 0.0735(0.0832) Grad: 0.1640  LR: 0.00000382  \n",
      "Epoch: [2][28/66] Elapsed 0m 9s (remain 0m 12s) Loss: 0.0611(0.0824) Grad: 0.2313  LR: 0.00000363  \n",
      "Epoch: [2][29/66] Elapsed 0m 10s (remain 0m 12s) Loss: 0.0658(0.0818) Grad: 0.2017  LR: 0.00000345  \n",
      "Epoch: [2][30/66] Elapsed 0m 10s (remain 0m 12s) Loss: 0.0839(0.0819) Grad: 0.1000  LR: 0.00000327  \n",
      "Epoch: [2][31/66] Elapsed 0m 10s (remain 0m 11s) Loss: 0.0671(0.0814) Grad: 0.1777  LR: 0.00000310  \n",
      "Epoch: [2][32/66] Elapsed 0m 11s (remain 0m 11s) Loss: 0.0583(0.0807) Grad: 0.2141  LR: 0.00000293  \n",
      "Epoch: [2][33/66] Elapsed 0m 11s (remain 0m 10s) Loss: 0.0620(0.0802) Grad: 0.1825  LR: 0.00000276  \n",
      "Epoch: [2][34/66] Elapsed 0m 12s (remain 0m 10s) Loss: 0.0473(0.0793) Grad: 0.2647  LR: 0.00000260  \n",
      "Epoch: [2][35/66] Elapsed 0m 12s (remain 0m 10s) Loss: 0.0970(0.0798) Grad: 0.0841  LR: 0.00000244  \n",
      "Epoch: [2][36/66] Elapsed 0m 12s (remain 0m 9s) Loss: 0.0768(0.0797) Grad: 0.1020  LR: 0.00000229  \n",
      "Epoch: [2][37/66] Elapsed 0m 13s (remain 0m 9s) Loss: 0.0834(0.0798) Grad: 0.0808  LR: 0.00000214  \n",
      "Epoch: [2][38/66] Elapsed 0m 13s (remain 0m 9s) Loss: 0.0898(0.0800) Grad: 0.0812  LR: 0.00000199  \n",
      "Epoch: [2][39/66] Elapsed 0m 13s (remain 0m 8s) Loss: 0.0511(0.0793) Grad: 0.2226  LR: 0.00000185  \n",
      "Epoch: [2][40/66] Elapsed 0m 14s (remain 0m 8s) Loss: 0.0734(0.0792) Grad: 0.1073  LR: 0.00000172  \n",
      "Epoch: [2][41/66] Elapsed 0m 14s (remain 0m 8s) Loss: 0.0519(0.0785) Grad: 0.2089  LR: 0.00000159  \n",
      "Epoch: [2][42/66] Elapsed 0m 14s (remain 0m 7s) Loss: 0.0646(0.0782) Grad: 0.1358  LR: 0.00000146  \n",
      "Epoch: [2][43/66] Elapsed 0m 15s (remain 0m 7s) Loss: 0.0775(0.0782) Grad: 0.0819  LR: 0.00000134  \n",
      "Epoch: [2][44/66] Elapsed 0m 15s (remain 0m 7s) Loss: 0.0564(0.0777) Grad: 0.1708  LR: 0.00000122  \n",
      "Epoch: [2][45/66] Elapsed 0m 15s (remain 0m 6s) Loss: 0.0428(0.0769) Grad: 0.2414  LR: 0.00000111  \n",
      "Epoch: [2][46/66] Elapsed 0m 16s (remain 0m 6s) Loss: 0.0549(0.0765) Grad: 0.1739  LR: 0.00000101  \n",
      "Epoch: [2][47/66] Elapsed 0m 16s (remain 0m 6s) Loss: 0.1375(0.0777) Grad: 0.3513  LR: 0.00000090  \n",
      "Epoch: [2][48/66] Elapsed 0m 17s (remain 0m 5s) Loss: 0.1260(0.0787) Grad: 0.2542  LR: 0.00000081  \n",
      "Epoch: [2][49/66] Elapsed 0m 17s (remain 0m 5s) Loss: 0.1413(0.0800) Grad: 0.3643  LR: 0.00000072  \n",
      "Epoch: [2][50/66] Elapsed 0m 17s (remain 0m 5s) Loss: 0.1189(0.0807) Grad: 0.2333  LR: 0.00000063  \n",
      "Epoch: [2][51/66] Elapsed 0m 18s (remain 0m 4s) Loss: 0.0988(0.0811) Grad: 0.1227  LR: 0.00000055  \n",
      "Epoch: [2][52/66] Elapsed 0m 18s (remain 0m 4s) Loss: 0.0738(0.0809) Grad: 0.0911  LR: 0.00000047  \n",
      "Epoch: [2][53/66] Elapsed 0m 18s (remain 0m 4s) Loss: 0.0423(0.0802) Grad: 0.2564  LR: 0.00000041  \n",
      "Epoch: [2][54/66] Elapsed 0m 19s (remain 0m 3s) Loss: 0.0921(0.0804) Grad: 0.0899  LR: 0.00000034  \n",
      "Epoch: [2][55/66] Elapsed 0m 19s (remain 0m 3s) Loss: 0.0637(0.0801) Grad: 0.1330  LR: 0.00000028  \n",
      "Epoch: [2][56/66] Elapsed 0m 19s (remain 0m 3s) Loss: 0.0741(0.0800) Grad: 0.0948  LR: 0.00000023  \n",
      "Epoch: [2][57/66] Elapsed 0m 20s (remain 0m 2s) Loss: 0.0730(0.0799) Grad: 0.0979  LR: 0.00000018  \n",
      "Epoch: [2][58/66] Elapsed 0m 20s (remain 0m 2s) Loss: 0.1405(0.0809) Grad: 0.3631  LR: 0.00000014  \n",
      "Epoch: [2][59/66] Elapsed 0m 20s (remain 0m 2s) Loss: 0.0762(0.0809) Grad: 0.0839  LR: 0.00000010  \n",
      "Epoch: [2][60/66] Elapsed 0m 21s (remain 0m 1s) Loss: 0.0571(0.0805) Grad: 0.1814  LR: 0.00000007  \n",
      "Epoch: [2][61/66] Elapsed 0m 21s (remain 0m 1s) Loss: 0.0934(0.0807) Grad: 0.1090  LR: 0.00000005  \n",
      "Epoch: [2][62/66] Elapsed 0m 21s (remain 0m 1s) Loss: 0.0822(0.0807) Grad: 0.0737  LR: 0.00000003  \n",
      "Epoch: [2][63/66] Elapsed 0m 22s (remain 0m 0s) Loss: 0.0609(0.0804) Grad: 0.1514  LR: 0.00000001  \n",
      "Epoch: [2][64/66] Elapsed 0m 22s (remain 0m 0s) Loss: 0.0633(0.0801) Grad: 0.1412  LR: 0.00000000  \n",
      "Epoch: [2][65/66] Elapsed 0m 22s (remain 0m 0s) Loss: 0.0818(0.0802) Grad: 0.0792  LR: 0.00000000  \n",
      "EVAL: [0/17] Elapsed 0m 0s (remain 0m 4s) Loss: 0.0788(0.0788) \n",
      "EVAL: [1/17] Elapsed 0m 0s (remain 0m 3s) Loss: 0.1507(0.1147) \n",
      "EVAL: [2/17] Elapsed 0m 0s (remain 0m 3s) Loss: 0.0380(0.0892) \n",
      "EVAL: [3/17] Elapsed 0m 1s (remain 0m 3s) Loss: 0.0498(0.0793) \n",
      "EVAL: [4/17] Elapsed 0m 1s (remain 0m 3s) Loss: 0.1072(0.0849) \n",
      "EVAL: [5/17] Elapsed 0m 1s (remain 0m 2s) Loss: 0.0919(0.0861) \n",
      "EVAL: [6/17] Elapsed 0m 1s (remain 0m 2s) Loss: 0.0454(0.0803) \n",
      "EVAL: [7/17] Elapsed 0m 2s (remain 0m 2s) Loss: 0.0982(0.0825) \n",
      "EVAL: [8/17] Elapsed 0m 2s (remain 0m 2s) Loss: 0.0694(0.0810) \n",
      "EVAL: [9/17] Elapsed 0m 2s (remain 0m 1s) Loss: 0.0788(0.0808) \n",
      "EVAL: [10/17] Elapsed 0m 2s (remain 0m 1s) Loss: 0.0695(0.0798) \n",
      "EVAL: [11/17] Elapsed 0m 3s (remain 0m 1s) Loss: 0.0799(0.0798) \n",
      "EVAL: [12/17] Elapsed 0m 3s (remain 0m 1s) Loss: 0.0514(0.0776) \n",
      "EVAL: [13/17] Elapsed 0m 3s (remain 0m 0s) Loss: 0.0862(0.0782) \n",
      "EVAL: [14/17] Elapsed 0m 3s (remain 0m 0s) Loss: 0.0799(0.0783) \n",
      "EVAL: [15/17] Elapsed 0m 4s (remain 0m 0s) Loss: 0.0662(0.0776) \n",
      "EVAL: [16/17] Elapsed 0m 4s (remain 0m 0s) Loss: 0.0635(0.0767) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0802  avg_val_loss: 0.0767  time: 28s\n",
      "Epoch 2 - avg_train_loss: 0.0802  avg_val_loss: 0.0767  time: 28s\n",
      "Epoch 2 - avg_train_loss: 0.0802  avg_val_loss: 0.0767  time: 28s\n",
      "Epoch 2 - avg_train_loss: 0.0802  avg_val_loss: 0.0767  time: 28s\n",
      "Epoch 2 - Score: 0.0000\n",
      "Epoch 2 - Score: 0.0000\n",
      "Epoch 2 - Score: 0.0000\n",
      "Epoch 2 - Score: 0.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (2860) does not match length of index (204)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13636\\4122689900.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrn_fold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                 \u001b[0m_oof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                 \u001b[0moof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moof_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_oof_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"========== fold: {fold} result ==========\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13636\\266586274.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(folds, fold)\u001b[0m\n\u001b[0;32m    118\u001b[0m     predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n\u001b[0;32m    119\u001b[0m                              map_location=torch.device('cpu'))['predictions']\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mvalid_folds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3598\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3599\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3600\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3601\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3602\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item_frame_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_setitem_array\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3647\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3648\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iset_not_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3650\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_iset_not_inplace\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3677\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3678\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0migetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3680\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3611\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3612\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3783\u001b[0m         \"\"\"\n\u001b[1;32m-> 3784\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3786\u001b[0m         if (\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4509\u001b[1;33m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         raise ValueError(\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[1;34m\"does not match length of index \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (2860) does not match length of index (204)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = create_labels_for_scoring(oof_df)\n",
    "        #print('labels: ', labels)\n",
    "        predictions = oof_df[[i for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        #print(preds)\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "        \n",
    "    if CFG.wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
