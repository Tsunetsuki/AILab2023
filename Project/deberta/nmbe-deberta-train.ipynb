{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start, the env setup \n",
    "1. use conda to create a new env\n",
    "```bash\n",
    "    conda create -n nmbe python=3.7\n",
    "    conda activate nmbe\n",
    "```\n",
    "\n",
    "2. install the following packages\n",
    "```bash\n",
    "    pip install torch==1.10.0\n",
    "    pip install sacremoses==0.0.41\n",
    "    pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # About this notebook\n",
    " - Deberta-base starter code\n",
    " - pip wheels is [here](https://www.kaggle.com/yasufuminakama/nbme-pip-wheels)\n",
    " - Inference notebook is [here](https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-inference)\n",
    "\n",
    " If this notebook is helpful, feel free to upvote :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "root = '.'\n",
    "OUTPUT_DIR = root + '/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    competition='NBME'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=True\n",
    "    apex=True\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model=\"microsoft/deberta-base\"\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=5\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=12\n",
    "    fc_dropout=0.2\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# wandb\n",
    "# ====================================================\n",
    "if CFG.wandb:\n",
    "    \n",
    "    import wandb\n",
    "\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "        wandb.login(key=secret_value_0)\n",
    "        anony = None\n",
    "    except:\n",
    "        anony = \"must\"\n",
    "        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "\n",
    "    def class2dict(f):\n",
    "        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "    run = wandb.init(project='NBME-Public', \n",
    "                     name=CFG.model,\n",
    "                     config=class2dict(CFG),\n",
    "                     group=CFG.model,\n",
    "                     job_type=\"train\",\n",
    "                     anonymous=anony)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.16.2\n",
      "Uninstalling transformers-4.16.2:\n",
      "  Successfully uninstalled transformers-4.16.2\n",
      "Looking in links: /Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/Mine/project/input/nbme-pip-wheels\n",
      "Processing ./input/nbme-pip-wheels/transformers-4.16.2-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: sacremoses in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (0.0.41)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from transformers) (6.7.0)\n",
      "Requirement already satisfied: fsspec in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: six in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from sacremoses->transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/shannon/anaconda3/envs/nmbe/lib/python3.7/site-packages (from sacremoses->transformers) (1.3.2)\n",
      "Installing collected packages: transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "optimum 1.15.0 requires transformers[sentencepiece]>=4.26.0, but you have transformers 4.16.2 which is incompatible.\n",
      "peft 0.3.0 requires torch>=1.13.0, but you have torch 1.10.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed transformers-4.16.2\n",
      "tokenizers.__version__: 0.13.3\n",
      "transformers.__version__: 4.16.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import ast\n",
    "import copy\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from torch.nn import Parameter\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.system('pip uninstall -y transformers')\n",
    "os.system('python -m pip install --no-index --find-links=/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/Mine/project/input/nbme-pip-wheels transformers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import (AutoConfig, AutoModel, AutoTokenizer,\n",
    "                          get_cosine_schedule_with_warmup,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (14300, 6)\n",
      "features.shape: (143, 3)\n",
      "patient_notes.shape: (42146, 3)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/train.csv')\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval) # ['mom'] -> [mom]\n",
    "train['location'] = train['location'].apply(ast.literal_eval) # ['0 1'] -> [0, 1]\n",
    "features = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/features.csv')\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "features = preprocess_features(features)\n",
    "patient_notes = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "#display(train.head())\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "#display(features.head())\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "#display(patient_notes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "#display(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "#display(train['annotation_length'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=CFG.n_fold) # for loop for 5 rounds, val_index is a list of index of validation data, others are index of training data\n",
    "groups = train['pn_num'].values # make sure same patient does not appear in train & valid at the same time \n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)): # Fold.split(x, y, groups), each fold has same distribution of y\n",
    "    train.loc[val_index, 'fold'] = int(n) # add fold column, and fill with fold number which this row belongs to evaluation\n",
    "train['fold'] = train['fold'].astype(int) # validation data is len(train)/5 = 2860, each fold 1~5 have 2860 testing data and 11440 training data \n",
    "# display(train.groupby('fold').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    # display(train.groupby('fold').size())\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    # display(train.groupby('fold').size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42146/42146 [00:21<00:00, 1971.68it/s]\n",
      "pn_history max(lengths): 425\n",
      "100%|██████████| 143/143 [00:00<00:00, 18838.08it/s]\n",
      "feature_text max(lengths): 30\n",
      "max_len: 458\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "for text_col in ['pn_history']:\n",
    "    pn_history_lengths = []\n",
    "    tk0 = tqdm(patient_notes[text_col].fillna(\"\").values, total=len(patient_notes)) # tqdm: progress bar \n",
    "    for text in tk0: # tk0 is a list of text which is text_col column of patient_notes \n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids']) # turn the sentence to index: len([1360, 12, 180, ...])\n",
    "        pn_history_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(pn_history_lengths)}')\n",
    "\n",
    "for text_col in ['feature_text']:\n",
    "    features_lengths = []\n",
    "    tk0 = tqdm(features[text_col].fillna(\"\").values, total=len(features))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids']) # 'Intermittent' -> ['In', 'term',...] -> [1360, 12, 180, ...]\n",
    "        features_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(features_lengths)}')\n",
    "\n",
    "CFG.max_len = max(pn_history_lengths) + max(features_lengths) + 3 # cls 開始 & sep 病例結果 & sep 特徵結尾 \n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text, feature_text):\n",
    "    inputs = cfg.tokenizer(text, feature_text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CFG.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items(): # k = \n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def create_label(cfg, text, annotation_length, location_list):\n",
    "    encoded = cfg.tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=CFG.max_len,\n",
    "                            padding=\"max_length\",\n",
    "                            return_offsets_mapping=True)\n",
    "    offset_mapping = encoded['offset_mapping'] # index of each token: (0, 0) (0, 1) (1, 2) (2, 3) (3, 4) ...\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0] # list all None token index\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1 # filled all None token index with = -1; [-1, 0, 0, ..., -1, -1, -1];  0 is no features, -1 is None token  \n",
    "    if annotation_length != 0:\n",
    "        for location in location_list: # location = '237 242;261 268' \n",
    "            for loc in [s.split() for s in location.split(';')]: # [['237', '242'], ['261', '268']] => loc = ['237', '242']\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]): # offset_mapping[idx] = (0, 0); start < 0 -> start_idx = 0 \n",
    "                        start_idx = idx - 1 # find start_ans is in the range of offset_mapping \n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]): \n",
    "                        end_idx = idx + 1 # label[start_idx:end_idx], the last token is not included, so end_idx + 1 \n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1 # each is token [-1, 0, 0, ..., 1, 1, ..., -1, -1, -1]; -1 is None token, 0 is no features, 1 is features\n",
    "    return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, \n",
    "                               self.pn_historys[item], \n",
    "                               self.feature_texts[item])\n",
    "        label = create_label(self.cfg, \n",
    "                             self.pn_historys[item], \n",
    "                             self.annotation_lengths[item], \n",
    "                             self.locations[item])\n",
    "        return inputs, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helpler functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    # scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter() # calculate the average loss of each batch, \n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        # with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "\n",
    "        y_preds = model(inputs) # [12, 458, 1]; batch is 12, max_len is 458 \n",
    "        \n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean() # choose the loss of label != -1, and calculate the mean of all none -1 loss\n",
    "        \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        \n",
    "        losses.update(loss.item(), batch_size) # loss=0.1, batch_size=12, losses.avg = (0.1*12 + 0.2*12 ...)/(12*num_batch)\n",
    "\n",
    "        loss.backward() # remove the scaler, bsc I don't have cuda: scaler.scale(loss).backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm) # constrain the gradient to be less than max_grad_norm, to avoid gradient explosion\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1 # global_step is including the epoch\n",
    "        \n",
    "        # if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "        #     scaler.step(optimizer)\n",
    "        #     scaler.update()\n",
    "        #     optimizer.zero_grad()\n",
    "        #     global_step += 1\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        end = time.time()\n",
    "        # if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "        print('Epoch: [{0}][{1}/{2}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                'Grad: {grad_norm:.4f}  '\n",
    "                'LR: {lr:.8f}  '\n",
    "                .format(epoch+1, step, len(train_loader), \n",
    "                        remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                        loss=losses,\n",
    "                        grad_norm=grad_norm,\n",
    "                        lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        # if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "        print('EVAL: [{0}/{1}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                .format(step, len(valid_loader),\n",
    "                        loss=losses,\n",
    "                        remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True) # if fold = 0, then fold_index = 1, 2, 3, 4 are training data\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_texts = valid_folds['pn_history'].values\n",
    "    valid_labels = create_labels_for_scoring(valid_folds)\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    best_score = 0.\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n",
    "        \n",
    "        # scoring\n",
    "        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/66] Elapsed 1m 46s (remain 115m 41s) Loss: 0.6128(0.6128) Grad: 11.4069  LR: 0.00002000  \n",
      "Epoch: [1][1/66] Elapsed 3m 36s (remain 115m 18s) Loss: 0.3612(0.4870) Grad: 6.9398  LR: 0.00001999  \n",
      "Epoch: [1][2/66] Elapsed 5m 27s (remain 114m 27s) Loss: 0.2315(0.4019) Grad: 3.6725  LR: 0.00001997  \n",
      "Epoch: [1][3/66] Elapsed 7m 17s (remain 113m 0s) Loss: 0.1243(0.3325) Grad: 2.2004  LR: 0.00001995  \n",
      "Epoch: [1][4/66] Elapsed 9m 6s (remain 111m 5s) Loss: 0.0920(0.2844) Grad: 0.9349  LR: 0.00001993  \n",
      "Epoch: [1][5/66] Elapsed 10m 53s (remain 108m 53s) Loss: 0.0948(0.2528) Grad: 0.1984  LR: 0.00001990  \n",
      "Epoch: [1][6/66] Elapsed 12m 40s (remain 106m 48s) Loss: 0.0326(0.2213) Grad: 0.2801  LR: 0.00001986  \n",
      "Epoch: [1][7/66] Elapsed 14m 25s (remain 104m 36s) Loss: 0.0704(0.2024) Grad: 0.2531  LR: 0.00001982  \n",
      "Epoch: [1][8/66] Elapsed 16m 13s (remain 102m 46s) Loss: 0.0735(0.1881) Grad: 0.3082  LR: 0.00001977  \n",
      "Epoch: [1][9/66] Elapsed 18m 1s (remain 100m 56s) Loss: 0.1209(0.1814) Grad: 0.5444  LR: 0.00001972  \n",
      "Epoch: [1][10/66] Elapsed 19m 46s (remain 98m 51s) Loss: 0.0758(0.1718) Grad: 0.3059  LR: 0.00001966  \n",
      "Epoch: [1][11/66] Elapsed 21m 32s (remain 96m 56s) Loss: 0.1941(0.1737) Grad: 0.9748  LR: 0.00001959  \n",
      "Epoch: [1][12/66] Elapsed 23m 19s (remain 95m 3s) Loss: 0.0820(0.1666) Grad: 0.3422  LR: 0.00001953  \n",
      "Epoch: [1][13/66] Elapsed 25m 5s (remain 93m 13s) Loss: 0.0760(0.1601) Grad: 0.3299  LR: 0.00001945  \n",
      "Epoch: [1][14/66] Elapsed 26m 54s (remain 91m 28s) Loss: 0.1152(0.1571) Grad: 0.4975  LR: 0.00001937  \n",
      "Epoch: [1][15/66] Elapsed 28m 39s (remain 89m 33s) Loss: 0.0692(0.1516) Grad: 0.2415  LR: 0.00001928  \n",
      "Epoch: [1][16/66] Elapsed 30m 24s (remain 87m 38s) Loss: 0.0758(0.1472) Grad: 0.2741  LR: 0.00001919  \n",
      "Epoch: [1][17/66] Elapsed 32m 9s (remain 85m 46s) Loss: 0.0920(0.1441) Grad: 0.3594  LR: 0.00001910  \n",
      "Epoch: [1][18/66] Elapsed 33m 57s (remain 83m 58s) Loss: 0.0946(0.1415) Grad: 0.3003  LR: 0.00001899  \n",
      "Epoch: [1][19/66] Elapsed 35m 39s (remain 82m 1s) Loss: 0.0785(0.1384) Grad: 0.1686  LR: 0.00001889  \n",
      "Epoch: [1][20/66] Elapsed 37m 23s (remain 80m 6s) Loss: 0.1040(0.1367) Grad: 0.2575  LR: 0.00001878  \n",
      "Epoch: [1][21/66] Elapsed 39m 10s (remain 78m 21s) Loss: 0.0969(0.1349) Grad: 0.2097  LR: 0.00001866  \n",
      "Epoch: [1][22/66] Elapsed 40m 58s (remain 76m 36s) Loss: 0.0835(0.1327) Grad: 0.3938  LR: 0.00001854  \n",
      "Epoch: [1][23/66] Elapsed 42m 43s (remain 74m 46s) Loss: 0.1324(0.1327) Grad: 0.2221  LR: 0.00001841  \n",
      "Epoch: [1][24/66] Elapsed 44m 32s (remain 73m 2s) Loss: 0.0892(0.1309) Grad: 0.3997  LR: 0.00001828  \n",
      "Epoch: [1][25/66] Elapsed 46m 19s (remain 71m 15s) Loss: 0.0722(0.1287) Grad: 0.6435  LR: 0.00001815  \n",
      "Epoch: [1][26/66] Elapsed 48m 7s (remain 69m 30s) Loss: 0.0734(0.1266) Grad: 0.3580  LR: 0.00001801  \n",
      "Epoch: [1][27/66] Elapsed 49m 52s (remain 67m 41s) Loss: 0.0644(0.1244) Grad: 0.3164  LR: 0.00001786  \n",
      "Epoch: [1][28/66] Elapsed 51m 37s (remain 65m 51s) Loss: 0.0595(0.1222) Grad: 0.2176  LR: 0.00001771  \n",
      "Epoch: [1][29/66] Elapsed 53m 21s (remain 64m 2s) Loss: 0.0620(0.1202) Grad: 0.1434  LR: 0.00001756  \n",
      "Epoch: [1][30/66] Elapsed 55m 4s (remain 62m 11s) Loss: 0.1123(0.1199) Grad: 0.2964  LR: 0.00001740  \n",
      "Epoch: [1][31/66] Elapsed 56m 47s (remain 60m 20s) Loss: 0.0962(0.1192) Grad: 0.2258  LR: 0.00001724  \n",
      "Epoch: [1][32/66] Elapsed 58m 30s (remain 58m 30s) Loss: 0.0642(0.1175) Grad: 0.1213  LR: 0.00001707  \n",
      "Epoch: [1][33/66] Elapsed 60m 12s (remain 56m 40s) Loss: 0.0583(0.1158) Grad: 0.0873  LR: 0.00001690  \n",
      "Epoch: [1][34/66] Elapsed 61m 56s (remain 54m 52s) Loss: 0.0336(0.1134) Grad: 0.1675  LR: 0.00001673  \n",
      "Epoch: [1][35/66] Elapsed 63m 41s (remain 53m 4s) Loss: 0.0692(0.1122) Grad: 0.1242  LR: 0.00001655  \n",
      "Epoch: [1][36/66] Elapsed 65m 24s (remain 51m 15s) Loss: 0.1368(0.1128) Grad: 0.4940  LR: 0.00001637  \n",
      "Epoch: [1][37/66] Elapsed 67m 8s (remain 49m 28s) Loss: 0.1000(0.1125) Grad: 0.3052  LR: 0.00001618  \n",
      "Epoch: [1][38/66] Elapsed 68m 53s (remain 47m 41s) Loss: 0.0995(0.1122) Grad: 0.2859  LR: 0.00001599  \n",
      "Epoch: [1][39/66] Elapsed 70m 36s (remain 45m 53s) Loss: 0.0767(0.1113) Grad: 0.1623  LR: 0.00001580  \n",
      "Epoch: [1][40/66] Elapsed 72m 18s (remain 44m 5s) Loss: 0.0874(0.1107) Grad: 0.1811  LR: 0.00001561  \n",
      "Epoch: [1][41/66] Elapsed 74m 1s (remain 42m 18s) Loss: 0.0560(0.1094) Grad: 0.1264  LR: 0.00001541  \n",
      "Epoch: [1][42/66] Elapsed 75m 44s (remain 40m 30s) Loss: 0.0766(0.1086) Grad: 0.1737  LR: 0.00001520  \n",
      "Epoch: [1][43/66] Elapsed 77m 28s (remain 38m 44s) Loss: 0.0454(0.1072) Grad: 0.1810  LR: 0.00001500  \n",
      "Epoch: [1][44/66] Elapsed 79m 13s (remain 36m 58s) Loss: 0.0599(0.1061) Grad: 0.1609  LR: 0.00001479  \n",
      "Epoch: [1][45/66] Elapsed 80m 58s (remain 35m 12s) Loss: 0.0751(0.1055) Grad: 0.1537  LR: 0.00001458  \n",
      "Epoch: [1][46/66] Elapsed 82m 42s (remain 33m 26s) Loss: 0.0859(0.1051) Grad: 0.1353  LR: 0.00001437  \n",
      "Epoch: [1][47/66] Elapsed 84m 27s (remain 31m 40s) Loss: 0.0567(0.1040) Grad: 0.1868  LR: 0.00001415  \n",
      "Epoch: [1][48/66] Elapsed 86m 12s (remain 29m 54s) Loss: 0.1440(0.1049) Grad: 0.5036  LR: 0.00001394  \n",
      "Epoch: [1][49/66] Elapsed 87m 57s (remain 28m 8s) Loss: 0.0596(0.1040) Grad: 0.1697  LR: 0.00001372  \n",
      "Epoch: [1][50/66] Elapsed 89m 40s (remain 26m 22s) Loss: 0.0430(0.1028) Grad: 0.2872  LR: 0.00001349  \n",
      "Epoch: [1][51/66] Elapsed 91m 25s (remain 24m 36s) Loss: 0.0714(0.1022) Grad: 0.1531  LR: 0.00001327  \n",
      "Epoch: [1][52/66] Elapsed 93m 11s (remain 22m 51s) Loss: 0.0561(0.1013) Grad: 0.2413  LR: 0.00001304  \n",
      "Epoch: [1][53/66] Elapsed 94m 56s (remain 21m 5s) Loss: 0.1301(0.1018) Grad: 0.3604  LR: 0.00001282  \n",
      "Epoch: [1][54/66] Elapsed 96m 39s (remain 19m 19s) Loss: 0.0935(0.1017) Grad: 0.1772  LR: 0.00001259  \n",
      "Epoch: [1][55/66] Elapsed 98m 22s (remain 17m 34s) Loss: 0.0741(0.1012) Grad: 0.1492  LR: 0.00001236  \n",
      "Epoch: [1][56/66] Elapsed 100m 6s (remain 15m 48s) Loss: 0.0821(0.1008) Grad: 0.1478  LR: 0.00001213  \n",
      "Epoch: [1][57/66] Elapsed 101m 49s (remain 14m 2s) Loss: 0.0730(0.1004) Grad: 0.1519  LR: 0.00001189  \n",
      "Epoch: [1][58/66] Elapsed 103m 33s (remain 12m 17s) Loss: 0.0522(0.0995) Grad: 2.5720  LR: 0.00001166  \n",
      "Epoch: [1][59/66] Elapsed 105m 18s (remain 10m 31s) Loss: 0.0623(0.0989) Grad: 0.1616  LR: 0.00001142  \n",
      "Epoch: [1][60/66] Elapsed 107m 0s (remain 8m 46s) Loss: 0.0565(0.0982) Grad: 0.1934  LR: 0.00001119  \n",
      "Epoch: [1][61/66] Elapsed 108m 44s (remain 7m 0s) Loss: 0.0472(0.0974) Grad: 0.2601  LR: 0.00001095  \n",
      "Epoch: [1][62/66] Elapsed 110m 27s (remain 5m 15s) Loss: 0.0829(0.0972) Grad: 0.1685  LR: 0.00001071  \n",
      "Epoch: [1][63/66] Elapsed 112m 10s (remain 3m 30s) Loss: 0.0556(0.0965) Grad: 0.2238  LR: 0.00001048  \n",
      "Epoch: [1][64/66] Elapsed 113m 53s (remain 1m 45s) Loss: 0.0591(0.0960) Grad: 0.1740  LR: 0.00001024  \n",
      "Epoch: [1][65/66] Elapsed 115m 36s (remain 0m 0s) Loss: 0.1005(0.0960) Grad: 0.3767  LR: 0.00001000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/17] Elapsed 0m 30s (remain 8m 9s) Loss: 0.0754(0.0754) \n",
      "EVAL: [1/17] Elapsed 1m 0s (remain 7m 35s) Loss: 0.1417(0.1086) \n",
      "EVAL: [2/17] Elapsed 1m 30s (remain 7m 4s) Loss: 0.0427(0.0866) \n",
      "EVAL: [3/17] Elapsed 2m 1s (remain 6m 33s) Loss: 0.0651(0.0812) \n",
      "EVAL: [4/17] Elapsed 2m 31s (remain 6m 3s) Loss: 0.0949(0.0840) \n",
      "EVAL: [5/17] Elapsed 3m 1s (remain 5m 32s) Loss: 0.0845(0.0841) \n",
      "EVAL: [6/17] Elapsed 3m 31s (remain 5m 2s) Loss: 0.0373(0.0774) \n",
      "EVAL: [7/17] Elapsed 4m 1s (remain 4m 32s) Loss: 0.0809(0.0778) \n",
      "EVAL: [8/17] Elapsed 4m 32s (remain 4m 1s) Loss: 0.0566(0.0755) \n",
      "EVAL: [9/17] Elapsed 5m 2s (remain 3m 31s) Loss: 0.0721(0.0751) \n",
      "EVAL: [10/17] Elapsed 5m 32s (remain 3m 1s) Loss: 0.0564(0.0734) \n",
      "EVAL: [11/17] Elapsed 6m 2s (remain 2m 31s) Loss: 0.0789(0.0739) \n",
      "EVAL: [12/17] Elapsed 6m 32s (remain 2m 0s) Loss: 0.0568(0.0726) \n",
      "EVAL: [13/17] Elapsed 7m 2s (remain 1m 30s) Loss: 0.0809(0.0732) \n",
      "EVAL: [14/17] Elapsed 7m 33s (remain 1m 0s) Loss: 0.0742(0.0732) \n",
      "EVAL: [15/17] Elapsed 8m 3s (remain 0m 30s) Loss: 0.0536(0.0720) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.0960  avg_val_loss: 0.0710  time: 7451s\n",
      "Epoch 1 - Score: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [16/17] Elapsed 8m 34s (remain 0m 0s) Loss: 0.0544(0.0710) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/66] Elapsed 1m 45s (remain 113m 55s) Loss: 0.0729(0.0729) Grad: 0.2125  LR: 0.00000976  \n",
      "Epoch: [2][1/66] Elapsed 3m 29s (remain 111m 30s) Loss: 0.0596(0.0662) Grad: 0.1784  LR: 0.00000952  \n",
      "Epoch: [2][2/66] Elapsed 5m 12s (remain 109m 19s) Loss: 0.0476(0.0600) Grad: 0.1371  LR: 0.00000929  \n",
      "Epoch: [2][3/66] Elapsed 6m 56s (remain 107m 31s) Loss: 0.0482(0.0571) Grad: 0.1472  LR: 0.00000905  \n",
      "Epoch: [2][4/66] Elapsed 8m 39s (remain 105m 34s) Loss: 0.0344(0.0526) Grad: 0.1905  LR: 0.00000881  \n",
      "Epoch: [2][5/66] Elapsed 10m 23s (remain 103m 51s) Loss: 0.0885(0.0585) Grad: 0.3004  LR: 0.00000858  \n",
      "Epoch: [2][6/66] Elapsed 12m 6s (remain 102m 2s) Loss: 0.1023(0.0648) Grad: 0.4951  LR: 0.00000834  \n",
      "Epoch: [2][7/66] Elapsed 13m 49s (remain 100m 14s) Loss: 0.0602(0.0642) Grad: 0.1678  LR: 0.00000811  \n",
      "Epoch: [2][8/66] Elapsed 15m 33s (remain 98m 30s) Loss: 0.0882(0.0669) Grad: 0.2880  LR: 0.00000787  \n",
      "Epoch: [2][9/66] Elapsed 17m 17s (remain 96m 47s) Loss: 0.0566(0.0659) Grad: 0.1693  LR: 0.00000764  \n",
      "Epoch: [2][10/66] Elapsed 19m 0s (remain 95m 2s) Loss: 0.0655(0.0658) Grad: 0.2389  LR: 0.00000741  \n",
      "Epoch: [2][11/66] Elapsed 20m 44s (remain 93m 21s) Loss: 0.0520(0.0647) Grad: 0.1564  LR: 0.00000718  \n",
      "Epoch: [2][12/66] Elapsed 22m 28s (remain 91m 39s) Loss: 0.0495(0.0635) Grad: 0.2056  LR: 0.00000696  \n",
      "Epoch: [2][13/66] Elapsed 24m 13s (remain 89m 58s) Loss: 0.0709(0.0640) Grad: 0.1917  LR: 0.00000673  \n",
      "Epoch: [2][14/66] Elapsed 25m 59s (remain 88m 20s) Loss: 0.0584(0.0637) Grad: 0.1878  LR: 0.00000651  \n",
      "Epoch: [2][15/66] Elapsed 27m 44s (remain 86m 40s) Loss: 0.0700(0.0641) Grad: 0.2238  LR: 0.00000628  \n",
      "Epoch: [2][16/66] Elapsed 29m 28s (remain 84m 57s) Loss: 0.1056(0.0665) Grad: 0.3977  LR: 0.00000606  \n",
      "Epoch: [2][17/66] Elapsed 46m 36s (remain 124m 18s) Loss: 0.1104(0.0689) Grad: 0.7015  LR: 0.00000585  \n",
      "Epoch: [2][18/66] Elapsed 48m 16s (remain 119m 25s) Loss: 0.0367(0.0672) Grad: 0.2327  LR: 0.00000563  \n",
      "Epoch: [2][19/66] Elapsed 49m 58s (remain 114m 57s) Loss: 0.0535(0.0665) Grad: 0.2805  LR: 0.00000542  \n",
      "Epoch: [2][20/66] Elapsed 66m 41s (remain 142m 54s) Loss: 0.0493(0.0657) Grad: 0.1995  LR: 0.00000521  \n",
      "Epoch: [2][21/66] Elapsed 83m 23s (remain 166m 46s) Loss: 0.0663(0.0658) Grad: 0.7827  LR: 0.00000500  \n",
      "Epoch: [2][22/66] Elapsed 95m 7s (remain 177m 49s) Loss: 0.0802(0.0664) Grad: 0.3019  LR: 0.00000480  \n",
      "Epoch: [2][23/66] Elapsed 96m 46s (remain 169m 22s) Loss: 0.0659(0.0664) Grad: 0.2919  LR: 0.00000459  \n",
      "Epoch: [2][24/66] Elapsed 124m 4s (remain 203m 29s) Loss: 0.0829(0.0670) Grad: 0.2988  LR: 0.00000439  \n",
      "Epoch: [2][25/66] Elapsed 125m 47s (remain 193m 31s) Loss: 0.0818(0.0676) Grad: 0.3125  LR: 0.00000420  \n",
      "Epoch: [2][26/66] Elapsed 127m 29s (remain 184m 9s) Loss: 0.0572(0.0672) Grad: 0.3533  LR: 0.00000401  \n",
      "Epoch: [2][27/66] Elapsed 129m 12s (remain 175m 20s) Loss: 0.0555(0.0668) Grad: 0.2628  LR: 0.00000382  \n",
      "Epoch: [2][28/66] Elapsed 130m 54s (remain 167m 1s) Loss: 0.0660(0.0668) Grad: 0.3235  LR: 0.00000363  \n",
      "Epoch: [2][29/66] Elapsed 132m 37s (remain 159m 8s) Loss: 0.0356(0.0657) Grad: 0.4357  LR: 0.00000345  \n",
      "Epoch: [2][30/66] Elapsed 134m 20s (remain 151m 40s) Loss: 0.0507(0.0652) Grad: 0.2380  LR: 0.00000327  \n",
      "Epoch: [2][31/66] Elapsed 136m 3s (remain 144m 33s) Loss: 0.0473(0.0647) Grad: 0.2444  LR: 0.00000310  \n",
      "Epoch: [2][32/66] Elapsed 137m 46s (remain 137m 46s) Loss: 0.0561(0.0644) Grad: 0.4267  LR: 0.00000293  \n",
      "Epoch: [2][33/66] Elapsed 139m 30s (remain 131m 18s) Loss: 0.0605(0.0643) Grad: 0.5946  LR: 0.00000276  \n",
      "Epoch: [2][34/66] Elapsed 141m 13s (remain 125m 4s) Loss: 0.0637(0.0643) Grad: 0.2707  LR: 0.00000260  \n",
      "Epoch: [2][35/66] Elapsed 142m 56s (remain 119m 7s) Loss: 0.0644(0.0643) Grad: 0.6071  LR: 0.00000244  \n",
      "Epoch: [2][36/66] Elapsed 144m 39s (remain 113m 23s) Loss: 0.0651(0.0643) Grad: 0.4434  LR: 0.00000229  \n",
      "Epoch: [2][37/66] Elapsed 146m 23s (remain 107m 52s) Loss: 0.0850(0.0649) Grad: 0.6317  LR: 0.00000214  \n",
      "Epoch: [2][38/66] Elapsed 148m 7s (remain 102m 32s) Loss: 0.0448(0.0643) Grad: 0.9363  LR: 0.00000199  \n",
      "Epoch: [2][39/66] Elapsed 149m 51s (remain 97m 24s) Loss: 0.0537(0.0641) Grad: 0.2722  LR: 0.00000185  \n",
      "Epoch: [2][40/66] Elapsed 151m 34s (remain 92m 25s) Loss: 0.0706(0.0642) Grad: 0.2952  LR: 0.00000172  \n",
      "Epoch: [2][41/66] Elapsed 153m 18s (remain 87m 36s) Loss: 0.0472(0.0638) Grad: 0.2718  LR: 0.00000159  \n",
      "Epoch: [2][42/66] Elapsed 155m 1s (remain 82m 55s) Loss: 0.0505(0.0635) Grad: 0.2862  LR: 0.00000146  \n",
      "Epoch: [2][43/66] Elapsed 156m 46s (remain 78m 23s) Loss: 0.0412(0.0630) Grad: 0.2026  LR: 0.00000134  \n",
      "Epoch: [2][44/66] Elapsed 158m 28s (remain 73m 57s) Loss: 0.0502(0.0627) Grad: 0.2523  LR: 0.00000122  \n",
      "Epoch: [2][45/66] Elapsed 160m 12s (remain 69m 39s) Loss: 0.0657(0.0628) Grad: 0.4291  LR: 0.00000111  \n",
      "Epoch: [2][46/66] Elapsed 161m 56s (remain 65m 27s) Loss: 0.0659(0.0629) Grad: 0.3392  LR: 0.00000101  \n",
      "Epoch: [2][47/66] Elapsed 163m 39s (remain 61m 22s) Loss: 0.0676(0.0630) Grad: 0.3806  LR: 0.00000090  \n",
      "Epoch: [2][48/66] Elapsed 165m 23s (remain 57m 22s) Loss: 0.1144(0.0640) Grad: 1.0714  LR: 0.00000081  \n",
      "Epoch: [2][49/66] Elapsed 167m 6s (remain 53m 28s) Loss: 0.0550(0.0638) Grad: 0.4560  LR: 0.00000072  \n",
      "Epoch: [2][50/66] Elapsed 168m 49s (remain 49m 39s) Loss: 0.0775(0.0641) Grad: 0.4489  LR: 0.00000063  \n",
      "Epoch: [2][51/66] Elapsed 170m 32s (remain 45m 54s) Loss: 0.0477(0.0638) Grad: 0.2678  LR: 0.00000055  \n",
      "Epoch: [2][52/66] Elapsed 172m 17s (remain 42m 15s) Loss: 0.0687(0.0639) Grad: 0.3491  LR: 0.00000047  \n",
      "Epoch: [2][53/66] Elapsed 174m 2s (remain 38m 40s) Loss: 0.0282(0.0632) Grad: 0.4156  LR: 0.00000041  \n",
      "Epoch: [2][54/66] Elapsed 175m 46s (remain 35m 9s) Loss: 0.0337(0.0627) Grad: 0.2279  LR: 0.00000034  \n",
      "Epoch: [2][55/66] Elapsed 177m 31s (remain 31m 42s) Loss: 0.0624(0.0627) Grad: 0.4310  LR: 0.00000028  \n",
      "Epoch: [2][56/66] Elapsed 179m 16s (remain 28m 18s) Loss: 0.0484(0.0624) Grad: 0.3444  LR: 0.00000023  \n",
      "Epoch: [2][57/66] Elapsed 181m 0s (remain 24m 58s) Loss: 0.0628(0.0624) Grad: 0.4851  LR: 0.00000018  \n",
      "Epoch: [2][58/66] Elapsed 182m 45s (remain 21m 40s) Loss: 0.0504(0.0622) Grad: 0.3066  LR: 0.00000014  \n",
      "Epoch: [2][59/66] Elapsed 184m 29s (remain 18m 26s) Loss: 0.0669(0.0623) Grad: 0.4067  LR: 0.00000010  \n",
      "Epoch: [2][60/66] Elapsed 186m 15s (remain 15m 15s) Loss: 0.1585(0.0639) Grad: 0.9694  LR: 0.00000007  \n",
      "Epoch: [2][61/66] Elapsed 204m 14s (remain 13m 10s) Loss: 0.0435(0.0636) Grad: 0.4119  LR: 0.00000005  \n",
      "Epoch: [2][62/66] Elapsed 216m 11s (remain 10m 17s) Loss: 0.0950(0.0640) Grad: 0.8422  LR: 0.00000003  \n",
      "Epoch: [2][63/66] Elapsed 228m 39s (remain 7m 8s) Loss: 0.0765(0.0642) Grad: 0.2760  LR: 0.00000001  \n",
      "Epoch: [2][64/66] Elapsed 246m 53s (remain 3m 47s) Loss: 0.0579(0.0641) Grad: 0.3779  LR: 0.00000000  \n",
      "Epoch: [2][65/66] Elapsed 263m 55s (remain 0m 0s) Loss: 0.0403(0.0638) Grad: 0.3150  LR: 0.00000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "[W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/17] Elapsed 0m 29s (remain 7m 58s) Loss: 0.0646(0.0646) \n",
      "EVAL: [1/17] Elapsed 0m 59s (remain 7m 24s) Loss: 0.1278(0.0962) \n",
      "EVAL: [2/17] Elapsed 18m 14s (remain 85m 9s) Loss: 0.0347(0.0757) \n",
      "EVAL: [3/17] Elapsed 18m 44s (remain 60m 53s) Loss: 0.0526(0.0699) \n",
      "EVAL: [4/17] Elapsed 19m 13s (remain 46m 8s) Loss: 0.0807(0.0721) \n",
      "EVAL: [5/17] Elapsed 19m 42s (remain 36m 8s) Loss: 0.0717(0.0720) \n",
      "EVAL: [6/17] Elapsed 36m 17s (remain 51m 50s) Loss: 0.0302(0.0660) \n",
      "EVAL: [7/17] Elapsed 36m 46s (remain 41m 22s) Loss: 0.0634(0.0657) \n",
      "EVAL: [8/17] Elapsed 37m 15s (remain 33m 7s) Loss: 0.0418(0.0631) \n",
      "EVAL: [9/17] Elapsed 37m 45s (remain 26m 25s) Loss: 0.0583(0.0626) \n",
      "EVAL: [10/17] Elapsed 38m 14s (remain 20m 51s) Loss: 0.0390(0.0604) \n",
      "EVAL: [11/17] Elapsed 53m 52s (remain 22m 26s) Loss: 0.0693(0.0612) \n",
      "EVAL: [12/17] Elapsed 54m 21s (remain 16m 43s) Loss: 0.0508(0.0604) \n",
      "EVAL: [13/17] Elapsed 54m 51s (remain 11m 45s) Loss: 0.0583(0.0602) \n",
      "EVAL: [14/17] Elapsed 55m 20s (remain 7m 22s) Loss: 0.0528(0.0597) \n",
      "EVAL: [15/17] Elapsed 59m 42s (remain 3m 43s) Loss: 0.0443(0.0588) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0638  avg_val_loss: 0.0584  time: 19447s\n",
      "Epoch 2 - Score: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [16/17] Elapsed 60m 11s (remain 0m 0s) Loss: 0.0523(0.0584) \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/Mine/project/microsoft-deberta-base_fold0_best.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8q/0b9vgsl108v8ggxr7smnrwm40000gn/T/ipykernel_10716/3529880504.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_fold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0m_oof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0moof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moof_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_oof_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"========== fold: {fold} result ==========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8q/0b9vgsl108v8ggxr7smnrwm40000gn/T/ipykernel_10716/549101437.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(folds, fold)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n\u001b[0;32m--> 113\u001b[0;31m                              map_location=torch.device('cpu'))['predictions']\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mvalid_folds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nmbe/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nmbe/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nmbe/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/Mine/project/microsoft-deberta-base_fold0_best.pth'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = create_labels_for_scoring(oof_df)\n",
    "        predictions = oof_df[[i for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "        \n",
    "    if CFG.wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
