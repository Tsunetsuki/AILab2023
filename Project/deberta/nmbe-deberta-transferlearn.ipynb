{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start, the env setup \n",
    "1. use conda to create a new env\n",
    "```bash\n",
    "    conda create -n nmbe python=3.7\n",
    "    conda activate nmbe\n",
    "```\n",
    "\n",
    "2. install the following packages\n",
    "```bash\n",
    "    pip install torch==1.10.0\n",
    "    pip install sacremoses==0.0.41\n",
    "    pip install transformers\n",
    "```\n",
    "```bash if using cuda\n",
    "    pip install torch==1.10.0.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    pip install sacremoses==0.0.41\n",
    "    pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # About this notebook\n",
    " - Deberta-base starter code\n",
    " - pip wheels is [here](https://www.kaggle.com/yasufuminakama/nbme-pip-wheels)\n",
    " - Inference notebook is [here](https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-inference)\n",
    "\n",
    " If this notebook is helpful, feel free to upvote :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "root = '.'\n",
    "OUTPUT_DIR = root + '/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG (Classifier Free Guidance?)\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    # These are not necessary - delete?\n",
    "    #competition='NBME'\n",
    "    #_wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model=\"microsoft/deberta-base\"\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=5\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=12\n",
    "    fc_dropout=0.2\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    # possible to simply delete folds that are already trained\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.13.3\n",
      "transformers.__version__: 4.16.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import ast\n",
    "import copy\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from torch.nn import Parameter\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.system('pip uninstall -y transformers')\n",
    "os.system('python -m pip install --no-index --find-links=./input/nbme-pip-wheels transformers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import (AutoConfig, AutoModel, AutoTokenizer,\n",
    "                          get_cosine_schedule_with_warmup,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = \"cpu\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_scaler = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'transferlearn'):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed) if torch.cuda.is_available() else torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (14300, 6)\n",
      "features.shape: (143, 3)\n",
      "patient_notes.shape: (42146, 3)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/train.csv')\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval) # ['mom'] -> [mom]\n",
    "train['location'] = train['location'].apply(ast.literal_eval) # ['0 1'] -> [0, 1]\n",
    "features = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/features.csv')\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "features = preprocess_features(features)\n",
    "patient_notes = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "#display(train.head())\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "#display(features.head())\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "#display(patient_notes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "#display(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "#display(train['annotation_length'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=CFG.n_fold) # for loop for 5 rounds, val_index is a list of index of validation data, others are index of training data\n",
    "groups = train['pn_num'].values # make sure same patient does not appear in train & valid at the same time \n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)): # Fold.split(x, y, groups), each fold has same distribution of y\n",
    "    train.loc[val_index, 'fold'] = int(n) # add fold column, and fill with fold number which this row belongs to evaluation\n",
    "train['fold'] = train['fold'].astype(int) # validation data is len(train)/5 = 2860, each fold 1~5 have 2860 testing data and 11440 training data \n",
    "# display(train.groupby('fold').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    # display(train.groupby('fold').size())\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    # display(train.groupby('fold').size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 48/42146 [00:00<01:28, 476.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42146/42146 [00:55<00:00, 757.20it/s]\n",
      "pn_history max(lengths): 433\n",
      "pn_history max(lengths): 433\n",
      "100%|██████████| 143/143 [00:00<00:00, 4344.76it/s]\n",
      "feature_text max(lengths): 30\n",
      "feature_text max(lengths): 30\n",
      "max_len: 466\n",
      "max_len: 466\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "for text_col in ['pn_history']:\n",
    "    pn_history_lengths = []\n",
    "    tk0 = tqdm(patient_notes[text_col].fillna(\"\").values, total=len(patient_notes)) # tqdm: progress bar \n",
    "    for text in tk0: # tk0 is a list of text which is text_col column of patient_notes \n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids']) # turn the sentence to index: len([1360, 12, 180, ...])\n",
    "        pn_history_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(pn_history_lengths)}')\n",
    "\n",
    "for text_col in ['feature_text']:\n",
    "    features_lengths = []\n",
    "    tk0 = tqdm(features[text_col].fillna(\"\").values, total=len(features))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids']) # 'Intermittent' -> ['In', 'term',...] -> [1360, 12, 180, ...]\n",
    "        features_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(features_lengths)}')\n",
    "\n",
    "CFG.max_len = max(pn_history_lengths) + max(features_lengths) + 3 # cls 開始 & sep 病例結果 & sep 特徵結尾 \n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text, feature_text):\n",
    "    inputs = cfg.tokenizer(text, feature_text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CFG.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items(): # k = \n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def create_label(cfg, text, annotation_length, location_list):\n",
    "    encoded = cfg.tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=CFG.max_len,\n",
    "                            padding=\"max_length\",\n",
    "                            return_offsets_mapping=True)\n",
    "    offset_mapping = encoded['offset_mapping'] # index of each token: (0, 0) (0, 1) (1, 2) (2, 3) (3, 4) ...\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0] # list all None token index\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1 # filled all None token index with = -1; [-1, 0, 0, ..., -1, -1, -1];  0 is no features, -1 is None token  \n",
    "    if annotation_length != 0:\n",
    "        for location in location_list: # location = '237 242;261 268' \n",
    "            for loc in [s.split() for s in location.split(';')]: # [['237', '242'], ['261', '268']] => loc = ['237', '242']\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]): # offset_mapping[idx] = (0, 0); start < 0 -> start_idx = 0 \n",
    "                        start_idx = idx - 1 # find start_ans is in the range of offset_mapping \n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]): \n",
    "                        end_idx = idx + 1 # label[start_idx:end_idx], the last token is not included, so end_idx + 1 \n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1 # each is token [-1, 0, 0, ..., 1, 1, ..., -1, -1, -1]; -1 is None token, 0 is no features, 1 is features\n",
    "    return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, \n",
    "                               self.pn_historys[item], \n",
    "                               self.feature_texts[item])\n",
    "        label = create_label(self.cfg, \n",
    "                             self.pn_historys[item], \n",
    "                             self.annotation_lengths[item], \n",
    "                             self.locations[item])\n",
    "        return inputs, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        # only last linear layer is trained\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        # TODO: only train this layer\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helpler functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    if use_scaler:\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)  \n",
    "    losses = AverageMeter() # calculate the average loss of each batch, \n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        if torch.cuda.is_available():\n",
    "            with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "                y_preds = model(inputs) # [12, 458, 1]; batch is 12, max_len is 458 \n",
    "        else:\n",
    "            y_preds = model(inputs)\n",
    "        \n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean() # choose the loss of label != -1, and calculate the mean of all none -1 loss\n",
    "        \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        \n",
    "        losses.update(loss.item(), batch_size) # loss=0.1, batch_size=12, losses.avg = (0.1*12 + 0.2*12 ...)/(12*num_batch)\n",
    "\n",
    "        scaler.scale(loss).backward() if use_scaler else loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm) # constrain the gradient to be less than max_grad_norm, to avoid gradient explosion\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1 # global_step is including the epoch\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0 and use_scaler:\n",
    "             scaler.step(optimizer)\n",
    "             scaler.update()\n",
    "             optimizer.zero_grad()\n",
    "             global_step += 1\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        end = time.time()\n",
    "        #if step % CFG.print_freq == 0 or step == (len(train_loader)-1) and torch.cuda.is_available():\n",
    "        print('Epoch: [{0}][{1}/{2}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                'Grad: {grad_norm:.4f}  '\n",
    "                'LR: {lr:.8f}  '\n",
    "                .format(epoch+1, step, len(train_loader), \n",
    "                        remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                        loss=losses,\n",
    "                        grad_norm=grad_norm,\n",
    "                        lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        # if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "        print('EVAL: [{0}/{1}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                .format(step, len(valid_loader),\n",
    "                        loss=losses,\n",
    "                        remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True) # if fold = 0, then fold_index = 1, 2, 3, 4 are training data\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_texts = valid_folds['pn_history'].values\n",
    "    valid_labels = create_labels_for_scoring(valid_folds)\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    # num_workers set to 0 because of an error in this specific torch version for windows\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    # TODO: understand this\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    best_score = 0.\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n",
    "        \n",
    "        # scoring\n",
    "        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else torch.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/953] Elapsed 0m 2s (remain 35m 29s) Loss: 0.6621(0.6621) Grad: 10.1974  LR: 0.00002000  \n",
      "Epoch: [1][1/953] Elapsed 0m 4s (remain 32m 30s) Loss: 0.6748(0.6684) Grad: 10.1288  LR: 0.00002000  \n",
      "Epoch: [1][2/953] Elapsed 0m 5s (remain 31m 14s) Loss: 0.6769(0.6712) Grad: 10.0725  LR: 0.00002000  \n",
      "Epoch: [1][3/953] Elapsed 0m 7s (remain 30m 29s) Loss: 0.6686(0.6706) Grad: 9.8396  LR: 0.00002000  \n",
      "Epoch: [1][4/953] Elapsed 0m 9s (remain 30m 24s) Loss: 0.6522(0.6669) Grad: 9.7550  LR: 0.00002000  \n",
      "Epoch: [1][5/953] Elapsed 0m 11s (remain 31m 14s) Loss: 0.6805(0.6692) Grad: 10.1057  LR: 0.00002000  \n",
      "Epoch: [1][6/953] Elapsed 0m 14s (remain 32m 59s) Loss: 0.6615(0.6681) Grad: 10.0797  LR: 0.00002000  \n",
      "Epoch: [1][7/953] Elapsed 0m 17s (remain 34m 11s) Loss: 0.6690(0.6682) Grad: 10.2556  LR: 0.00002000  \n",
      "Epoch: [1][8/953] Elapsed 0m 20s (remain 35m 25s) Loss: 0.6539(0.6666) Grad: 9.7914  LR: 0.00002000  \n",
      "Epoch: [1][9/953] Elapsed 0m 23s (remain 36m 19s) Loss: 0.6509(0.6650) Grad: 9.7698  LR: 0.00002000  \n",
      "Epoch: [1][10/953] Elapsed 0m 25s (remain 36m 57s) Loss: 0.6410(0.6629) Grad: 9.6633  LR: 0.00002000  \n",
      "Epoch: [1][11/953] Elapsed 0m 28s (remain 37m 27s) Loss: 0.6633(0.6629) Grad: 9.8465  LR: 0.00002000  \n",
      "Epoch: [1][12/953] Elapsed 0m 31s (remain 37m 47s) Loss: 0.6543(0.6622) Grad: 9.9302  LR: 0.00002000  \n",
      "Epoch: [1][13/953] Elapsed 0m 33s (remain 37m 50s) Loss: 0.6308(0.6600) Grad: 9.5494  LR: 0.00002000  \n",
      "Epoch: [1][14/953] Elapsed 0m 36s (remain 38m 10s) Loss: 0.6416(0.6588) Grad: 9.5315  LR: 0.00002000  \n",
      "Epoch: [1][15/953] Elapsed 0m 38s (remain 37m 38s) Loss: 0.6490(0.6581) Grad: 9.7965  LR: 0.00002000  \n",
      "Epoch: [1][16/953] Elapsed 0m 40s (remain 37m 17s) Loss: 0.6498(0.6577) Grad: 9.6137  LR: 0.00002000  \n",
      "Epoch: [1][17/953] Elapsed 0m 42s (remain 36m 48s) Loss: 0.6383(0.6566) Grad: 9.8468  LR: 0.00002000  \n",
      "Epoch: [1][18/953] Elapsed 0m 44s (remain 36m 20s) Loss: 0.6564(0.6566) Grad: 9.8875  LR: 0.00002000  \n",
      "Epoch: [1][19/953] Elapsed 0m 46s (remain 35m 52s) Loss: 0.6475(0.6561) Grad: 9.9211  LR: 0.00002000  \n",
      "Epoch: [1][20/953] Elapsed 0m 47s (remain 35m 27s) Loss: 0.6375(0.6552) Grad: 9.7833  LR: 0.00002000  \n",
      "Epoch: [1][21/953] Elapsed 0m 49s (remain 35m 4s) Loss: 0.6364(0.6544) Grad: 9.9174  LR: 0.00002000  \n",
      "Epoch: [1][22/953] Elapsed 0m 51s (remain 34m 50s) Loss: 0.6174(0.6528) Grad: 9.7039  LR: 0.00002000  \n",
      "Epoch: [1][23/953] Elapsed 0m 53s (remain 34m 29s) Loss: 0.6262(0.6517) Grad: 9.6314  LR: 0.00002000  \n",
      "Epoch: [1][24/953] Elapsed 0m 55s (remain 34m 9s) Loss: 0.6228(0.6505) Grad: 9.3712  LR: 0.00002000  \n",
      "Epoch: [1][25/953] Elapsed 0m 56s (remain 33m 51s) Loss: 0.6420(0.6502) Grad: 9.6459  LR: 0.00002000  \n",
      "Epoch: [1][26/953] Elapsed 0m 58s (remain 33m 34s) Loss: 0.6319(0.6495) Grad: 9.4912  LR: 0.00002000  \n",
      "Epoch: [1][27/953] Elapsed 1m 0s (remain 33m 18s) Loss: 0.6290(0.6488) Grad: 9.5365  LR: 0.00002000  \n",
      "Epoch: [1][28/953] Elapsed 1m 2s (remain 33m 3s) Loss: 0.6270(0.6480) Grad: 9.8471  LR: 0.00002000  \n",
      "Epoch: [1][29/953] Elapsed 1m 4s (remain 32m 49s) Loss: 0.6120(0.6468) Grad: 9.4982  LR: 0.00002000  \n",
      "Epoch: [1][30/953] Elapsed 1m 5s (remain 32m 36s) Loss: 0.6270(0.6462) Grad: 9.4476  LR: 0.00002000  \n",
      "Epoch: [1][31/953] Elapsed 1m 7s (remain 32m 23s) Loss: 0.6248(0.6455) Grad: 9.6443  LR: 0.00002000  \n",
      "Epoch: [1][32/953] Elapsed 1m 9s (remain 32m 11s) Loss: 0.6104(0.6444) Grad: 9.4484  LR: 0.00002000  \n",
      "Epoch: [1][33/953] Elapsed 1m 11s (remain 32m 0s) Loss: 0.6140(0.6436) Grad: 9.5922  LR: 0.00002000  \n",
      "Epoch: [1][34/953] Elapsed 1m 12s (remain 31m 50s) Loss: 0.6061(0.6425) Grad: 9.3237  LR: 0.00002000  \n",
      "Epoch: [1][35/953] Elapsed 1m 14s (remain 31m 39s) Loss: 0.6062(0.6415) Grad: 9.2231  LR: 0.00002000  \n",
      "Epoch: [1][36/953] Elapsed 1m 16s (remain 31m 30s) Loss: 0.6143(0.6407) Grad: 9.3965  LR: 0.00002000  \n",
      "Epoch: [1][37/953] Elapsed 1m 18s (remain 31m 23s) Loss: 0.6118(0.6400) Grad: 9.2942  LR: 0.00002000  \n",
      "Epoch: [1][38/953] Elapsed 1m 20s (remain 31m 25s) Loss: 0.6127(0.6393) Grad: 9.3621  LR: 0.00002000  \n",
      "Epoch: [1][39/953] Elapsed 1m 22s (remain 31m 21s) Loss: 0.5861(0.6380) Grad: 9.1043  LR: 0.00002000  \n",
      "Epoch: [1][40/953] Elapsed 1m 24s (remain 31m 13s) Loss: 0.6051(0.6372) Grad: 9.2522  LR: 0.00002000  \n",
      "Epoch: [1][41/953] Elapsed 1m 26s (remain 31m 5s) Loss: 0.5878(0.6360) Grad: 8.9792  LR: 0.00002000  \n",
      "Epoch: [1][42/953] Elapsed 1m 27s (remain 30m 58s) Loss: 0.6002(0.6351) Grad: 9.4040  LR: 0.00002000  \n",
      "Epoch: [1][43/953] Elapsed 1m 29s (remain 30m 52s) Loss: 0.5950(0.6342) Grad: 9.2247  LR: 0.00002000  \n",
      "Epoch: [1][44/953] Elapsed 1m 31s (remain 30m 45s) Loss: 0.5948(0.6334) Grad: 9.3418  LR: 0.00002000  \n",
      "Epoch: [1][45/953] Elapsed 1m 33s (remain 30m 46s) Loss: 0.5824(0.6322) Grad: 8.9810  LR: 0.00002000  \n",
      "Epoch: [1][46/953] Elapsed 1m 35s (remain 30m 44s) Loss: 0.5938(0.6314) Grad: 9.3794  LR: 0.00002000  \n",
      "Epoch: [1][47/953] Elapsed 1m 37s (remain 30m 43s) Loss: 0.5903(0.6306) Grad: 8.8550  LR: 0.00001999  \n",
      "Epoch: [1][48/953] Elapsed 1m 39s (remain 30m 39s) Loss: 0.5847(0.6296) Grad: 9.3054  LR: 0.00001999  \n",
      "Epoch: [1][49/953] Elapsed 1m 41s (remain 30m 34s) Loss: 0.5993(0.6290) Grad: 9.1295  LR: 0.00001999  \n",
      "Epoch: [1][50/953] Elapsed 1m 43s (remain 30m 30s) Loss: 0.5784(0.6280) Grad: 8.5568  LR: 0.00001999  \n",
      "Epoch: [1][51/953] Elapsed 1m 45s (remain 30m 24s) Loss: 0.5686(0.6269) Grad: 8.8341  LR: 0.00001999  \n",
      "Epoch: [1][52/953] Elapsed 1m 47s (remain 30m 18s) Loss: 0.5902(0.6262) Grad: 9.2491  LR: 0.00001999  \n",
      "Epoch: [1][53/953] Elapsed 1m 48s (remain 30m 13s) Loss: 0.5912(0.6256) Grad: 9.2179  LR: 0.00001999  \n",
      "Epoch: [1][54/953] Elapsed 1m 50s (remain 30m 7s) Loss: 0.5708(0.6246) Grad: 8.9483  LR: 0.00001999  \n",
      "Epoch: [1][55/953] Elapsed 1m 52s (remain 30m 2s) Loss: 0.5717(0.6236) Grad: 8.9342  LR: 0.00001999  \n",
      "Epoch: [1][56/953] Elapsed 1m 54s (remain 29m 56s) Loss: 0.5777(0.6228) Grad: 9.3329  LR: 0.00001999  \n",
      "Epoch: [1][57/953] Elapsed 1m 56s (remain 29m 51s) Loss: 0.5486(0.6215) Grad: 8.7764  LR: 0.00001999  \n",
      "Epoch: [1][58/953] Elapsed 1m 57s (remain 29m 46s) Loss: 0.5549(0.6204) Grad: 8.6478  LR: 0.00001999  \n",
      "Epoch: [1][59/953] Elapsed 1m 59s (remain 29m 41s) Loss: 0.5653(0.6195) Grad: 8.7114  LR: 0.00001999  \n",
      "Epoch: [1][60/953] Elapsed 2m 1s (remain 29m 36s) Loss: 0.5584(0.6185) Grad: 8.5950  LR: 0.00001999  \n",
      "Epoch: [1][61/953] Elapsed 2m 3s (remain 29m 31s) Loss: 0.5724(0.6177) Grad: 8.7512  LR: 0.00001999  \n",
      "Epoch: [1][62/953] Elapsed 2m 5s (remain 29m 26s) Loss: 0.5615(0.6168) Grad: 8.6351  LR: 0.00001999  \n",
      "Epoch: [1][63/953] Elapsed 2m 6s (remain 29m 21s) Loss: 0.5575(0.6159) Grad: 8.8450  LR: 0.00001999  \n",
      "Epoch: [1][64/953] Elapsed 2m 8s (remain 29m 18s) Loss: 0.5493(0.6149) Grad: 8.6121  LR: 0.00001999  \n",
      "Epoch: [1][65/953] Elapsed 2m 11s (remain 29m 20s) Loss: 0.5420(0.6138) Grad: 8.2825  LR: 0.00001999  \n",
      "Epoch: [1][66/953] Elapsed 2m 13s (remain 29m 19s) Loss: 0.5475(0.6128) Grad: 8.6902  LR: 0.00001999  \n",
      "Epoch: [1][67/953] Elapsed 2m 14s (remain 29m 14s) Loss: 0.5528(0.6119) Grad: 8.6975  LR: 0.00001999  \n",
      "Epoch: [1][68/953] Elapsed 2m 16s (remain 29m 11s) Loss: 0.5436(0.6109) Grad: 8.7735  LR: 0.00001999  \n",
      "Epoch: [1][69/953] Elapsed 2m 18s (remain 29m 8s) Loss: 0.5284(0.6097) Grad: 8.2553  LR: 0.00001999  \n",
      "Epoch: [1][70/953] Elapsed 2m 20s (remain 29m 5s) Loss: 0.5453(0.6088) Grad: 8.4365  LR: 0.00001999  \n",
      "Epoch: [1][71/953] Elapsed 2m 22s (remain 29m 0s) Loss: 0.5456(0.6080) Grad: 8.5083  LR: 0.00001999  \n",
      "Epoch: [1][72/953] Elapsed 2m 24s (remain 28m 56s) Loss: 0.5419(0.6071) Grad: 8.6285  LR: 0.00001999  \n",
      "Epoch: [1][73/953] Elapsed 2m 25s (remain 28m 51s) Loss: 0.5250(0.6059) Grad: 8.2863  LR: 0.00001999  \n",
      "Epoch: [1][74/953] Elapsed 2m 27s (remain 28m 46s) Loss: 0.5350(0.6050) Grad: 8.5876  LR: 0.00001999  \n",
      "Epoch: [1][75/953] Elapsed 2m 29s (remain 28m 42s) Loss: 0.5240(0.6039) Grad: 8.4208  LR: 0.00001999  \n",
      "Epoch: [1][76/953] Elapsed 2m 31s (remain 28m 38s) Loss: 0.5400(0.6031) Grad: 8.5382  LR: 0.00001999  \n",
      "Epoch: [1][77/953] Elapsed 2m 32s (remain 28m 35s) Loss: 0.5462(0.6024) Grad: 8.8904  LR: 0.00001999  \n",
      "Epoch: [1][78/953] Elapsed 2m 34s (remain 28m 32s) Loss: 0.5330(0.6015) Grad: 8.8273  LR: 0.00001999  \n",
      "Epoch: [1][79/953] Elapsed 2m 36s (remain 28m 29s) Loss: 0.5348(0.6007) Grad: 8.4402  LR: 0.00001999  \n",
      "Epoch: [1][80/953] Elapsed 2m 38s (remain 28m 26s) Loss: 0.5073(0.5995) Grad: 8.1397  LR: 0.00001999  \n",
      "Epoch: [1][81/953] Elapsed 2m 40s (remain 28m 22s) Loss: 0.5257(0.5986) Grad: 8.3966  LR: 0.00001999  \n",
      "Epoch: [1][82/953] Elapsed 2m 42s (remain 28m 21s) Loss: 0.5208(0.5977) Grad: 8.1693  LR: 0.00001999  \n",
      "Epoch: [1][83/953] Elapsed 2m 44s (remain 28m 19s) Loss: 0.5353(0.5969) Grad: 8.4961  LR: 0.00001998  \n",
      "Epoch: [1][84/953] Elapsed 2m 46s (remain 28m 17s) Loss: 0.5100(0.5959) Grad: 8.0822  LR: 0.00001998  \n",
      "Epoch: [1][85/953] Elapsed 2m 48s (remain 28m 16s) Loss: 0.5205(0.5950) Grad: 8.2289  LR: 0.00001998  \n",
      "Epoch: [1][86/953] Elapsed 2m 50s (remain 28m 13s) Loss: 0.5345(0.5943) Grad: 8.5247  LR: 0.00001998  \n",
      "Epoch: [1][87/953] Elapsed 2m 51s (remain 28m 9s) Loss: 0.5206(0.5935) Grad: 8.1949  LR: 0.00001998  \n",
      "Epoch: [1][88/953] Elapsed 2m 53s (remain 28m 6s) Loss: 0.5149(0.5926) Grad: 8.2246  LR: 0.00001998  \n",
      "Epoch: [1][89/953] Elapsed 2m 55s (remain 28m 2s) Loss: 0.5230(0.5918) Grad: 8.4685  LR: 0.00001998  \n",
      "Epoch: [1][90/953] Elapsed 2m 57s (remain 28m 0s) Loss: 0.5153(0.5910) Grad: 8.1038  LR: 0.00001998  \n",
      "Epoch: [1][91/953] Elapsed 2m 59s (remain 27m 57s) Loss: 0.5124(0.5901) Grad: 7.9749  LR: 0.00001998  \n",
      "Epoch: [1][92/953] Elapsed 3m 1s (remain 27m 54s) Loss: 0.5056(0.5892) Grad: 7.8119  LR: 0.00001998  \n",
      "Epoch: [1][93/953] Elapsed 3m 2s (remain 27m 50s) Loss: 0.5085(0.5884) Grad: 8.0316  LR: 0.00001998  \n",
      "Epoch: [1][94/953] Elapsed 3m 4s (remain 27m 49s) Loss: 0.5205(0.5877) Grad: 7.9275  LR: 0.00001998  \n",
      "Epoch: [1][95/953] Elapsed 3m 6s (remain 27m 47s) Loss: 0.5169(0.5869) Grad: 8.0472  LR: 0.00001998  \n",
      "Epoch: [1][96/953] Elapsed 3m 8s (remain 27m 44s) Loss: 0.5175(0.5862) Grad: 8.4297  LR: 0.00001998  \n",
      "Epoch: [1][97/953] Elapsed 3m 10s (remain 27m 41s) Loss: 0.5004(0.5853) Grad: 8.0890  LR: 0.00001998  \n",
      "Epoch: [1][98/953] Elapsed 3m 12s (remain 27m 40s) Loss: 0.5051(0.5845) Grad: 7.8432  LR: 0.00001998  \n",
      "Epoch: [1][99/953] Elapsed 3m 15s (remain 27m 47s) Loss: 0.5190(0.5839) Grad: 8.2131  LR: 0.00001998  \n",
      "Epoch: [1][100/953] Elapsed 3m 18s (remain 27m 51s) Loss: 0.5020(0.5831) Grad: 8.0857  LR: 0.00001998  \n",
      "Epoch: [1][101/953] Elapsed 3m 20s (remain 27m 56s) Loss: 0.5021(0.5823) Grad: 8.3217  LR: 0.00001998  \n",
      "Epoch: [1][102/953] Elapsed 3m 23s (remain 28m 1s) Loss: 0.4925(0.5814) Grad: 8.0187  LR: 0.00001998  \n",
      "Epoch: [1][103/953] Elapsed 3m 25s (remain 27m 58s) Loss: 0.4878(0.5805) Grad: 7.8085  LR: 0.00001998  \n",
      "Epoch: [1][104/953] Elapsed 3m 27s (remain 27m 58s) Loss: 0.4892(0.5796) Grad: 7.8856  LR: 0.00001998  \n",
      "Epoch: [1][105/953] Elapsed 3m 30s (remain 28m 2s) Loss: 0.4982(0.5789) Grad: 7.7252  LR: 0.00001998  \n",
      "Epoch: [1][106/953] Elapsed 3m 32s (remain 28m 0s) Loss: 0.4827(0.5780) Grad: 8.1007  LR: 0.00001998  \n",
      "Epoch: [1][107/953] Elapsed 3m 34s (remain 27m 57s) Loss: 0.4958(0.5772) Grad: 8.0190  LR: 0.00001997  \n",
      "Epoch: [1][108/953] Elapsed 3m 36s (remain 27m 53s) Loss: 0.5000(0.5765) Grad: 8.1755  LR: 0.00001997  \n",
      "Epoch: [1][109/953] Elapsed 3m 37s (remain 27m 50s) Loss: 0.4951(0.5757) Grad: 8.0094  LR: 0.00001997  \n",
      "Epoch: [1][110/953] Elapsed 3m 39s (remain 27m 46s) Loss: 0.4882(0.5750) Grad: 7.8287  LR: 0.00001997  \n",
      "Epoch: [1][111/953] Elapsed 3m 41s (remain 27m 45s) Loss: 0.4728(0.5740) Grad: 7.4611  LR: 0.00001997  \n",
      "Epoch: [1][112/953] Elapsed 3m 44s (remain 27m 48s) Loss: 0.4834(0.5732) Grad: 7.5549  LR: 0.00001997  \n",
      "Epoch: [1][113/953] Elapsed 3m 47s (remain 27m 51s) Loss: 0.4942(0.5726) Grad: 7.9177  LR: 0.00001997  \n",
      "Epoch: [1][114/953] Elapsed 3m 49s (remain 27m 48s) Loss: 0.4711(0.5717) Grad: 7.6387  LR: 0.00001997  \n",
      "Epoch: [1][115/953] Elapsed 3m 51s (remain 27m 50s) Loss: 0.4843(0.5709) Grad: 7.8654  LR: 0.00001997  \n",
      "Epoch: [1][116/953] Elapsed 3m 54s (remain 27m 53s) Loss: 0.4846(0.5702) Grad: 7.5175  LR: 0.00001997  \n",
      "Epoch: [1][117/953] Elapsed 3m 56s (remain 27m 56s) Loss: 0.4775(0.5694) Grad: 7.6888  LR: 0.00001997  \n",
      "Epoch: [1][118/953] Elapsed 3m 59s (remain 27m 59s) Loss: 0.4779(0.5686) Grad: 7.7876  LR: 0.00001997  \n",
      "Epoch: [1][119/953] Elapsed 4m 1s (remain 27m 58s) Loss: 0.4934(0.5680) Grad: 7.5370  LR: 0.00001997  \n",
      "Epoch: [1][120/953] Elapsed 4m 3s (remain 27m 57s) Loss: 0.4699(0.5672) Grad: 7.5447  LR: 0.00001997  \n",
      "Epoch: [1][121/953] Elapsed 4m 5s (remain 27m 54s) Loss: 0.4722(0.5664) Grad: 7.5612  LR: 0.00001997  \n",
      "Epoch: [1][122/953] Elapsed 4m 7s (remain 27m 52s) Loss: 0.4757(0.5657) Grad: 7.6168  LR: 0.00001997  \n",
      "Epoch: [1][123/953] Elapsed 4m 9s (remain 27m 49s) Loss: 0.4742(0.5649) Grad: 7.7897  LR: 0.00001997  \n",
      "Epoch: [1][124/953] Elapsed 4m 11s (remain 27m 48s) Loss: 0.4732(0.5642) Grad: 7.9637  LR: 0.00001997  \n",
      "Epoch: [1][125/953] Elapsed 4m 13s (remain 27m 46s) Loss: 0.4642(0.5634) Grad: 7.7094  LR: 0.00001997  \n",
      "Epoch: [1][126/953] Elapsed 4m 15s (remain 27m 43s) Loss: 0.4659(0.5626) Grad: 7.6348  LR: 0.00001996  \n",
      "Epoch: [1][127/953] Elapsed 4m 17s (remain 27m 41s) Loss: 0.4650(0.5619) Grad: 7.4791  LR: 0.00001996  \n",
      "Epoch: [1][128/953] Elapsed 4m 19s (remain 27m 38s) Loss: 0.4708(0.5612) Grad: 7.7143  LR: 0.00001996  \n",
      "Epoch: [1][129/953] Elapsed 4m 21s (remain 27m 36s) Loss: 0.4662(0.5604) Grad: 7.7709  LR: 0.00001996  \n",
      "Epoch: [1][130/953] Elapsed 4m 23s (remain 27m 34s) Loss: 0.4549(0.5596) Grad: 7.6135  LR: 0.00001996  \n",
      "Epoch: [1][131/953] Elapsed 4m 25s (remain 27m 33s) Loss: 0.4571(0.5589) Grad: 7.2552  LR: 0.00001996  \n",
      "Epoch: [1][132/953] Elapsed 4m 28s (remain 27m 36s) Loss: 0.4440(0.5580) Grad: 7.4276  LR: 0.00001996  \n",
      "Epoch: [1][133/953] Elapsed 4m 31s (remain 27m 39s) Loss: 0.4597(0.5573) Grad: 7.4545  LR: 0.00001996  \n",
      "Epoch: [1][134/953] Elapsed 4m 34s (remain 27m 43s) Loss: 0.4470(0.5564) Grad: 7.0505  LR: 0.00001996  \n",
      "Epoch: [1][135/953] Elapsed 4m 37s (remain 27m 46s) Loss: 0.4455(0.5556) Grad: 7.3429  LR: 0.00001996  \n",
      "Epoch: [1][136/953] Elapsed 4m 40s (remain 27m 48s) Loss: 0.4584(0.5549) Grad: 7.5421  LR: 0.00001996  \n",
      "Epoch: [1][137/953] Elapsed 4m 42s (remain 27m 50s) Loss: 0.4384(0.5541) Grad: 6.9733  LR: 0.00001996  \n",
      "Epoch: [1][138/953] Elapsed 4m 44s (remain 27m 46s) Loss: 0.4580(0.5534) Grad: 7.3681  LR: 0.00001996  \n",
      "Epoch: [1][139/953] Elapsed 4m 46s (remain 27m 43s) Loss: 0.4556(0.5527) Grad: 7.4213  LR: 0.00001996  \n",
      "Epoch: [1][140/953] Elapsed 4m 48s (remain 27m 40s) Loss: 0.4420(0.5519) Grad: 7.4857  LR: 0.00001996  \n",
      "Epoch: [1][141/953] Elapsed 4m 50s (remain 27m 36s) Loss: 0.4356(0.5511) Grad: 7.3182  LR: 0.00001996  \n",
      "Epoch: [1][142/953] Elapsed 4m 52s (remain 27m 33s) Loss: 0.4357(0.5503) Grad: 6.7781  LR: 0.00001996  \n",
      "Epoch: [1][143/953] Elapsed 4m 53s (remain 27m 31s) Loss: 0.4370(0.5495) Grad: 6.9797  LR: 0.00001995  \n",
      "Epoch: [1][144/953] Elapsed 4m 55s (remain 27m 27s) Loss: 0.4489(0.5488) Grad: 7.7440  LR: 0.00001995  \n",
      "Epoch: [1][145/953] Elapsed 4m 57s (remain 27m 24s) Loss: 0.4455(0.5481) Grad: 7.2685  LR: 0.00001995  \n",
      "Epoch: [1][146/953] Elapsed 4m 59s (remain 27m 21s) Loss: 0.4314(0.5473) Grad: 7.1774  LR: 0.00001995  \n",
      "Epoch: [1][147/953] Elapsed 5m 1s (remain 27m 17s) Loss: 0.4432(0.5466) Grad: 7.0706  LR: 0.00001995  \n",
      "Epoch: [1][148/953] Elapsed 5m 2s (remain 27m 14s) Loss: 0.4330(0.5458) Grad: 7.1353  LR: 0.00001995  \n",
      "Epoch: [1][149/953] Elapsed 5m 4s (remain 27m 11s) Loss: 0.4194(0.5450) Grad: 7.0419  LR: 0.00001995  \n",
      "Epoch: [1][150/953] Elapsed 5m 6s (remain 27m 8s) Loss: 0.4290(0.5442) Grad: 7.1780  LR: 0.00001995  \n",
      "Epoch: [1][151/953] Elapsed 5m 8s (remain 27m 4s) Loss: 0.4337(0.5435) Grad: 6.8689  LR: 0.00001995  \n",
      "Epoch: [1][152/953] Elapsed 5m 10s (remain 27m 1s) Loss: 0.4338(0.5428) Grad: 6.8493  LR: 0.00001995  \n",
      "Epoch: [1][153/953] Elapsed 5m 11s (remain 26m 58s) Loss: 0.4330(0.5421) Grad: 7.1080  LR: 0.00001995  \n",
      "Epoch: [1][154/953] Elapsed 5m 13s (remain 26m 55s) Loss: 0.4215(0.5413) Grad: 7.0430  LR: 0.00001995  \n",
      "Epoch: [1][155/953] Elapsed 5m 15s (remain 26m 52s) Loss: 0.4371(0.5406) Grad: 6.9899  LR: 0.00001995  \n",
      "Epoch: [1][156/953] Elapsed 5m 17s (remain 26m 49s) Loss: 0.4327(0.5399) Grad: 6.7434  LR: 0.00001995  \n",
      "Epoch: [1][157/953] Elapsed 5m 19s (remain 26m 46s) Loss: 0.4178(0.5392) Grad: 6.6046  LR: 0.00001995  \n",
      "Epoch: [1][158/953] Elapsed 5m 21s (remain 26m 43s) Loss: 0.4228(0.5384) Grad: 6.9213  LR: 0.00001995  \n",
      "Epoch: [1][159/953] Elapsed 5m 22s (remain 26m 40s) Loss: 0.4062(0.5376) Grad: 6.7373  LR: 0.00001994  \n",
      "Epoch: [1][160/953] Elapsed 5m 24s (remain 26m 37s) Loss: 0.4245(0.5369) Grad: 6.8233  LR: 0.00001994  \n",
      "Epoch: [1][161/953] Elapsed 5m 26s (remain 26m 34s) Loss: 0.4193(0.5362) Grad: 6.7007  LR: 0.00001994  \n",
      "Epoch: [1][162/953] Elapsed 5m 28s (remain 26m 31s) Loss: 0.4120(0.5354) Grad: 6.7028  LR: 0.00001994  \n",
      "Epoch: [1][163/953] Elapsed 5m 30s (remain 26m 28s) Loss: 0.4394(0.5348) Grad: 6.9063  LR: 0.00001994  \n",
      "Epoch: [1][164/953] Elapsed 5m 32s (remain 26m 25s) Loss: 0.4157(0.5341) Grad: 6.3499  LR: 0.00001994  \n",
      "Epoch: [1][165/953] Elapsed 5m 33s (remain 26m 22s) Loss: 0.4310(0.5335) Grad: 6.5541  LR: 0.00001994  \n",
      "Epoch: [1][166/953] Elapsed 5m 35s (remain 26m 19s) Loss: 0.4179(0.5328) Grad: 6.6608  LR: 0.00001994  \n",
      "Epoch: [1][167/953] Elapsed 5m 37s (remain 26m 17s) Loss: 0.4234(0.5321) Grad: 6.9862  LR: 0.00001994  \n",
      "Epoch: [1][168/953] Elapsed 5m 39s (remain 26m 14s) Loss: 0.4202(0.5315) Grad: 6.8592  LR: 0.00001994  \n",
      "Epoch: [1][169/953] Elapsed 5m 41s (remain 26m 11s) Loss: 0.4250(0.5308) Grad: 6.4033  LR: 0.00001994  \n",
      "Epoch: [1][170/953] Elapsed 5m 43s (remain 26m 8s) Loss: 0.4186(0.5302) Grad: 6.3568  LR: 0.00001994  \n",
      "Epoch: [1][171/953] Elapsed 5m 44s (remain 26m 6s) Loss: 0.4236(0.5296) Grad: 6.7593  LR: 0.00001994  \n",
      "Epoch: [1][172/953] Elapsed 5m 46s (remain 26m 3s) Loss: 0.4164(0.5289) Grad: 7.0485  LR: 0.00001994  \n",
      "Epoch: [1][173/953] Elapsed 5m 48s (remain 26m 0s) Loss: 0.4124(0.5282) Grad: 6.5517  LR: 0.00001993  \n",
      "Epoch: [1][174/953] Elapsed 5m 50s (remain 25m 57s) Loss: 0.3924(0.5275) Grad: 6.5175  LR: 0.00001993  \n",
      "Epoch: [1][175/953] Elapsed 5m 52s (remain 25m 55s) Loss: 0.4016(0.5268) Grad: 6.7165  LR: 0.00001993  \n",
      "Epoch: [1][176/953] Elapsed 5m 54s (remain 25m 52s) Loss: 0.4058(0.5261) Grad: 6.6415  LR: 0.00001993  \n",
      "Epoch: [1][177/953] Elapsed 5m 55s (remain 25m 49s) Loss: 0.4041(0.5254) Grad: 6.5184  LR: 0.00001993  \n",
      "Epoch: [1][178/953] Elapsed 5m 57s (remain 25m 46s) Loss: 0.4015(0.5247) Grad: 6.8309  LR: 0.00001993  \n",
      "Epoch: [1][179/953] Elapsed 5m 59s (remain 25m 44s) Loss: 0.4036(0.5240) Grad: 6.8191  LR: 0.00001993  \n",
      "Epoch: [1][180/953] Elapsed 6m 1s (remain 25m 41s) Loss: 0.3910(0.5233) Grad: 6.5587  LR: 0.00001993  \n",
      "Epoch: [1][181/953] Elapsed 6m 3s (remain 25m 38s) Loss: 0.3891(0.5225) Grad: 6.4022  LR: 0.00001993  \n",
      "Epoch: [1][182/953] Elapsed 6m 5s (remain 25m 35s) Loss: 0.4134(0.5220) Grad: 6.2034  LR: 0.00001993  \n",
      "Epoch: [1][183/953] Elapsed 6m 6s (remain 25m 33s) Loss: 0.3948(0.5213) Grad: 6.5186  LR: 0.00001993  \n",
      "Epoch: [1][184/953] Elapsed 6m 8s (remain 25m 30s) Loss: 0.4089(0.5207) Grad: 6.4848  LR: 0.00001993  \n",
      "Epoch: [1][185/953] Elapsed 6m 10s (remain 25m 27s) Loss: 0.4138(0.5201) Grad: 6.5459  LR: 0.00001992  \n",
      "Epoch: [1][186/953] Elapsed 6m 12s (remain 25m 24s) Loss: 0.4045(0.5195) Grad: 6.7300  LR: 0.00001992  \n",
      "Epoch: [1][187/953] Elapsed 6m 14s (remain 25m 22s) Loss: 0.3863(0.5188) Grad: 6.3960  LR: 0.00001992  \n",
      "Epoch: [1][188/953] Elapsed 6m 15s (remain 25m 19s) Loss: 0.4019(0.5181) Grad: 6.6364  LR: 0.00001992  \n",
      "Epoch: [1][189/953] Elapsed 6m 17s (remain 25m 16s) Loss: 0.3922(0.5175) Grad: 6.4652  LR: 0.00001992  \n",
      "Epoch: [1][190/953] Elapsed 6m 19s (remain 25m 13s) Loss: 0.3949(0.5168) Grad: 6.4785  LR: 0.00001992  \n",
      "Epoch: [1][191/953] Elapsed 6m 21s (remain 25m 11s) Loss: 0.3845(0.5161) Grad: 6.7006  LR: 0.00001992  \n",
      "Epoch: [1][192/953] Elapsed 6m 23s (remain 25m 8s) Loss: 0.4000(0.5155) Grad: 6.4822  LR: 0.00001992  \n",
      "Epoch: [1][193/953] Elapsed 6m 24s (remain 25m 6s) Loss: 0.3894(0.5149) Grad: 6.3763  LR: 0.00001992  \n",
      "Epoch: [1][194/953] Elapsed 6m 26s (remain 25m 3s) Loss: 0.3885(0.5142) Grad: 6.3410  LR: 0.00001992  \n",
      "Epoch: [1][195/953] Elapsed 6m 28s (remain 25m 0s) Loss: 0.3883(0.5136) Grad: 6.4022  LR: 0.00001992  \n",
      "Epoch: [1][196/953] Elapsed 6m 30s (remain 24m 58s) Loss: 0.3884(0.5130) Grad: 6.2092  LR: 0.00001992  \n",
      "Epoch: [1][197/953] Elapsed 6m 32s (remain 24m 55s) Loss: 0.3829(0.5123) Grad: 6.3630  LR: 0.00001991  \n",
      "Epoch: [1][198/953] Elapsed 6m 34s (remain 24m 52s) Loss: 0.3805(0.5116) Grad: 6.1339  LR: 0.00001991  \n",
      "Epoch: [1][199/953] Elapsed 6m 35s (remain 24m 50s) Loss: 0.3858(0.5110) Grad: 6.2688  LR: 0.00001991  \n",
      "Epoch: [1][200/953] Elapsed 6m 37s (remain 24m 47s) Loss: 0.3723(0.5103) Grad: 6.2042  LR: 0.00001991  \n",
      "Epoch: [1][201/953] Elapsed 6m 39s (remain 24m 45s) Loss: 0.3772(0.5097) Grad: 6.2970  LR: 0.00001991  \n",
      "Epoch: [1][202/953] Elapsed 6m 41s (remain 24m 42s) Loss: 0.3822(0.5090) Grad: 6.0633  LR: 0.00001991  \n",
      "Epoch: [1][203/953] Elapsed 6m 43s (remain 24m 40s) Loss: 0.3710(0.5084) Grad: 6.3751  LR: 0.00001991  \n",
      "Epoch: [1][204/953] Elapsed 6m 45s (remain 24m 37s) Loss: 0.3788(0.5077) Grad: 6.1799  LR: 0.00001991  \n",
      "Epoch: [1][205/953] Elapsed 6m 46s (remain 24m 35s) Loss: 0.3708(0.5071) Grad: 5.8969  LR: 0.00001991  \n",
      "Epoch: [1][206/953] Elapsed 6m 48s (remain 24m 32s) Loss: 0.3714(0.5064) Grad: 5.6679  LR: 0.00001991  \n",
      "Epoch: [1][207/953] Elapsed 6m 50s (remain 24m 30s) Loss: 0.3648(0.5057) Grad: 5.9026  LR: 0.00001991  \n",
      "Epoch: [1][208/953] Elapsed 6m 52s (remain 24m 27s) Loss: 0.3751(0.5051) Grad: 6.2071  LR: 0.00001991  \n",
      "Epoch: [1][209/953] Elapsed 6m 54s (remain 24m 25s) Loss: 0.3828(0.5045) Grad: 6.1943  LR: 0.00001990  \n",
      "Epoch: [1][210/953] Elapsed 6m 55s (remain 24m 22s) Loss: 0.3623(0.5038) Grad: 5.7457  LR: 0.00001990  \n",
      "Epoch: [1][211/953] Elapsed 6m 57s (remain 24m 20s) Loss: 0.3739(0.5032) Grad: 6.2705  LR: 0.00001990  \n",
      "Epoch: [1][212/953] Elapsed 6m 59s (remain 24m 17s) Loss: 0.3711(0.5026) Grad: 6.2232  LR: 0.00001990  \n",
      "Epoch: [1][213/953] Elapsed 7m 1s (remain 24m 15s) Loss: 0.3609(0.5020) Grad: 5.8038  LR: 0.00001990  \n",
      "Epoch: [1][214/953] Elapsed 7m 3s (remain 24m 12s) Loss: 0.3544(0.5013) Grad: 6.1764  LR: 0.00001990  \n",
      "Epoch: [1][215/953] Elapsed 7m 5s (remain 24m 10s) Loss: 0.3752(0.5007) Grad: 5.9073  LR: 0.00001990  \n",
      "Epoch: [1][216/953] Elapsed 7m 6s (remain 24m 7s) Loss: 0.3560(0.5000) Grad: 6.0841  LR: 0.00001990  \n",
      "Epoch: [1][217/953] Elapsed 7m 8s (remain 24m 5s) Loss: 0.3671(0.4994) Grad: 6.2060  LR: 0.00001990  \n",
      "Epoch: [1][218/953] Elapsed 7m 10s (remain 24m 2s) Loss: 0.3491(0.4987) Grad: 5.6327  LR: 0.00001990  \n",
      "Epoch: [1][219/953] Elapsed 7m 12s (remain 24m 0s) Loss: 0.3644(0.4981) Grad: 6.1378  LR: 0.00001990  \n",
      "Epoch: [1][220/953] Elapsed 7m 14s (remain 23m 58s) Loss: 0.3606(0.4975) Grad: 5.9782  LR: 0.00001989  \n",
      "Epoch: [1][221/953] Elapsed 7m 15s (remain 23m 55s) Loss: 0.3632(0.4969) Grad: 5.8999  LR: 0.00001989  \n",
      "Epoch: [1][222/953] Elapsed 7m 17s (remain 23m 52s) Loss: 0.3616(0.4963) Grad: 6.1019  LR: 0.00001989  \n",
      "Epoch: [1][223/953] Elapsed 7m 19s (remain 23m 50s) Loss: 0.3616(0.4957) Grad: 5.8601  LR: 0.00001989  \n",
      "Epoch: [1][224/953] Elapsed 7m 21s (remain 23m 48s) Loss: 0.3544(0.4950) Grad: 5.9553  LR: 0.00001989  \n",
      "Epoch: [1][225/953] Elapsed 7m 23s (remain 23m 45s) Loss: 0.3609(0.4945) Grad: 6.2633  LR: 0.00001989  \n",
      "Epoch: [1][226/953] Elapsed 7m 25s (remain 23m 43s) Loss: 0.3581(0.4939) Grad: 6.0570  LR: 0.00001989  \n",
      "Epoch: [1][227/953] Elapsed 7m 26s (remain 23m 40s) Loss: 0.3547(0.4932) Grad: 5.8594  LR: 0.00001989  \n",
      "Epoch: [1][228/953] Elapsed 7m 28s (remain 23m 38s) Loss: 0.3462(0.4926) Grad: 5.9314  LR: 0.00001989  \n",
      "Epoch: [1][229/953] Elapsed 7m 30s (remain 23m 35s) Loss: 0.3555(0.4920) Grad: 5.8405  LR: 0.00001989  \n",
      "Epoch: [1][230/953] Elapsed 7m 32s (remain 23m 33s) Loss: 0.3371(0.4913) Grad: 5.8229  LR: 0.00001988  \n",
      "Epoch: [1][231/953] Elapsed 7m 33s (remain 23m 30s) Loss: 0.3523(0.4907) Grad: 6.0057  LR: 0.00001988  \n",
      "Epoch: [1][232/953] Elapsed 7m 35s (remain 23m 28s) Loss: 0.3440(0.4901) Grad: 5.7668  LR: 0.00001988  \n",
      "Epoch: [1][233/953] Elapsed 7m 37s (remain 23m 25s) Loss: 0.3642(0.4896) Grad: 5.6796  LR: 0.00001988  \n",
      "Epoch: [1][234/953] Elapsed 7m 39s (remain 23m 23s) Loss: 0.3576(0.4890) Grad: 5.8718  LR: 0.00001988  \n",
      "Epoch: [1][235/953] Elapsed 7m 41s (remain 23m 21s) Loss: 0.3564(0.4884) Grad: 5.8287  LR: 0.00001988  \n",
      "Epoch: [1][236/953] Elapsed 7m 43s (remain 23m 18s) Loss: 0.3352(0.4878) Grad: 5.6501  LR: 0.00001988  \n",
      "Epoch: [1][237/953] Elapsed 7m 44s (remain 23m 16s) Loss: 0.3592(0.4873) Grad: 5.8012  LR: 0.00001988  \n",
      "Epoch: [1][238/953] Elapsed 7m 46s (remain 23m 13s) Loss: 0.3537(0.4867) Grad: 5.4119  LR: 0.00001988  \n",
      "Epoch: [1][239/953] Elapsed 7m 48s (remain 23m 11s) Loss: 0.3405(0.4861) Grad: 5.7665  LR: 0.00001988  \n",
      "Epoch: [1][240/953] Elapsed 7m 50s (remain 23m 9s) Loss: 0.3391(0.4855) Grad: 5.7037  LR: 0.00001987  \n",
      "Epoch: [1][241/953] Elapsed 7m 51s (remain 23m 6s) Loss: 0.3477(0.4849) Grad: 5.5282  LR: 0.00001987  \n",
      "Epoch: [1][242/953] Elapsed 7m 53s (remain 23m 4s) Loss: 0.3537(0.4844) Grad: 5.5324  LR: 0.00001987  \n",
      "Epoch: [1][243/953] Elapsed 7m 55s (remain 23m 1s) Loss: 0.3444(0.4838) Grad: 5.6700  LR: 0.00001987  \n",
      "Epoch: [1][244/953] Elapsed 7m 57s (remain 22m 59s) Loss: 0.3456(0.4832) Grad: 5.4391  LR: 0.00001987  \n",
      "Epoch: [1][245/953] Elapsed 7m 59s (remain 22m 57s) Loss: 0.3492(0.4827) Grad: 5.8084  LR: 0.00001987  \n",
      "Epoch: [1][246/953] Elapsed 8m 1s (remain 22m 55s) Loss: 0.3467(0.4821) Grad: 5.4744  LR: 0.00001987  \n",
      "Epoch: [1][247/953] Elapsed 8m 2s (remain 22m 52s) Loss: 0.3304(0.4815) Grad: 5.8493  LR: 0.00001987  \n",
      "Epoch: [1][248/953] Elapsed 8m 4s (remain 22m 50s) Loss: 0.3274(0.4809) Grad: 5.6038  LR: 0.00001987  \n",
      "Epoch: [1][249/953] Elapsed 8m 6s (remain 22m 47s) Loss: 0.3313(0.4803) Grad: 5.7057  LR: 0.00001986  \n",
      "Epoch: [1][250/953] Elapsed 8m 8s (remain 22m 45s) Loss: 0.3419(0.4798) Grad: 5.6055  LR: 0.00001986  \n",
      "Epoch: [1][251/953] Elapsed 8m 10s (remain 22m 43s) Loss: 0.3484(0.4792) Grad: 5.5365  LR: 0.00001986  \n",
      "Epoch: [1][252/953] Elapsed 8m 11s (remain 22m 40s) Loss: 0.3335(0.4787) Grad: 5.4858  LR: 0.00001986  \n",
      "Epoch: [1][253/953] Elapsed 8m 13s (remain 22m 38s) Loss: 0.3257(0.4781) Grad: 5.5154  LR: 0.00001986  \n",
      "Epoch: [1][254/953] Elapsed 8m 15s (remain 22m 35s) Loss: 0.3342(0.4775) Grad: 5.5602  LR: 0.00001986  \n",
      "Epoch: [1][255/953] Elapsed 8m 17s (remain 22m 33s) Loss: 0.3290(0.4769) Grad: 5.4860  LR: 0.00001986  \n",
      "Epoch: [1][256/953] Elapsed 8m 18s (remain 22m 31s) Loss: 0.3238(0.4763) Grad: 5.4703  LR: 0.00001986  \n",
      "Epoch: [1][257/953] Elapsed 8m 20s (remain 22m 28s) Loss: 0.3335(0.4758) Grad: 5.2180  LR: 0.00001986  \n",
      "Epoch: [1][258/953] Elapsed 8m 22s (remain 22m 26s) Loss: 0.3218(0.4752) Grad: 5.3644  LR: 0.00001985  \n",
      "Epoch: [1][259/953] Elapsed 8m 24s (remain 22m 24s) Loss: 0.3225(0.4746) Grad: 5.5304  LR: 0.00001985  \n",
      "Epoch: [1][260/953] Elapsed 8m 26s (remain 22m 21s) Loss: 0.3304(0.4740) Grad: 5.4969  LR: 0.00001985  \n",
      "Epoch: [1][261/953] Elapsed 8m 27s (remain 22m 19s) Loss: 0.3349(0.4735) Grad: 5.3259  LR: 0.00001985  \n",
      "Epoch: [1][262/953] Elapsed 8m 29s (remain 22m 16s) Loss: 0.3295(0.4729) Grad: 5.3327  LR: 0.00001985  \n",
      "Epoch: [1][263/953] Elapsed 8m 31s (remain 22m 14s) Loss: 0.3398(0.4724) Grad: 5.3992  LR: 0.00001985  \n",
      "Epoch: [1][264/953] Elapsed 8m 33s (remain 22m 12s) Loss: 0.3193(0.4719) Grad: 5.0649  LR: 0.00001985  \n",
      "Epoch: [1][265/953] Elapsed 8m 35s (remain 22m 10s) Loss: 0.3293(0.4713) Grad: 5.3176  LR: 0.00001985  \n",
      "Epoch: [1][266/953] Elapsed 8m 36s (remain 22m 8s) Loss: 0.3312(0.4708) Grad: 5.4461  LR: 0.00001985  \n",
      "Epoch: [1][267/953] Elapsed 8m 38s (remain 22m 6s) Loss: 0.3351(0.4703) Grad: 5.1913  LR: 0.00001984  \n",
      "Epoch: [1][268/953] Elapsed 8m 40s (remain 22m 3s) Loss: 0.3143(0.4697) Grad: 5.3013  LR: 0.00001984  \n",
      "Epoch: [1][269/953] Elapsed 8m 42s (remain 22m 1s) Loss: 0.3213(0.4692) Grad: 5.4348  LR: 0.00001984  \n",
      "Epoch: [1][270/953] Elapsed 8m 44s (remain 21m 59s) Loss: 0.3150(0.4686) Grad: 5.2672  LR: 0.00001984  \n",
      "Epoch: [1][271/953] Elapsed 8m 46s (remain 21m 56s) Loss: 0.3200(0.4681) Grad: 5.3901  LR: 0.00001984  \n",
      "Epoch: [1][272/953] Elapsed 8m 47s (remain 21m 54s) Loss: 0.3089(0.4675) Grad: 5.0343  LR: 0.00001984  \n",
      "Epoch: [1][273/953] Elapsed 8m 49s (remain 21m 52s) Loss: 0.3221(0.4669) Grad: 5.5173  LR: 0.00001984  \n",
      "Epoch: [1][274/953] Elapsed 8m 51s (remain 21m 50s) Loss: 0.3213(0.4664) Grad: 5.3113  LR: 0.00001984  \n",
      "Epoch: [1][275/953] Elapsed 8m 53s (remain 21m 48s) Loss: 0.3246(0.4659) Grad: 5.4014  LR: 0.00001983  \n",
      "Epoch: [1][276/953] Elapsed 8m 55s (remain 21m 46s) Loss: 0.3089(0.4653) Grad: 4.8704  LR: 0.00001983  \n",
      "Epoch: [1][277/953] Elapsed 8m 56s (remain 21m 43s) Loss: 0.3108(0.4648) Grad: 5.1071  LR: 0.00001983  \n",
      "Epoch: [1][278/953] Elapsed 8m 58s (remain 21m 41s) Loss: 0.3065(0.4642) Grad: 4.9597  LR: 0.00001983  \n",
      "Epoch: [1][279/953] Elapsed 9m 0s (remain 21m 39s) Loss: 0.3109(0.4637) Grad: 5.2779  LR: 0.00001983  \n",
      "Epoch: [1][280/953] Elapsed 9m 2s (remain 21m 36s) Loss: 0.3340(0.4632) Grad: 5.1610  LR: 0.00001983  \n",
      "Epoch: [1][281/953] Elapsed 9m 4s (remain 21m 34s) Loss: 0.3093(0.4627) Grad: 5.2605  LR: 0.00001983  \n",
      "Epoch: [1][282/953] Elapsed 9m 5s (remain 21m 32s) Loss: 0.3234(0.4622) Grad: 4.9423  LR: 0.00001983  \n",
      "Epoch: [1][283/953] Elapsed 9m 7s (remain 21m 30s) Loss: 0.3053(0.4616) Grad: 5.1988  LR: 0.00001983  \n",
      "Epoch: [1][284/953] Elapsed 9m 9s (remain 21m 28s) Loss: 0.3101(0.4611) Grad: 5.0697  LR: 0.00001982  \n",
      "Epoch: [1][285/953] Elapsed 9m 11s (remain 21m 26s) Loss: 0.3189(0.4606) Grad: 5.1770  LR: 0.00001982  \n",
      "Epoch: [1][286/953] Elapsed 9m 13s (remain 21m 23s) Loss: 0.3017(0.4600) Grad: 5.0380  LR: 0.00001982  \n",
      "Epoch: [1][287/953] Elapsed 9m 15s (remain 21m 21s) Loss: 0.3138(0.4595) Grad: 4.7230  LR: 0.00001982  \n",
      "Epoch: [1][288/953] Elapsed 9m 16s (remain 21m 19s) Loss: 0.3064(0.4590) Grad: 4.7929  LR: 0.00001982  \n",
      "Epoch: [1][289/953] Elapsed 9m 18s (remain 21m 17s) Loss: 0.3058(0.4585) Grad: 4.8289  LR: 0.00001982  \n",
      "Epoch: [1][290/953] Elapsed 9m 20s (remain 21m 15s) Loss: 0.3046(0.4579) Grad: 5.2152  LR: 0.00001982  \n",
      "Epoch: [1][291/953] Elapsed 9m 22s (remain 21m 13s) Loss: 0.3058(0.4574) Grad: 5.0631  LR: 0.00001982  \n",
      "Epoch: [1][292/953] Elapsed 9m 24s (remain 21m 11s) Loss: 0.3026(0.4569) Grad: 5.1572  LR: 0.00001981  \n",
      "Epoch: [1][293/953] Elapsed 9m 26s (remain 21m 9s) Loss: 0.3073(0.4564) Grad: 4.9264  LR: 0.00001981  \n",
      "Epoch: [1][294/953] Elapsed 9m 28s (remain 21m 7s) Loss: 0.3093(0.4559) Grad: 4.7707  LR: 0.00001981  \n",
      "Epoch: [1][295/953] Elapsed 9m 29s (remain 21m 4s) Loss: 0.3061(0.4554) Grad: 5.0522  LR: 0.00001981  \n",
      "Epoch: [1][296/953] Elapsed 9m 31s (remain 21m 2s) Loss: 0.3053(0.4549) Grad: 5.1362  LR: 0.00001981  \n",
      "Epoch: [1][297/953] Elapsed 9m 33s (remain 21m 0s) Loss: 0.3018(0.4543) Grad: 4.9231  LR: 0.00001981  \n",
      "Epoch: [1][298/953] Elapsed 9m 35s (remain 20m 58s) Loss: 0.2964(0.4538) Grad: 5.1670  LR: 0.00001981  \n",
      "Epoch: [1][299/953] Elapsed 9m 37s (remain 20m 55s) Loss: 0.3023(0.4533) Grad: 5.2310  LR: 0.00001981  \n",
      "Epoch: [1][300/953] Elapsed 9m 38s (remain 20m 54s) Loss: 0.2894(0.4528) Grad: 4.9614  LR: 0.00001980  \n",
      "Epoch: [1][301/953] Elapsed 9m 40s (remain 20m 52s) Loss: 0.3076(0.4523) Grad: 4.7114  LR: 0.00001980  \n",
      "Epoch: [1][302/953] Elapsed 9m 42s (remain 20m 49s) Loss: 0.3019(0.4518) Grad: 4.9254  LR: 0.00001980  \n",
      "Epoch: [1][303/953] Elapsed 9m 44s (remain 20m 47s) Loss: 0.2830(0.4512) Grad: 4.7510  LR: 0.00001980  \n",
      "Epoch: [1][304/953] Elapsed 9m 46s (remain 20m 45s) Loss: 0.2931(0.4507) Grad: 4.8195  LR: 0.00001980  \n",
      "Epoch: [1][305/953] Elapsed 9m 48s (remain 20m 43s) Loss: 0.2822(0.4502) Grad: 4.7204  LR: 0.00001980  \n",
      "Epoch: [1][306/953] Elapsed 9m 49s (remain 20m 41s) Loss: 0.2895(0.4496) Grad: 4.6537  LR: 0.00001980  \n",
      "Epoch: [1][307/953] Elapsed 9m 51s (remain 20m 38s) Loss: 0.3110(0.4492) Grad: 4.3610  LR: 0.00001979  \n",
      "Epoch: [1][308/953] Elapsed 9m 53s (remain 20m 36s) Loss: 0.2915(0.4487) Grad: 4.9480  LR: 0.00001979  \n",
      "Epoch: [1][309/953] Elapsed 9m 55s (remain 20m 34s) Loss: 0.2892(0.4482) Grad: 4.8389  LR: 0.00001979  \n",
      "Epoch: [1][310/953] Elapsed 9m 57s (remain 20m 32s) Loss: 0.3045(0.4477) Grad: 4.7308  LR: 0.00001979  \n",
      "Epoch: [1][311/953] Elapsed 9m 58s (remain 20m 30s) Loss: 0.2851(0.4472) Grad: 4.8135  LR: 0.00001979  \n",
      "Epoch: [1][312/953] Elapsed 10m 0s (remain 20m 28s) Loss: 0.3018(0.4467) Grad: 4.2587  LR: 0.00001979  \n",
      "Epoch: [1][313/953] Elapsed 10m 2s (remain 20m 26s) Loss: 0.2881(0.4462) Grad: 4.7246  LR: 0.00001979  \n",
      "Epoch: [1][314/953] Elapsed 10m 4s (remain 20m 23s) Loss: 0.2813(0.4457) Grad: 4.9593  LR: 0.00001979  \n",
      "Epoch: [1][315/953] Elapsed 10m 6s (remain 20m 21s) Loss: 0.2743(0.4452) Grad: 4.8092  LR: 0.00001978  \n",
      "Epoch: [1][316/953] Elapsed 10m 7s (remain 20m 19s) Loss: 0.2931(0.4447) Grad: 4.7194  LR: 0.00001978  \n",
      "Epoch: [1][317/953] Elapsed 10m 9s (remain 20m 17s) Loss: 0.2928(0.4442) Grad: 4.5446  LR: 0.00001978  \n",
      "Epoch: [1][318/953] Elapsed 10m 11s (remain 20m 15s) Loss: 0.2841(0.4437) Grad: 4.6047  LR: 0.00001978  \n",
      "Epoch: [1][319/953] Elapsed 10m 13s (remain 20m 13s) Loss: 0.2874(0.4432) Grad: 4.7792  LR: 0.00001978  \n",
      "Epoch: [1][320/953] Elapsed 10m 15s (remain 20m 11s) Loss: 0.2805(0.4427) Grad: 4.2536  LR: 0.00001978  \n",
      "Epoch: [1][321/953] Elapsed 10m 16s (remain 20m 8s) Loss: 0.2857(0.4422) Grad: 4.4742  LR: 0.00001978  \n",
      "Epoch: [1][322/953] Elapsed 10m 18s (remain 20m 6s) Loss: 0.2783(0.4417) Grad: 4.7475  LR: 0.00001977  \n",
      "Epoch: [1][323/953] Elapsed 10m 20s (remain 20m 4s) Loss: 0.2739(0.4412) Grad: 4.5573  LR: 0.00001977  \n",
      "Epoch: [1][324/953] Elapsed 10m 22s (remain 20m 2s) Loss: 0.2818(0.4407) Grad: 4.7524  LR: 0.00001977  \n",
      "Epoch: [1][325/953] Elapsed 10m 24s (remain 20m 0s) Loss: 0.2857(0.4402) Grad: 4.5165  LR: 0.00001977  \n",
      "Epoch: [1][326/953] Elapsed 10m 25s (remain 19m 58s) Loss: 0.2794(0.4397) Grad: 4.6810  LR: 0.00001977  \n",
      "Epoch: [1][327/953] Elapsed 10m 28s (remain 19m 57s) Loss: 0.2822(0.4392) Grad: 4.5667  LR: 0.00001977  \n",
      "Epoch: [1][328/953] Elapsed 10m 31s (remain 19m 57s) Loss: 0.2735(0.4387) Grad: 4.6704  LR: 0.00001977  \n",
      "Epoch: [1][329/953] Elapsed 10m 34s (remain 19m 57s) Loss: 0.2725(0.4382) Grad: 4.5743  LR: 0.00001976  \n",
      "Epoch: [1][330/953] Elapsed 10m 37s (remain 19m 57s) Loss: 0.2771(0.4378) Grad: 4.6312  LR: 0.00001976  \n",
      "Epoch: [1][331/953] Elapsed 10m 40s (remain 19m 57s) Loss: 0.2857(0.4373) Grad: 4.5974  LR: 0.00001976  \n",
      "Epoch: [1][332/953] Elapsed 10m 42s (remain 19m 56s) Loss: 0.2837(0.4368) Grad: 4.5756  LR: 0.00001976  \n",
      "Epoch: [1][333/953] Elapsed 10m 44s (remain 19m 54s) Loss: 0.2795(0.4364) Grad: 4.7010  LR: 0.00001976  \n",
      "Epoch: [1][334/953] Elapsed 10m 47s (remain 19m 53s) Loss: 0.2821(0.4359) Grad: 4.0766  LR: 0.00001976  \n",
      "Epoch: [1][335/953] Elapsed 10m 49s (remain 19m 53s) Loss: 0.2934(0.4355) Grad: 4.4177  LR: 0.00001976  \n",
      "Epoch: [1][336/953] Elapsed 10m 52s (remain 19m 52s) Loss: 0.2671(0.4350) Grad: 4.6905  LR: 0.00001975  \n",
      "Epoch: [1][337/953] Elapsed 10m 54s (remain 19m 51s) Loss: 0.2835(0.4345) Grad: 4.3565  LR: 0.00001975  \n",
      "Epoch: [1][338/953] Elapsed 10m 57s (remain 19m 50s) Loss: 0.2763(0.4341) Grad: 4.4647  LR: 0.00001975  \n",
      "Epoch: [1][339/953] Elapsed 11m 0s (remain 19m 50s) Loss: 0.2827(0.4336) Grad: 4.2640  LR: 0.00001975  \n",
      "Epoch: [1][340/953] Elapsed 11m 2s (remain 19m 49s) Loss: 0.2799(0.4332) Grad: 4.2301  LR: 0.00001975  \n",
      "Epoch: [1][341/953] Elapsed 11m 5s (remain 19m 48s) Loss: 0.2800(0.4327) Grad: 4.1452  LR: 0.00001975  \n",
      "Epoch: [1][342/953] Elapsed 11m 7s (remain 19m 47s) Loss: 0.2715(0.4323) Grad: 4.5609  LR: 0.00001975  \n",
      "Epoch: [1][343/953] Elapsed 11m 10s (remain 19m 47s) Loss: 0.2807(0.4318) Grad: 4.3320  LR: 0.00001974  \n",
      "Epoch: [1][344/953] Elapsed 11m 13s (remain 19m 46s) Loss: 0.2618(0.4313) Grad: 4.4955  LR: 0.00001974  \n",
      "Epoch: [1][345/953] Elapsed 11m 15s (remain 19m 45s) Loss: 0.2844(0.4309) Grad: 4.4243  LR: 0.00001974  \n",
      "Epoch: [1][346/953] Elapsed 11m 18s (remain 19m 44s) Loss: 0.2617(0.4304) Grad: 4.3855  LR: 0.00001974  \n",
      "Epoch: [1][347/953] Elapsed 11m 20s (remain 19m 43s) Loss: 0.2709(0.4299) Grad: 4.4277  LR: 0.00001974  \n",
      "Epoch: [1][348/953] Elapsed 11m 23s (remain 19m 42s) Loss: 0.2794(0.4295) Grad: 4.2842  LR: 0.00001974  \n",
      "Epoch: [1][349/953] Elapsed 11m 25s (remain 19m 41s) Loss: 0.2695(0.4291) Grad: 4.5380  LR: 0.00001974  \n",
      "Epoch: [1][350/953] Elapsed 11m 28s (remain 19m 40s) Loss: 0.2713(0.4286) Grad: 4.4523  LR: 0.00001973  \n",
      "Epoch: [1][351/953] Elapsed 11m 31s (remain 19m 39s) Loss: 0.2689(0.4282) Grad: 4.2339  LR: 0.00001973  \n",
      "Epoch: [1][352/953] Elapsed 11m 33s (remain 19m 38s) Loss: 0.2615(0.4277) Grad: 4.3578  LR: 0.00001973  \n",
      "Epoch: [1][353/953] Elapsed 11m 36s (remain 19m 38s) Loss: 0.2726(0.4272) Grad: 4.2318  LR: 0.00001973  \n",
      "Epoch: [1][354/953] Elapsed 11m 39s (remain 19m 38s) Loss: 0.2755(0.4268) Grad: 4.3064  LR: 0.00001973  \n",
      "Epoch: [1][355/953] Elapsed 11m 42s (remain 19m 37s) Loss: 0.2759(0.4264) Grad: 4.3250  LR: 0.00001973  \n",
      "Epoch: [1][356/953] Elapsed 11m 45s (remain 19m 37s) Loss: 0.2695(0.4260) Grad: 4.2018  LR: 0.00001972  \n",
      "Epoch: [1][357/953] Elapsed 11m 48s (remain 19m 36s) Loss: 0.2531(0.4255) Grad: 4.5366  LR: 0.00001972  \n",
      "Epoch: [1][358/953] Elapsed 11m 50s (remain 19m 35s) Loss: 0.2616(0.4250) Grad: 4.3754  LR: 0.00001972  \n",
      "Epoch: [1][359/953] Elapsed 11m 53s (remain 19m 34s) Loss: 0.2590(0.4246) Grad: 4.2027  LR: 0.00001972  \n",
      "Epoch: [1][360/953] Elapsed 11m 55s (remain 19m 34s) Loss: 0.2719(0.4241) Grad: 4.2451  LR: 0.00001972  \n",
      "Epoch: [1][361/953] Elapsed 11m 58s (remain 19m 33s) Loss: 0.2608(0.4237) Grad: 4.3659  LR: 0.00001972  \n",
      "Epoch: [1][362/953] Elapsed 12m 1s (remain 19m 32s) Loss: 0.2900(0.4233) Grad: 4.1367  LR: 0.00001972  \n",
      "Epoch: [1][363/953] Elapsed 12m 4s (remain 19m 32s) Loss: 0.2599(0.4229) Grad: 4.3103  LR: 0.00001971  \n",
      "Epoch: [1][364/953] Elapsed 12m 7s (remain 19m 31s) Loss: 0.2715(0.4224) Grad: 4.0338  LR: 0.00001971  \n",
      "Epoch: [1][365/953] Elapsed 12m 9s (remain 19m 29s) Loss: 0.2575(0.4220) Grad: 4.3032  LR: 0.00001971  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11512\\3529880504.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrn_fold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0m_oof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[0moof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moof_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_oof_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"========== fold: {fold} result ==========\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11512\\4055035133.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(folds, fold)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m# eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11512\\1794449400.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0my_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# [12, 458, 1]; batch is 12, max_len is 458\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0my_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11512\\1514513120.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_dropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11512\\1514513120.py\u001b[0m in \u001b[0;36mfeature\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mlast_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlast_hidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    957\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 959\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    960\u001b[0m         )\n\u001b[0;32m    961\u001b[0m         \u001b[0mencoded_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    451\u001b[0m                     \u001b[0mrelative_pos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m                     \u001b[0mrel_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m                 )\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0mrelative_pos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m             \u001b[0mrel_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         )\n\u001b[0;32m    360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mrelative_pos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrelative_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mrel_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m         )\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    645\u001b[0m             \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_logits_proj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXSoftmax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtalking_head\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Le\\.conda\\envs\\deberta\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, mask, dim)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mrmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m~\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-inf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = create_labels_for_scoring(oof_df)\n",
    "        predictions = oof_df[[i for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "        \n",
    "    if CFG.wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
