{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start, the env setup \n",
    "1. use conda to create a new env\n",
    "```bash\n",
    "    conda create -n nmbe python=3.7\n",
    "    conda activate nmbe\n",
    "```\n",
    "\n",
    "2. install the following packages\n",
    "```bash\n",
    "    pip install torch==1.10.0\n",
    "    pip install sacremoses==0.0.41\n",
    "    pip install transformers\n",
    "```\n",
    "```bash if using cuda\n",
    "    pip install torch==1.10.0.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    pip install sacremoses==0.0.41\n",
    "    pip install transformers\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # About this notebook\n",
    " - Deberta-base starter code\n",
    " - pip wheels is [here](https://www.kaggle.com/yasufuminakama/nbme-pip-wheels)\n",
    " - Inference notebook is [here](https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-inference)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "root = '.'\n",
    "OUTPUT_DIR = root + '/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG (Classifier Free Guidance?)\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    # These are not necessary - delete?\n",
    "    #competition='NBME'\n",
    "    #_wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model=\"microsoft/deberta-base\"\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=5\n",
    "    encoder_lr=2e-4\n",
    "    decoder_lr=2e-4\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=12\n",
    "    fc_dropout=0.2\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    # possible to simply delete folds that are already trained\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.13.3\n",
      "transformers.__version__: 4.16.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import ast\n",
    "import copy\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from torch.nn import Parameter\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.system('pip uninstall -y transformers')\n",
    "os.system('python -m pip install --no-index --find-links=./input/nbme-pip-wheels transformers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import (AutoConfig, AutoModel, AutoTokenizer,\n",
    "                          get_cosine_schedule_with_warmup,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = \"cpu\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_scaler = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'transferlearn'):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed) if torch.cuda.is_available() else torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (14300, 6)\n",
      "features.shape: (143, 3)\n",
      "patient_notes.shape: (42146, 3)\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/train.csv')\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval) # ['mom'] -> [mom]\n",
    "train['location'] = train['location'].apply(ast.literal_eval) # ['0 1'] -> [0, 1]\n",
    "features = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/features.csv')\n",
    "def preprocess_features(features):\n",
    "    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    return features\n",
    "features = preprocess_features(features)\n",
    "patient_notes = pd.read_csv(root+'/input/nbme-score-clinical-patient-notes/patient_notes.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "#display(train.head())\n",
    "print(f\"features.shape: {features.shape}\")\n",
    "#display(features.head())\n",
    "print(f\"patient_notes.shape: {patient_notes.shape}\")\n",
    "#display(patient_notes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "#display(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "#display(train['annotation_length'].value_counts())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=CFG.n_fold) # for loop for 5 rounds, val_index is a list of index of validation data, others are index of training data\n",
    "groups = train['pn_num'].values # make sure same patient does not appear in train & valid at the same time \n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train['location'], groups)): # Fold.split(x, y, groups), each fold has same distribution of y\n",
    "    train.loc[val_index, 'fold'] = int(n) # add fold column, and fill with fold number which this row belongs to evaluation\n",
    "train['fold'] = train['fold'].astype(int) # validation data is len(train)/5 = 2860, each fold 1~5 have 2860 testing data and 11440 training data \n",
    "# display(train.groupby('fold').size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    # display(train.groupby('fold').size())\n",
    "    train = train.sample(n=1000, random_state=0).reset_index(drop=True)\n",
    "    # display(train.groupby('fold').size())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42146 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42146/42146 [00:58<00:00, 720.55it/s]\n",
      "pn_history max(lengths): 433\n",
      "pn_history max(lengths): 433\n",
      "100%|██████████| 143/143 [00:00<00:00, 4779.21it/s]\n",
      "feature_text max(lengths): 30\n",
      "feature_text max(lengths): 30\n",
      "max_len: 466\n",
      "max_len: 466\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "for text_col in ['pn_history']:\n",
    "    pn_history_lengths = []\n",
    "    tk0 = tqdm(patient_notes[text_col].fillna(\"\").values, total=len(patient_notes)) # tqdm: progress bar \n",
    "    for text in tk0: # tk0 is a list of text which is text_col column of patient_notes \n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids']) # turn the sentence to index: len([1360, 12, 180, ...])\n",
    "        pn_history_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(pn_history_lengths)}')\n",
    "\n",
    "for text_col in ['feature_text']:\n",
    "    features_lengths = []\n",
    "    tk0 = tqdm(features[text_col].fillna(\"\").values, total=len(features))\n",
    "    for text in tk0:\n",
    "        length = len(tokenizer(text, add_special_tokens=False)['input_ids']) # 'Intermittent' -> ['In', 'term',...] -> [1360, 12, 180, ...]\n",
    "        features_lengths.append(length)\n",
    "    LOGGER.info(f'{text_col} max(lengths): {max(features_lengths)}')\n",
    "\n",
    "CFG.max_len = max(pn_history_lengths) + max(features_lengths) + 3 # cls 開始 & sep 病例結果 & sep 特徵結尾 \n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text, feature_text):\n",
    "    inputs = cfg.tokenizer(text, feature_text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CFG.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items(): # k = \n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def create_label(cfg, text, annotation_length, location_list):\n",
    "    encoded = cfg.tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=CFG.max_len,\n",
    "                            padding=\"max_length\",\n",
    "                            return_offsets_mapping=True)\n",
    "    offset_mapping = encoded['offset_mapping'] # index of each token: (0, 0) (0, 1) (1, 2) (2, 3) (3, 4) ...\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0] # list all None token index\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1 # filled all None token index with = -1; [-1, 0, 0, ..., -1, -1, -1];  0 is no features, -1 is None token  \n",
    "    if annotation_length != 0:\n",
    "        for location in location_list: # location = '237 242;261 268' \n",
    "            for loc in [s.split() for s in location.split(';')]: # [['237', '242'], ['261', '268']] => loc = ['237', '242']\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]): # offset_mapping[idx] = (0, 0); start < 0 -> start_idx = 0 \n",
    "                        start_idx = idx - 1 # find start_ans is in the range of offset_mapping \n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]): \n",
    "                        end_idx = idx + 1 # label[start_idx:end_idx], the last token is not included, so end_idx + 1 \n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1 # each is token [-1, 0, 0, ..., 1, 1, ..., -1, -1, -1]; -1 is None token, 0 is no features, 1 is features\n",
    "    return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.feature_texts = df['feature_text'].values\n",
    "        self.pn_historys = df['pn_history'].values\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "        self.locations = df['location'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, \n",
    "                               self.pn_historys[item], \n",
    "                               self.feature_texts[item])\n",
    "        label = create_label(self.cfg, \n",
    "                             self.pn_historys[item], \n",
    "                             self.annotation_lengths[item], \n",
    "                             self.locations[item])\n",
    "        return inputs, label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        # only last linear layer is trained\n",
    "        #for param in self.model.parameters():\n",
    "        #    param.requires_grad = False\n",
    "        for name, param in self.model.named_parameters():\n",
    "            print(name)\n",
    "            if '10' not in name and '11' not in name and '.rel_embeddings' not in name: # last 2 internal deberta layers, includes attention intermediate and output\n",
    "                param.requires_grad = False\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        # TODO: only train this layer\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Helpler functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    if use_scaler:\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)  \n",
    "    losses = AverageMeter() # calculate the average loss of each batch, \n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        if torch.cuda.is_available():\n",
    "            with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "                y_preds = model(inputs) # [12, 458, 1]; batch is 12, max_len is 458 \n",
    "        else:\n",
    "            y_preds = model(inputs)\n",
    "        \n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean() # choose the loss of label != -1, and calculate the mean of all none -1 loss\n",
    "        \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        \n",
    "        losses.update(loss.item(), batch_size) # loss=0.1, batch_size=12, losses.avg = (0.1*12 + 0.2*12 ...)/(12*num_batch)\n",
    "\n",
    "        scaler.scale(loss).backward() if use_scaler else loss.backward()\n",
    "        \n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm) # constrain the gradient to be less than max_grad_norm, to avoid gradient explosion\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1 # global_step is including the epoch\n",
    "        \n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0 and use_scaler:\n",
    "             scaler.step(optimizer)\n",
    "             scaler.update()\n",
    "             optimizer.zero_grad()\n",
    "             global_step += 1\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        end = time.time()\n",
    "        #if step % CFG.print_freq == 0 or step == (len(train_loader)-1) and torch.cuda.is_available():\n",
    "        print('Epoch: [{0}][{1}/{2}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                'Grad: {grad_norm:.4f}  '\n",
    "                'LR: {lr:.8f}  '\n",
    "                .format(epoch+1, step, len(train_loader), \n",
    "                        remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                        loss=losses,\n",
    "                        grad_norm=grad_norm,\n",
    "                        lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        # if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "        print('EVAL: [{0}/{1}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                .format(step, len(valid_loader),\n",
    "                        loss=losses,\n",
    "                        remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True) # if fold = 0, then fold_index = 1, 2, 3, 4 are training data\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_texts = valid_folds['pn_history'].values\n",
    "    valid_labels = create_labels_for_scoring(valid_folds)\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    # num_workers set to 0 because of an error in this specific torch version for windows\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=0, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    # TODO: understand this\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    best_score = 0.\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        predictions = predictions.reshape((len(valid_folds), CFG.max_len))\n",
    "        \n",
    "        # scoring\n",
    "        char_probs = get_char_probs(valid_texts, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[i for i in range(CFG.max_len)]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else torch.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.q_bias\n",
      "encoder.layer.0.attention.self.v_bias\n",
      "encoder.layer.0.attention.self.in_proj.weight\n",
      "encoder.layer.0.attention.self.pos_proj.weight\n",
      "encoder.layer.0.attention.self.pos_q_proj.weight\n",
      "encoder.layer.0.attention.self.pos_q_proj.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.q_bias\n",
      "encoder.layer.1.attention.self.v_bias\n",
      "encoder.layer.1.attention.self.in_proj.weight\n",
      "encoder.layer.1.attention.self.pos_proj.weight\n",
      "encoder.layer.1.attention.self.pos_q_proj.weight\n",
      "encoder.layer.1.attention.self.pos_q_proj.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.q_bias\n",
      "encoder.layer.2.attention.self.v_bias\n",
      "encoder.layer.2.attention.self.in_proj.weight\n",
      "encoder.layer.2.attention.self.pos_proj.weight\n",
      "encoder.layer.2.attention.self.pos_q_proj.weight\n",
      "encoder.layer.2.attention.self.pos_q_proj.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.q_bias\n",
      "encoder.layer.3.attention.self.v_bias\n",
      "encoder.layer.3.attention.self.in_proj.weight\n",
      "encoder.layer.3.attention.self.pos_proj.weight\n",
      "encoder.layer.3.attention.self.pos_q_proj.weight\n",
      "encoder.layer.3.attention.self.pos_q_proj.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.q_bias\n",
      "encoder.layer.4.attention.self.v_bias\n",
      "encoder.layer.4.attention.self.in_proj.weight\n",
      "encoder.layer.4.attention.self.pos_proj.weight\n",
      "encoder.layer.4.attention.self.pos_q_proj.weight\n",
      "encoder.layer.4.attention.self.pos_q_proj.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.q_bias\n",
      "encoder.layer.5.attention.self.v_bias\n",
      "encoder.layer.5.attention.self.in_proj.weight\n",
      "encoder.layer.5.attention.self.pos_proj.weight\n",
      "encoder.layer.5.attention.self.pos_q_proj.weight\n",
      "encoder.layer.5.attention.self.pos_q_proj.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.q_bias\n",
      "encoder.layer.6.attention.self.v_bias\n",
      "encoder.layer.6.attention.self.in_proj.weight\n",
      "encoder.layer.6.attention.self.pos_proj.weight\n",
      "encoder.layer.6.attention.self.pos_q_proj.weight\n",
      "encoder.layer.6.attention.self.pos_q_proj.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.q_bias\n",
      "encoder.layer.7.attention.self.v_bias\n",
      "encoder.layer.7.attention.self.in_proj.weight\n",
      "encoder.layer.7.attention.self.pos_proj.weight\n",
      "encoder.layer.7.attention.self.pos_q_proj.weight\n",
      "encoder.layer.7.attention.self.pos_q_proj.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.q_bias\n",
      "encoder.layer.8.attention.self.v_bias\n",
      "encoder.layer.8.attention.self.in_proj.weight\n",
      "encoder.layer.8.attention.self.pos_proj.weight\n",
      "encoder.layer.8.attention.self.pos_q_proj.weight\n",
      "encoder.layer.8.attention.self.pos_q_proj.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.q_bias\n",
      "encoder.layer.9.attention.self.v_bias\n",
      "encoder.layer.9.attention.self.in_proj.weight\n",
      "encoder.layer.9.attention.self.pos_proj.weight\n",
      "encoder.layer.9.attention.self.pos_q_proj.weight\n",
      "encoder.layer.9.attention.self.pos_q_proj.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.q_bias\n",
      "encoder.layer.10.attention.self.v_bias\n",
      "encoder.layer.10.attention.self.in_proj.weight\n",
      "encoder.layer.10.attention.self.pos_proj.weight\n",
      "encoder.layer.10.attention.self.pos_q_proj.weight\n",
      "encoder.layer.10.attention.self.pos_q_proj.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.q_bias\n",
      "encoder.layer.11.attention.self.v_bias\n",
      "encoder.layer.11.attention.self.in_proj.weight\n",
      "encoder.layer.11.attention.self.pos_proj.weight\n",
      "encoder.layer.11.attention.self.pos_q_proj.weight\n",
      "encoder.layer.11.attention.self.pos_q_proj.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "encoder.rel_embeddings.weight\n",
      "Epoch: [1][0/953] Elapsed 0m 0s (remain 10m 28s) Loss: 0.8042(0.8042) Grad: 12.3779  LR: 0.00002000  \n",
      "Epoch: [1][1/953] Elapsed 0m 1s (remain 8m 1s) Loss: 0.7178(0.7610) Grad: 11.2749  LR: 0.00002000  \n",
      "Epoch: [1][2/953] Elapsed 0m 1s (remain 7m 17s) Loss: 0.6732(0.7317) Grad: 10.9414  LR: 0.00002000  \n",
      "Epoch: [1][3/953] Elapsed 0m 1s (remain 6m 55s) Loss: 0.5929(0.6970) Grad: 9.3133  LR: 0.00002000  \n",
      "Epoch: [1][4/953] Elapsed 0m 2s (remain 6m 42s) Loss: 0.5434(0.6663) Grad: 9.0533  LR: 0.00002000  \n",
      "Epoch: [1][5/953] Elapsed 0m 2s (remain 6m 39s) Loss: 0.4888(0.6367) Grad: 8.2997  LR: 0.00002000  \n",
      "Epoch: [1][6/953] Elapsed 0m 2s (remain 6m 32s) Loss: 0.4461(0.6095) Grad: 7.6287  LR: 0.00002000  \n",
      "Epoch: [1][7/953] Elapsed 0m 3s (remain 6m 26s) Loss: 0.4007(0.5834) Grad: 7.1348  LR: 0.00002000  \n",
      "Epoch: [1][8/953] Elapsed 0m 3s (remain 6m 22s) Loss: 0.3724(0.5599) Grad: 6.3478  LR: 0.00002000  \n",
      "Epoch: [1][9/953] Elapsed 0m 4s (remain 6m 18s) Loss: 0.3196(0.5359) Grad: 5.5411  LR: 0.00002000  \n",
      "Epoch: [1][10/953] Elapsed 0m 4s (remain 6m 15s) Loss: 0.3085(0.5152) Grad: 5.2719  LR: 0.00002000  \n",
      "Epoch: [1][11/953] Elapsed 0m 4s (remain 6m 13s) Loss: 0.2882(0.4963) Grad: 4.8671  LR: 0.00002000  \n",
      "Epoch: [1][12/953] Elapsed 0m 5s (remain 6m 10s) Loss: 0.2591(0.4781) Grad: 4.2112  LR: 0.00002000  \n",
      "Epoch: [1][13/953] Elapsed 0m 5s (remain 6m 8s) Loss: 0.2331(0.4606) Grad: 3.8893  LR: 0.00002000  \n",
      "Epoch: [1][14/953] Elapsed 0m 5s (remain 6m 7s) Loss: 0.2105(0.4439) Grad: 3.7774  LR: 0.00002000  \n",
      "Epoch: [1][15/953] Elapsed 0m 6s (remain 6m 5s) Loss: 0.1914(0.4281) Grad: 3.2631  LR: 0.00002000  \n",
      "Epoch: [1][16/953] Elapsed 0m 6s (remain 6m 3s) Loss: 0.1968(0.4145) Grad: 2.9280  LR: 0.00002000  \n",
      "Epoch: [1][17/953] Elapsed 0m 6s (remain 6m 2s) Loss: 0.1767(0.4013) Grad: 2.6675  LR: 0.00002000  \n",
      "Epoch: [1][18/953] Elapsed 0m 7s (remain 6m 1s) Loss: 0.1506(0.3881) Grad: 2.4127  LR: 0.00002000  \n",
      "Epoch: [1][19/953] Elapsed 0m 7s (remain 6m 0s) Loss: 0.1516(0.3763) Grad: 2.4346  LR: 0.00002000  \n",
      "Epoch: [1][20/953] Elapsed 0m 8s (remain 5m 59s) Loss: 0.1371(0.3649) Grad: 1.9942  LR: 0.00002000  \n",
      "Epoch: [1][21/953] Elapsed 0m 8s (remain 5m 58s) Loss: 0.1502(0.3551) Grad: 1.7399  LR: 0.00002000  \n",
      "Epoch: [1][22/953] Elapsed 0m 8s (remain 5m 57s) Loss: 0.1249(0.3451) Grad: 1.6458  LR: 0.00002000  \n",
      "Epoch: [1][23/953] Elapsed 0m 9s (remain 5m 56s) Loss: 0.1297(0.3361) Grad: 1.5506  LR: 0.00002000  \n",
      "Epoch: [1][24/953] Elapsed 0m 9s (remain 5m 55s) Loss: 0.1225(0.3276) Grad: 1.4463  LR: 0.00002000  \n",
      "Epoch: [1][25/953] Elapsed 0m 9s (remain 5m 54s) Loss: 0.1376(0.3203) Grad: 1.0707  LR: 0.00002000  \n",
      "Epoch: [1][26/953] Elapsed 0m 10s (remain 5m 54s) Loss: 0.1200(0.3129) Grad: 1.1612  LR: 0.00002000  \n",
      "Epoch: [1][27/953] Elapsed 0m 10s (remain 5m 53s) Loss: 0.0814(0.3046) Grad: 1.2044  LR: 0.00002000  \n",
      "Epoch: [1][28/953] Elapsed 0m 11s (remain 5m 52s) Loss: 0.1383(0.2989) Grad: 0.6905  LR: 0.00002000  \n",
      "Epoch: [1][29/953] Elapsed 0m 11s (remain 5m 52s) Loss: 0.0742(0.2914) Grad: 1.1504  LR: 0.00002000  \n",
      "Epoch: [1][30/953] Elapsed 0m 11s (remain 5m 51s) Loss: 0.1256(0.2860) Grad: 0.6819  LR: 0.00002000  \n",
      "Epoch: [1][31/953] Elapsed 0m 12s (remain 5m 50s) Loss: 0.0911(0.2799) Grad: 0.8105  LR: 0.00002000  \n",
      "Epoch: [1][32/953] Elapsed 0m 12s (remain 5m 50s) Loss: 0.0792(0.2739) Grad: 0.7785  LR: 0.00002000  \n",
      "Epoch: [1][33/953] Elapsed 0m 12s (remain 5m 49s) Loss: 0.1078(0.2690) Grad: 0.6747  LR: 0.00002000  \n",
      "Epoch: [1][34/953] Elapsed 0m 13s (remain 5m 48s) Loss: 0.1138(0.2645) Grad: 0.5647  LR: 0.00002000  \n",
      "Epoch: [1][35/953] Elapsed 0m 13s (remain 5m 48s) Loss: 0.0911(0.2597) Grad: 0.4733  LR: 0.00002000  \n",
      "Epoch: [1][36/953] Elapsed 0m 14s (remain 5m 47s) Loss: 0.0567(0.2542) Grad: 0.7454  LR: 0.00002000  \n",
      "Epoch: [1][37/953] Elapsed 0m 14s (remain 5m 47s) Loss: 0.0849(0.2498) Grad: 0.5769  LR: 0.00002000  \n",
      "Epoch: [1][38/953] Elapsed 0m 14s (remain 5m 46s) Loss: 0.0657(0.2451) Grad: 0.6180  LR: 0.00002000  \n",
      "Epoch: [1][39/953] Elapsed 0m 15s (remain 5m 45s) Loss: 0.0895(0.2412) Grad: 0.4250  LR: 0.00002000  \n",
      "Epoch: [1][40/953] Elapsed 0m 15s (remain 5m 45s) Loss: 0.0735(0.2371) Grad: 0.5200  LR: 0.00002000  \n",
      "Epoch: [1][41/953] Elapsed 0m 15s (remain 5m 44s) Loss: 0.0983(0.2338) Grad: 0.2815  LR: 0.00002000  \n",
      "Epoch: [1][42/953] Elapsed 0m 16s (remain 5m 44s) Loss: 0.0631(0.2298) Grad: 0.5178  LR: 0.00002000  \n",
      "Epoch: [1][43/953] Elapsed 0m 16s (remain 5m 43s) Loss: 0.0781(0.2264) Grad: 0.4013  LR: 0.00002000  \n",
      "Epoch: [1][44/953] Elapsed 0m 17s (remain 5m 43s) Loss: 0.0572(0.2226) Grad: 0.5250  LR: 0.00002000  \n",
      "Epoch: [1][45/953] Elapsed 0m 17s (remain 5m 42s) Loss: 0.0611(0.2191) Grad: 0.4715  LR: 0.00002000  \n",
      "Epoch: [1][46/953] Elapsed 0m 17s (remain 5m 42s) Loss: 0.0882(0.2163) Grad: 0.2952  LR: 0.00002000  \n",
      "Epoch: [1][47/953] Elapsed 0m 18s (remain 5m 41s) Loss: 0.1133(0.2142) Grad: 0.1846  LR: 0.00001999  \n",
      "Epoch: [1][48/953] Elapsed 0m 18s (remain 5m 41s) Loss: 0.0910(0.2116) Grad: 0.2755  LR: 0.00001999  \n",
      "Epoch: [1][49/953] Elapsed 0m 18s (remain 5m 40s) Loss: 0.0959(0.2093) Grad: 0.2355  LR: 0.00001999  \n",
      "Epoch: [1][50/953] Elapsed 0m 19s (remain 5m 40s) Loss: 0.0954(0.2071) Grad: 0.1468  LR: 0.00001999  \n",
      "Epoch: [1][51/953] Elapsed 0m 19s (remain 5m 39s) Loss: 0.0707(0.2045) Grad: 0.3016  LR: 0.00001999  \n",
      "Epoch: [1][52/953] Elapsed 0m 19s (remain 5m 39s) Loss: 0.0686(0.2019) Grad: 0.3185  LR: 0.00001999  \n",
      "Epoch: [1][53/953] Elapsed 0m 20s (remain 5m 39s) Loss: 0.0998(0.2000) Grad: 0.1427  LR: 0.00001999  \n",
      "Epoch: [1][54/953] Elapsed 0m 20s (remain 5m 38s) Loss: 0.0652(0.1976) Grad: 0.2656  LR: 0.00001999  \n",
      "Epoch: [1][55/953] Elapsed 0m 21s (remain 5m 38s) Loss: 0.1060(0.1959) Grad: 0.1557  LR: 0.00001999  \n",
      "Epoch: [1][56/953] Elapsed 0m 21s (remain 5m 37s) Loss: 0.0680(0.1937) Grad: 0.2942  LR: 0.00001999  \n",
      "Epoch: [1][57/953] Elapsed 0m 21s (remain 5m 37s) Loss: 0.0530(0.1913) Grad: 0.3189  LR: 0.00001999  \n",
      "Epoch: [1][58/953] Elapsed 0m 22s (remain 5m 36s) Loss: 0.0596(0.1890) Grad: 0.3127  LR: 0.00001999  \n",
      "Epoch: [1][59/953] Elapsed 0m 22s (remain 5m 36s) Loss: 0.1264(0.1880) Grad: 0.2132  LR: 0.00001999  \n",
      "Epoch: [1][60/953] Elapsed 0m 22s (remain 5m 35s) Loss: 0.0760(0.1862) Grad: 0.1981  LR: 0.00001999  \n",
      "Epoch: [1][61/953] Elapsed 0m 23s (remain 5m 35s) Loss: 0.0857(0.1845) Grad: 0.2086  LR: 0.00001999  \n",
      "Epoch: [1][62/953] Elapsed 0m 23s (remain 5m 35s) Loss: 0.0878(0.1830) Grad: 0.1628  LR: 0.00001999  \n",
      "Epoch: [1][63/953] Elapsed 0m 24s (remain 5m 34s) Loss: 0.0948(0.1816) Grad: 0.1338  LR: 0.00001999  \n",
      "Epoch: [1][64/953] Elapsed 0m 24s (remain 5m 34s) Loss: 0.0602(0.1798) Grad: 0.2615  LR: 0.00001999  \n",
      "Epoch: [1][65/953] Elapsed 0m 24s (remain 5m 33s) Loss: 0.0712(0.1781) Grad: 0.1975  LR: 0.00001999  \n",
      "Epoch: [1][66/953] Elapsed 0m 25s (remain 5m 33s) Loss: 0.0639(0.1764) Grad: 0.1917  LR: 0.00001999  \n",
      "Epoch: [1][67/953] Elapsed 0m 25s (remain 5m 33s) Loss: 0.0946(0.1752) Grad: 0.1350  LR: 0.00001999  \n",
      "Epoch: [1][68/953] Elapsed 0m 25s (remain 5m 32s) Loss: 0.1200(0.1744) Grad: 0.2145  LR: 0.00001999  \n",
      "Epoch: [1][69/953] Elapsed 0m 26s (remain 5m 32s) Loss: 0.0708(0.1729) Grad: 0.1529  LR: 0.00001999  \n",
      "Epoch: [1][70/953] Elapsed 0m 26s (remain 5m 31s) Loss: 0.0497(0.1712) Grad: 0.2626  LR: 0.00001999  \n",
      "Epoch: [1][71/953] Elapsed 0m 27s (remain 5m 31s) Loss: 0.0637(0.1697) Grad: 0.2357  LR: 0.00001999  \n",
      "Epoch: [1][72/953] Elapsed 0m 27s (remain 5m 31s) Loss: 0.0664(0.1683) Grad: 0.1659  LR: 0.00001999  \n",
      "Epoch: [1][73/953] Elapsed 0m 27s (remain 5m 30s) Loss: 0.0665(0.1669) Grad: 0.1581  LR: 0.00001999  \n",
      "Epoch: [1][74/953] Elapsed 0m 28s (remain 5m 30s) Loss: 0.0557(0.1654) Grad: 0.2598  LR: 0.00001999  \n",
      "Epoch: [1][75/953] Elapsed 0m 28s (remain 5m 29s) Loss: 0.0698(0.1642) Grad: 0.1558  LR: 0.00001999  \n",
      "Epoch: [1][76/953] Elapsed 0m 28s (remain 5m 29s) Loss: 0.1186(0.1636) Grad: 0.2166  LR: 0.00001999  \n",
      "Epoch: [1][77/953] Elapsed 0m 29s (remain 5m 28s) Loss: 0.1054(0.1628) Grad: 0.1444  LR: 0.00001999  \n",
      "Epoch: [1][78/953] Elapsed 0m 29s (remain 5m 28s) Loss: 0.0631(0.1616) Grad: 0.1953  LR: 0.00001999  \n",
      "Epoch: [1][79/953] Elapsed 0m 30s (remain 5m 28s) Loss: 0.0640(0.1603) Grad: 0.2002  LR: 0.00001999  \n",
      "Epoch: [1][80/953] Elapsed 0m 30s (remain 5m 27s) Loss: 0.0554(0.1590) Grad: 0.1852  LR: 0.00001999  \n",
      "Epoch: [1][81/953] Elapsed 0m 30s (remain 5m 27s) Loss: 0.0748(0.1580) Grad: 0.1244  LR: 0.00001999  \n",
      "Epoch: [1][82/953] Elapsed 0m 31s (remain 5m 26s) Loss: 0.0740(0.1570) Grad: 0.1468  LR: 0.00001999  \n",
      "Epoch: [1][83/953] Elapsed 0m 31s (remain 5m 26s) Loss: 0.1148(0.1565) Grad: 0.1738  LR: 0.00001998  \n",
      "Epoch: [1][84/953] Elapsed 0m 31s (remain 5m 26s) Loss: 0.0499(0.1552) Grad: 0.2398  LR: 0.00001998  \n",
      "Epoch: [1][85/953] Elapsed 0m 32s (remain 5m 25s) Loss: 0.0609(0.1541) Grad: 0.2087  LR: 0.00001998  \n",
      "Epoch: [1][86/953] Elapsed 0m 32s (remain 5m 25s) Loss: 0.0600(0.1531) Grad: 0.1845  LR: 0.00001998  \n",
      "Epoch: [1][87/953] Elapsed 0m 33s (remain 5m 25s) Loss: 0.0572(0.1520) Grad: 0.1910  LR: 0.00001998  \n",
      "Epoch: [1][88/953] Elapsed 0m 33s (remain 5m 24s) Loss: 0.0650(0.1510) Grad: 0.1287  LR: 0.00001998  \n",
      "Epoch: [1][89/953] Elapsed 0m 33s (remain 5m 24s) Loss: 0.0429(0.1498) Grad: 0.2584  LR: 0.00001998  \n",
      "Epoch: [1][90/953] Elapsed 0m 34s (remain 5m 23s) Loss: 0.0756(0.1490) Grad: 0.1543  LR: 0.00001998  \n",
      "Epoch: [1][91/953] Elapsed 0m 34s (remain 5m 23s) Loss: 0.0531(0.1479) Grad: 0.2041  LR: 0.00001998  \n",
      "Epoch: [1][92/953] Elapsed 0m 34s (remain 5m 23s) Loss: 0.0654(0.1471) Grad: 0.1275  LR: 0.00001998  \n",
      "Epoch: [1][93/953] Elapsed 0m 35s (remain 5m 22s) Loss: 0.0844(0.1464) Grad: 0.1015  LR: 0.00001998  \n",
      "Epoch: [1][94/953] Elapsed 0m 35s (remain 5m 22s) Loss: 0.0910(0.1458) Grad: 0.1269  LR: 0.00001998  \n",
      "Epoch: [1][95/953] Elapsed 0m 36s (remain 5m 21s) Loss: 0.0774(0.1451) Grad: 0.0963  LR: 0.00001998  \n",
      "Epoch: [1][96/953] Elapsed 0m 36s (remain 5m 21s) Loss: 0.0608(0.1442) Grad: 0.1564  LR: 0.00001998  \n",
      "Epoch: [1][97/953] Elapsed 0m 36s (remain 5m 21s) Loss: 0.0792(0.1436) Grad: 0.1243  LR: 0.00001998  \n",
      "Epoch: [1][98/953] Elapsed 0m 37s (remain 5m 20s) Loss: 0.0606(0.1427) Grad: 0.1656  LR: 0.00001998  \n",
      "Epoch: [1][99/953] Elapsed 0m 37s (remain 5m 20s) Loss: 0.0648(0.1419) Grad: 0.1381  LR: 0.00001998  \n",
      "Epoch: [1][100/953] Elapsed 0m 37s (remain 5m 19s) Loss: 0.0839(0.1414) Grad: 0.1087  LR: 0.00001998  \n",
      "Epoch: [1][101/953] Elapsed 0m 38s (remain 5m 19s) Loss: 0.0658(0.1406) Grad: 0.1394  LR: 0.00001998  \n",
      "Epoch: [1][102/953] Elapsed 0m 38s (remain 5m 19s) Loss: 0.0616(0.1399) Grad: 0.1728  LR: 0.00001998  \n",
      "Epoch: [1][103/953] Elapsed 0m 39s (remain 5m 18s) Loss: 0.0486(0.1390) Grad: 0.1745  LR: 0.00001998  \n",
      "Epoch: [1][104/953] Elapsed 0m 39s (remain 5m 18s) Loss: 0.0559(0.1382) Grad: 0.1888  LR: 0.00001998  \n",
      "Epoch: [1][105/953] Elapsed 0m 39s (remain 5m 18s) Loss: 0.0630(0.1375) Grad: 0.1701  LR: 0.00001998  \n",
      "Epoch: [1][106/953] Elapsed 0m 40s (remain 5m 17s) Loss: 0.0866(0.1370) Grad: 0.0935  LR: 0.00001998  \n",
      "Epoch: [1][107/953] Elapsed 0m 40s (remain 5m 17s) Loss: 0.0500(0.1362) Grad: 0.1966  LR: 0.00001997  \n",
      "Epoch: [1][108/953] Elapsed 0m 40s (remain 5m 16s) Loss: 0.0648(0.1355) Grad: 0.1084  LR: 0.00001997  \n",
      "Epoch: [1][109/953] Elapsed 0m 41s (remain 5m 16s) Loss: 0.0662(0.1349) Grad: 0.1403  LR: 0.00001997  \n",
      "Epoch: [1][110/953] Elapsed 0m 41s (remain 5m 16s) Loss: 0.0740(0.1344) Grad: 0.0984  LR: 0.00001997  \n",
      "Epoch: [1][111/953] Elapsed 0m 42s (remain 5m 15s) Loss: 0.0501(0.1336) Grad: 0.2187  LR: 0.00001997  \n",
      "Epoch: [1][112/953] Elapsed 0m 42s (remain 5m 15s) Loss: 0.0574(0.1329) Grad: 0.1756  LR: 0.00001997  \n",
      "Epoch: [1][113/953] Elapsed 0m 42s (remain 5m 14s) Loss: 0.0996(0.1326) Grad: 0.1506  LR: 0.00001997  \n",
      "Epoch: [1][114/953] Elapsed 0m 43s (remain 5m 14s) Loss: 0.1166(0.1325) Grad: 0.2782  LR: 0.00001997  \n",
      "Epoch: [1][115/953] Elapsed 0m 43s (remain 5m 14s) Loss: 0.0933(0.1322) Grad: 0.1084  LR: 0.00001997  \n",
      "Epoch: [1][116/953] Elapsed 0m 43s (remain 5m 13s) Loss: 0.0626(0.1316) Grad: 0.1475  LR: 0.00001997  \n",
      "Epoch: [1][117/953] Elapsed 0m 44s (remain 5m 13s) Loss: 0.0702(0.1311) Grad: 0.0861  LR: 0.00001997  \n",
      "Epoch: [1][118/953] Elapsed 0m 44s (remain 5m 13s) Loss: 0.0854(0.1307) Grad: 0.1073  LR: 0.00001997  \n",
      "Epoch: [1][119/953] Elapsed 0m 45s (remain 5m 12s) Loss: 0.0553(0.1300) Grad: 0.1668  LR: 0.00001997  \n",
      "Epoch: [1][120/953] Elapsed 0m 45s (remain 5m 12s) Loss: 0.0810(0.1296) Grad: 0.1517  LR: 0.00001997  \n",
      "Epoch: [1][121/953] Elapsed 0m 45s (remain 5m 11s) Loss: 0.0574(0.1290) Grad: 0.1199  LR: 0.00001997  \n",
      "Epoch: [1][122/953] Elapsed 0m 46s (remain 5m 11s) Loss: 0.0823(0.1287) Grad: 0.1296  LR: 0.00001997  \n",
      "Epoch: [1][123/953] Elapsed 0m 46s (remain 5m 11s) Loss: 0.0641(0.1281) Grad: 0.1201  LR: 0.00001997  \n",
      "Epoch: [1][124/953] Elapsed 0m 46s (remain 5m 10s) Loss: 0.0800(0.1278) Grad: 0.0910  LR: 0.00001997  \n",
      "Epoch: [1][125/953] Elapsed 0m 47s (remain 5m 10s) Loss: 0.0944(0.1275) Grad: 0.1132  LR: 0.00001997  \n",
      "Epoch: [1][126/953] Elapsed 0m 47s (remain 5m 10s) Loss: 0.0649(0.1270) Grad: 0.1335  LR: 0.00001996  \n",
      "Epoch: [1][127/953] Elapsed 0m 48s (remain 5m 9s) Loss: 0.0779(0.1266) Grad: 0.1113  LR: 0.00001996  \n",
      "Epoch: [1][128/953] Elapsed 0m 48s (remain 5m 9s) Loss: 0.0611(0.1261) Grad: 0.1460  LR: 0.00001996  \n",
      "Epoch: [1][129/953] Elapsed 0m 48s (remain 5m 8s) Loss: 0.0694(0.1257) Grad: 0.1044  LR: 0.00001996  \n",
      "Epoch: [1][130/953] Elapsed 0m 49s (remain 5m 8s) Loss: 0.0758(0.1253) Grad: 0.1054  LR: 0.00001996  \n",
      "Epoch: [1][131/953] Elapsed 0m 49s (remain 5m 8s) Loss: 0.0427(0.1247) Grad: 0.1719  LR: 0.00001996  \n",
      "Epoch: [1][132/953] Elapsed 0m 49s (remain 5m 7s) Loss: 0.0809(0.1243) Grad: 0.1075  LR: 0.00001996  \n",
      "Epoch: [1][133/953] Elapsed 0m 50s (remain 5m 7s) Loss: 0.0672(0.1239) Grad: 0.1168  LR: 0.00001996  \n",
      "Epoch: [1][134/953] Elapsed 0m 50s (remain 5m 7s) Loss: 0.0443(0.1233) Grad: 0.1932  LR: 0.00001996  \n",
      "Epoch: [1][135/953] Elapsed 0m 51s (remain 5m 6s) Loss: 0.0774(0.1230) Grad: 0.1168  LR: 0.00001996  \n",
      "Epoch: [1][136/953] Elapsed 0m 51s (remain 5m 6s) Loss: 0.0773(0.1227) Grad: 0.0904  LR: 0.00001996  \n",
      "Epoch: [1][137/953] Elapsed 0m 51s (remain 5m 5s) Loss: 0.0590(0.1222) Grad: 0.1394  LR: 0.00001996  \n",
      "Epoch: [1][138/953] Elapsed 0m 52s (remain 5m 5s) Loss: 0.0552(0.1217) Grad: 0.1263  LR: 0.00001996  \n",
      "Epoch: [1][139/953] Elapsed 0m 52s (remain 5m 5s) Loss: 0.0769(0.1214) Grad: 0.1166  LR: 0.00001996  \n",
      "Epoch: [1][140/953] Elapsed 0m 52s (remain 5m 4s) Loss: 0.0574(0.1209) Grad: 0.1542  LR: 0.00001996  \n",
      "Epoch: [1][141/953] Elapsed 0m 53s (remain 5m 4s) Loss: 0.0615(0.1205) Grad: 0.1160  LR: 0.00001996  \n",
      "Epoch: [1][142/953] Elapsed 0m 53s (remain 5m 4s) Loss: 0.0842(0.1203) Grad: 0.1150  LR: 0.00001996  \n",
      "Epoch: [1][143/953] Elapsed 0m 54s (remain 5m 3s) Loss: 0.0700(0.1199) Grad: 0.0944  LR: 0.00001995  \n",
      "Epoch: [1][144/953] Elapsed 0m 54s (remain 5m 3s) Loss: 0.0448(0.1194) Grad: 0.2073  LR: 0.00001995  \n",
      "Epoch: [1][145/953] Elapsed 0m 54s (remain 5m 2s) Loss: 0.0817(0.1191) Grad: 0.1100  LR: 0.00001995  \n",
      "Epoch: [1][146/953] Elapsed 0m 55s (remain 5m 2s) Loss: 0.1014(0.1190) Grad: 0.1730  LR: 0.00001995  \n",
      "Epoch: [1][147/953] Elapsed 0m 55s (remain 5m 2s) Loss: 0.1199(0.1190) Grad: 0.2410  LR: 0.00001995  \n",
      "Epoch: [1][148/953] Elapsed 0m 55s (remain 5m 1s) Loss: 0.0603(0.1186) Grad: 0.1463  LR: 0.00001995  \n",
      "Epoch: [1][149/953] Elapsed 0m 56s (remain 5m 1s) Loss: 0.0772(0.1184) Grad: 0.1075  LR: 0.00001995  \n",
      "Epoch: [1][150/953] Elapsed 0m 56s (remain 5m 1s) Loss: 0.0784(0.1181) Grad: 0.1395  LR: 0.00001995  \n",
      "Epoch: [1][151/953] Elapsed 0m 57s (remain 5m 0s) Loss: 0.0851(0.1179) Grad: 0.1116  LR: 0.00001995  \n",
      "Epoch: [1][152/953] Elapsed 0m 57s (remain 5m 0s) Loss: 0.0467(0.1174) Grad: 0.2093  LR: 0.00001995  \n",
      "Epoch: [1][153/953] Elapsed 0m 57s (remain 4m 59s) Loss: 0.0811(0.1172) Grad: 0.1165  LR: 0.00001995  \n",
      "Epoch: [1][154/953] Elapsed 0m 58s (remain 4m 59s) Loss: 0.0866(0.1170) Grad: 0.1133  LR: 0.00001995  \n",
      "Epoch: [1][155/953] Elapsed 0m 58s (remain 4m 59s) Loss: 0.0721(0.1167) Grad: 0.1001  LR: 0.00001995  \n",
      "Epoch: [1][156/953] Elapsed 0m 58s (remain 4m 58s) Loss: 0.0584(0.1163) Grad: 0.1542  LR: 0.00001995  \n",
      "Epoch: [1][157/953] Elapsed 0m 59s (remain 4m 58s) Loss: 0.0418(0.1158) Grad: 0.2394  LR: 0.00001995  \n",
      "Epoch: [1][158/953] Elapsed 0m 59s (remain 4m 58s) Loss: 0.0695(0.1155) Grad: 0.0885  LR: 0.00001995  \n",
      "Epoch: [1][159/953] Elapsed 1m 0s (remain 4m 57s) Loss: 0.0532(0.1152) Grad: 0.1500  LR: 0.00001994  \n",
      "Epoch: [1][160/953] Elapsed 1m 0s (remain 4m 57s) Loss: 0.0956(0.1150) Grad: 0.1259  LR: 0.00001994  \n",
      "Epoch: [1][161/953] Elapsed 1m 0s (remain 4m 57s) Loss: 0.0493(0.1146) Grad: 0.1794  LR: 0.00001994  \n",
      "Epoch: [1][162/953] Elapsed 1m 1s (remain 4m 56s) Loss: 0.0755(0.1144) Grad: 0.1060  LR: 0.00001994  \n",
      "Epoch: [1][163/953] Elapsed 1m 1s (remain 4m 56s) Loss: 0.0483(0.1140) Grad: 0.2398  LR: 0.00001994  \n",
      "Epoch: [1][164/953] Elapsed 1m 1s (remain 4m 55s) Loss: 0.0648(0.1137) Grad: 0.1097  LR: 0.00001994  \n",
      "Epoch: [1][165/953] Elapsed 1m 2s (remain 4m 55s) Loss: 0.0624(0.1134) Grad: 0.1184  LR: 0.00001994  \n",
      "Epoch: [1][166/953] Elapsed 1m 2s (remain 4m 55s) Loss: 0.0622(0.1131) Grad: 0.1066  LR: 0.00001994  \n",
      "Epoch: [1][167/953] Elapsed 1m 3s (remain 4m 54s) Loss: 0.0603(0.1128) Grad: 0.1228  LR: 0.00001994  \n",
      "Epoch: [1][168/953] Elapsed 1m 3s (remain 4m 54s) Loss: 0.0919(0.1126) Grad: 0.1555  LR: 0.00001994  \n",
      "Epoch: [1][169/953] Elapsed 1m 3s (remain 4m 54s) Loss: 0.0291(0.1121) Grad: 0.2630  LR: 0.00001994  \n",
      "Epoch: [1][170/953] Elapsed 1m 4s (remain 4m 53s) Loss: 0.0960(0.1121) Grad: 0.1286  LR: 0.00001994  \n",
      "Epoch: [1][171/953] Elapsed 1m 4s (remain 4m 53s) Loss: 0.0666(0.1118) Grad: 0.1163  LR: 0.00001994  \n",
      "Epoch: [1][172/953] Elapsed 1m 4s (remain 4m 52s) Loss: 0.0583(0.1115) Grad: 0.1399  LR: 0.00001994  \n",
      "Epoch: [1][173/953] Elapsed 1m 5s (remain 4m 52s) Loss: 0.0660(0.1112) Grad: 0.1766  LR: 0.00001993  \n",
      "Epoch: [1][174/953] Elapsed 1m 5s (remain 4m 52s) Loss: 0.1174(0.1113) Grad: 0.2507  LR: 0.00001993  \n",
      "Epoch: [1][175/953] Elapsed 1m 6s (remain 4m 51s) Loss: 0.0642(0.1110) Grad: 0.1222  LR: 0.00001993  \n",
      "Epoch: [1][176/953] Elapsed 1m 6s (remain 4m 51s) Loss: 0.0893(0.1109) Grad: 0.1515  LR: 0.00001993  \n",
      "Epoch: [1][177/953] Elapsed 1m 6s (remain 4m 51s) Loss: 0.0737(0.1107) Grad: 0.1386  LR: 0.00001993  \n",
      "Epoch: [1][178/953] Elapsed 1m 7s (remain 4m 50s) Loss: 0.0403(0.1103) Grad: 0.2097  LR: 0.00001993  \n",
      "Epoch: [1][179/953] Elapsed 1m 7s (remain 4m 50s) Loss: 0.0625(0.1100) Grad: 0.1573  LR: 0.00001993  \n",
      "Epoch: [1][180/953] Elapsed 1m 7s (remain 4m 49s) Loss: 0.0600(0.1097) Grad: 0.1080  LR: 0.00001993  \n",
      "Epoch: [1][181/953] Elapsed 1m 8s (remain 4m 49s) Loss: 0.0720(0.1095) Grad: 0.1141  LR: 0.00001993  \n",
      "Epoch: [1][182/953] Elapsed 1m 8s (remain 4m 49s) Loss: 0.0600(0.1092) Grad: 0.1372  LR: 0.00001993  \n",
      "Epoch: [1][183/953] Elapsed 1m 9s (remain 4m 48s) Loss: 0.0882(0.1091) Grad: 0.1278  LR: 0.00001993  \n",
      "Epoch: [1][184/953] Elapsed 1m 9s (remain 4m 48s) Loss: 0.0666(0.1089) Grad: 0.1170  LR: 0.00001993  \n",
      "Epoch: [1][185/953] Elapsed 1m 9s (remain 4m 48s) Loss: 0.0473(0.1086) Grad: 0.1855  LR: 0.00001992  \n",
      "Epoch: [1][186/953] Elapsed 1m 10s (remain 4m 47s) Loss: 0.0569(0.1083) Grad: 0.1355  LR: 0.00001992  \n",
      "Epoch: [1][187/953] Elapsed 1m 10s (remain 4m 47s) Loss: 0.0495(0.1080) Grad: 0.1932  LR: 0.00001992  \n",
      "Epoch: [1][188/953] Elapsed 1m 11s (remain 4m 47s) Loss: 0.1127(0.1080) Grad: 0.2407  LR: 0.00001992  \n",
      "Epoch: [1][189/953] Elapsed 1m 11s (remain 4m 46s) Loss: 0.0713(0.1078) Grad: 0.1108  LR: 0.00001992  \n",
      "Epoch: [1][190/953] Elapsed 1m 11s (remain 4m 46s) Loss: 0.0480(0.1075) Grad: 0.1724  LR: 0.00001992  \n",
      "Epoch: [1][191/953] Elapsed 1m 12s (remain 4m 45s) Loss: 0.0787(0.1073) Grad: 0.1048  LR: 0.00001992  \n",
      "Epoch: [1][192/953] Elapsed 1m 12s (remain 4m 45s) Loss: 0.0641(0.1071) Grad: 0.1191  LR: 0.00001992  \n",
      "Epoch: [1][193/953] Elapsed 1m 12s (remain 4m 45s) Loss: 0.0808(0.1070) Grad: 0.1105  LR: 0.00001992  \n",
      "Epoch: [1][194/953] Elapsed 1m 13s (remain 4m 44s) Loss: 0.1247(0.1071) Grad: 0.3141  LR: 0.00001992  \n",
      "Epoch: [1][195/953] Elapsed 1m 13s (remain 4m 44s) Loss: 0.0989(0.1070) Grad: 0.1883  LR: 0.00001992  \n",
      "Epoch: [1][196/953] Elapsed 1m 14s (remain 4m 44s) Loss: 0.0804(0.1069) Grad: 0.1061  LR: 0.00001992  \n",
      "Epoch: [1][197/953] Elapsed 1m 14s (remain 4m 43s) Loss: 0.0613(0.1067) Grad: 0.1138  LR: 0.00001991  \n",
      "Epoch: [1][198/953] Elapsed 1m 14s (remain 4m 43s) Loss: 0.0912(0.1066) Grad: 0.1070  LR: 0.00001991  \n",
      "Epoch: [1][199/953] Elapsed 1m 15s (remain 4m 42s) Loss: 0.0829(0.1065) Grad: 0.1073  LR: 0.00001991  \n",
      "Epoch: [1][200/953] Elapsed 1m 15s (remain 4m 42s) Loss: 0.0810(0.1063) Grad: 0.0973  LR: 0.00001991  \n",
      "Epoch: [1][201/953] Elapsed 1m 15s (remain 4m 42s) Loss: 0.0767(0.1062) Grad: 0.1055  LR: 0.00001991  \n",
      "Epoch: [1][202/953] Elapsed 1m 16s (remain 4m 41s) Loss: 0.0332(0.1058) Grad: 0.2421  LR: 0.00001991  \n",
      "Epoch: [1][203/953] Elapsed 1m 16s (remain 4m 41s) Loss: 0.0863(0.1057) Grad: 0.1100  LR: 0.00001991  \n",
      "Epoch: [1][204/953] Elapsed 1m 17s (remain 4m 41s) Loss: 0.0871(0.1057) Grad: 0.1006  LR: 0.00001991  \n",
      "Epoch: [1][205/953] Elapsed 1m 17s (remain 4m 40s) Loss: 0.1090(0.1057) Grad: 0.2017  LR: 0.00001991  \n",
      "Epoch: [1][206/953] Elapsed 1m 17s (remain 4m 40s) Loss: 0.0705(0.1055) Grad: 0.0942  LR: 0.00001991  \n",
      "Epoch: [1][207/953] Elapsed 1m 18s (remain 4m 40s) Loss: 0.0935(0.1054) Grad: 0.1865  LR: 0.00001991  \n",
      "Epoch: [1][208/953] Elapsed 1m 18s (remain 4m 39s) Loss: 0.0741(0.1053) Grad: 0.1156  LR: 0.00001991  \n",
      "Epoch: [1][209/953] Elapsed 1m 18s (remain 4m 39s) Loss: 0.0618(0.1051) Grad: 0.1038  LR: 0.00001990  \n",
      "Epoch: [1][210/953] Elapsed 1m 19s (remain 4m 39s) Loss: 0.0582(0.1049) Grad: 0.1308  LR: 0.00001990  \n",
      "Epoch: [1][211/953] Elapsed 1m 19s (remain 4m 38s) Loss: 0.0519(0.1046) Grad: 0.2132  LR: 0.00001990  \n",
      "Epoch: [1][212/953] Elapsed 1m 20s (remain 4m 38s) Loss: 0.0580(0.1044) Grad: 0.1377  LR: 0.00001990  \n",
      "Epoch: [1][213/953] Elapsed 1m 20s (remain 4m 37s) Loss: 0.0701(0.1042) Grad: 0.1623  LR: 0.00001990  \n",
      "Epoch: [1][214/953] Elapsed 1m 20s (remain 4m 37s) Loss: 0.0570(0.1040) Grad: 0.2111  LR: 0.00001990  \n",
      "Epoch: [1][215/953] Elapsed 1m 21s (remain 4m 37s) Loss: 0.0713(0.1039) Grad: 0.0953  LR: 0.00001990  \n",
      "Epoch: [1][216/953] Elapsed 1m 21s (remain 4m 36s) Loss: 0.0438(0.1036) Grad: 0.2090  LR: 0.00001990  \n",
      "Epoch: [1][217/953] Elapsed 1m 21s (remain 4m 36s) Loss: 0.0840(0.1035) Grad: 0.1035  LR: 0.00001990  \n",
      "Epoch: [1][218/953] Elapsed 1m 22s (remain 4m 36s) Loss: 0.0789(0.1034) Grad: 0.1199  LR: 0.00001990  \n",
      "Epoch: [1][219/953] Elapsed 1m 22s (remain 4m 35s) Loss: 0.0947(0.1033) Grad: 0.1312  LR: 0.00001990  \n",
      "Epoch: [1][220/953] Elapsed 1m 23s (remain 4m 35s) Loss: 0.0662(0.1032) Grad: 0.1092  LR: 0.00001989  \n",
      "Epoch: [1][221/953] Elapsed 1m 23s (remain 4m 34s) Loss: 0.0753(0.1031) Grad: 0.0871  LR: 0.00001989  \n",
      "Epoch: [1][222/953] Elapsed 1m 23s (remain 4m 34s) Loss: 0.0643(0.1029) Grad: 0.1085  LR: 0.00001989  \n",
      "Epoch: [1][223/953] Elapsed 1m 24s (remain 4m 34s) Loss: 0.0699(0.1027) Grad: 0.0985  LR: 0.00001989  \n",
      "Epoch: [1][224/953] Elapsed 1m 24s (remain 4m 33s) Loss: 0.0558(0.1025) Grad: 0.1887  LR: 0.00001989  \n",
      "Epoch: [1][225/953] Elapsed 1m 25s (remain 4m 33s) Loss: 0.0533(0.1023) Grad: 0.1779  LR: 0.00001989  \n",
      "Epoch: [1][226/953] Elapsed 1m 25s (remain 4m 33s) Loss: 0.0662(0.1021) Grad: 0.1007  LR: 0.00001989  \n",
      "Epoch: [1][227/953] Elapsed 1m 25s (remain 4m 32s) Loss: 0.0653(0.1020) Grad: 0.1228  LR: 0.00001989  \n",
      "Epoch: [1][228/953] Elapsed 1m 26s (remain 4m 32s) Loss: 0.0766(0.1019) Grad: 0.1086  LR: 0.00001989  \n",
      "Epoch: [1][229/953] Elapsed 1m 26s (remain 4m 32s) Loss: 0.0803(0.1018) Grad: 0.1058  LR: 0.00001989  \n",
      "Epoch: [1][230/953] Elapsed 1m 26s (remain 4m 31s) Loss: 0.0711(0.1016) Grad: 0.1040  LR: 0.00001988  \n",
      "Epoch: [1][231/953] Elapsed 1m 27s (remain 4m 31s) Loss: 0.0876(0.1016) Grad: 0.1252  LR: 0.00001988  \n",
      "Epoch: [1][232/953] Elapsed 1m 27s (remain 4m 30s) Loss: 0.0780(0.1015) Grad: 0.0807  LR: 0.00001988  \n",
      "Epoch: [1][233/953] Elapsed 1m 28s (remain 4m 30s) Loss: 0.0579(0.1013) Grad: 0.1720  LR: 0.00001988  \n",
      "Epoch: [1][234/953] Elapsed 1m 28s (remain 4m 30s) Loss: 0.0734(0.1012) Grad: 0.1050  LR: 0.00001988  \n",
      "Epoch: [1][235/953] Elapsed 1m 28s (remain 4m 29s) Loss: 0.0705(0.1010) Grad: 0.0992  LR: 0.00001988  \n",
      "Epoch: [1][236/953] Elapsed 1m 29s (remain 4m 29s) Loss: 0.0863(0.1010) Grad: 0.1139  LR: 0.00001988  \n",
      "Epoch: [1][237/953] Elapsed 1m 29s (remain 4m 29s) Loss: 0.0590(0.1008) Grad: 0.1463  LR: 0.00001988  \n",
      "Epoch: [1][238/953] Elapsed 1m 29s (remain 4m 28s) Loss: 0.0749(0.1007) Grad: 0.0885  LR: 0.00001988  \n",
      "Epoch: [1][239/953] Elapsed 1m 30s (remain 4m 28s) Loss: 0.0606(0.1005) Grad: 0.1439  LR: 0.00001988  \n",
      "Epoch: [1][240/953] Elapsed 1m 30s (remain 4m 27s) Loss: 0.0638(0.1004) Grad: 0.0995  LR: 0.00001987  \n",
      "Epoch: [1][241/953] Elapsed 1m 31s (remain 4m 27s) Loss: 0.1238(0.1005) Grad: 0.2835  LR: 0.00001987  \n",
      "Epoch: [1][242/953] Elapsed 1m 31s (remain 4m 27s) Loss: 0.0702(0.1004) Grad: 0.0892  LR: 0.00001987  \n",
      "Epoch: [1][243/953] Elapsed 1m 31s (remain 4m 26s) Loss: 0.0748(0.1002) Grad: 0.1323  LR: 0.00001987  \n",
      "Epoch: [1][244/953] Elapsed 1m 32s (remain 4m 26s) Loss: 0.0673(0.1001) Grad: 0.1329  LR: 0.00001987  \n",
      "Epoch: [1][245/953] Elapsed 1m 32s (remain 4m 26s) Loss: 0.0727(0.1000) Grad: 0.1128  LR: 0.00001987  \n",
      "Epoch: [1][246/953] Elapsed 1m 32s (remain 4m 25s) Loss: 0.0662(0.0999) Grad: 0.1376  LR: 0.00001987  \n",
      "Epoch: [1][247/953] Elapsed 1m 33s (remain 4m 25s) Loss: 0.0815(0.0998) Grad: 0.1078  LR: 0.00001987  \n",
      "Epoch: [1][248/953] Elapsed 1m 33s (remain 4m 25s) Loss: 0.0533(0.0996) Grad: 0.1671  LR: 0.00001987  \n",
      "Epoch: [1][249/953] Elapsed 1m 34s (remain 4m 24s) Loss: 0.0869(0.0996) Grad: 0.1054  LR: 0.00001986  \n",
      "Epoch: [1][250/953] Elapsed 1m 34s (remain 4m 24s) Loss: 0.0650(0.0994) Grad: 0.1156  LR: 0.00001986  \n",
      "Epoch: [1][251/953] Elapsed 1m 34s (remain 4m 23s) Loss: 0.0646(0.0993) Grad: 0.1345  LR: 0.00001986  \n",
      "Epoch: [1][252/953] Elapsed 1m 35s (remain 4m 23s) Loss: 0.0686(0.0992) Grad: 0.1412  LR: 0.00001986  \n",
      "Epoch: [1][253/953] Elapsed 1m 35s (remain 4m 23s) Loss: 0.0721(0.0991) Grad: 0.1053  LR: 0.00001986  \n",
      "Epoch: [1][254/953] Elapsed 1m 36s (remain 4m 22s) Loss: 0.0900(0.0990) Grad: 0.1124  LR: 0.00001986  \n",
      "Epoch: [1][255/953] Elapsed 1m 36s (remain 4m 22s) Loss: 0.0678(0.0989) Grad: 0.0853  LR: 0.00001986  \n",
      "Epoch: [1][256/953] Elapsed 1m 36s (remain 4m 22s) Loss: 0.0523(0.0987) Grad: 0.1618  LR: 0.00001986  \n",
      "Epoch: [1][257/953] Elapsed 1m 37s (remain 4m 21s) Loss: 0.0582(0.0986) Grad: 0.1364  LR: 0.00001986  \n",
      "Epoch: [1][258/953] Elapsed 1m 37s (remain 4m 21s) Loss: 0.0717(0.0985) Grad: 0.1204  LR: 0.00001985  \n",
      "Epoch: [1][259/953] Elapsed 1m 37s (remain 4m 20s) Loss: 0.0543(0.0983) Grad: 0.1422  LR: 0.00001985  \n",
      "Epoch: [1][260/953] Elapsed 1m 38s (remain 4m 20s) Loss: 0.0708(0.0982) Grad: 0.1031  LR: 0.00001985  \n",
      "Epoch: [1][261/953] Elapsed 1m 38s (remain 4m 20s) Loss: 0.0832(0.0981) Grad: 0.1170  LR: 0.00001985  \n",
      "Epoch: [1][262/953] Elapsed 1m 39s (remain 4m 19s) Loss: 0.0623(0.0980) Grad: 0.1560  LR: 0.00001985  \n",
      "Epoch: [1][263/953] Elapsed 1m 39s (remain 4m 19s) Loss: 0.0912(0.0980) Grad: 0.1747  LR: 0.00001985  \n",
      "Epoch: [1][264/953] Elapsed 1m 39s (remain 4m 19s) Loss: 0.0552(0.0978) Grad: 0.1604  LR: 0.00001985  \n",
      "Epoch: [1][265/953] Elapsed 1m 40s (remain 4m 18s) Loss: 0.0948(0.0978) Grad: 0.1568  LR: 0.00001985  \n",
      "Epoch: [1][266/953] Elapsed 1m 40s (remain 4m 18s) Loss: 0.0590(0.0976) Grad: 0.1128  LR: 0.00001985  \n",
      "Epoch: [1][267/953] Elapsed 1m 40s (remain 4m 18s) Loss: 0.0907(0.0976) Grad: 0.1269  LR: 0.00001984  \n",
      "Epoch: [1][268/953] Elapsed 1m 41s (remain 4m 17s) Loss: 0.0713(0.0975) Grad: 0.1041  LR: 0.00001984  \n",
      "Epoch: [1][269/953] Elapsed 1m 41s (remain 4m 17s) Loss: 0.0481(0.0973) Grad: 0.1866  LR: 0.00001984  \n",
      "Epoch: [1][270/953] Elapsed 1m 42s (remain 4m 16s) Loss: 0.0642(0.0972) Grad: 0.1242  LR: 0.00001984  \n",
      "Epoch: [1][271/953] Elapsed 1m 42s (remain 4m 16s) Loss: 0.0512(0.0970) Grad: 0.1547  LR: 0.00001984  \n",
      "Epoch: [1][272/953] Elapsed 1m 42s (remain 4m 16s) Loss: 0.0478(0.0969) Grad: 0.1595  LR: 0.00001984  \n",
      "Epoch: [1][273/953] Elapsed 1m 43s (remain 4m 15s) Loss: 0.0978(0.0969) Grad: 0.1714  LR: 0.00001984  \n",
      "Epoch: [1][274/953] Elapsed 1m 43s (remain 4m 15s) Loss: 0.0482(0.0967) Grad: 0.1732  LR: 0.00001984  \n",
      "Epoch: [1][275/953] Elapsed 1m 43s (remain 4m 15s) Loss: 0.0474(0.0965) Grad: 0.1680  LR: 0.00001983  \n",
      "Epoch: [1][276/953] Elapsed 1m 44s (remain 4m 14s) Loss: 0.0672(0.0964) Grad: 0.1035  LR: 0.00001983  \n",
      "Epoch: [1][277/953] Elapsed 1m 44s (remain 4m 14s) Loss: 0.0550(0.0963) Grad: 0.1108  LR: 0.00001983  \n",
      "Epoch: [1][278/953] Elapsed 1m 45s (remain 4m 13s) Loss: 0.0770(0.0962) Grad: 0.1015  LR: 0.00001983  \n",
      "Epoch: [1][279/953] Elapsed 1m 45s (remain 4m 13s) Loss: 0.0457(0.0960) Grad: 0.2024  LR: 0.00001983  \n",
      "Epoch: [1][280/953] Elapsed 1m 45s (remain 4m 13s) Loss: 0.0407(0.0958) Grad: 0.2024  LR: 0.00001983  \n",
      "Epoch: [1][281/953] Elapsed 1m 46s (remain 4m 12s) Loss: 0.0726(0.0957) Grad: 0.1090  LR: 0.00001983  \n",
      "Epoch: [1][282/953] Elapsed 1m 46s (remain 4m 12s) Loss: 0.0699(0.0956) Grad: 0.1200  LR: 0.00001983  \n",
      "Epoch: [1][283/953] Elapsed 1m 47s (remain 4m 12s) Loss: 0.1108(0.0957) Grad: 0.2317  LR: 0.00001983  \n",
      "Epoch: [1][284/953] Elapsed 1m 47s (remain 4m 11s) Loss: 0.0899(0.0957) Grad: 0.1315  LR: 0.00001982  \n",
      "Epoch: [1][285/953] Elapsed 1m 47s (remain 4m 11s) Loss: 0.0748(0.0956) Grad: 0.0964  LR: 0.00001982  \n",
      "Epoch: [1][286/953] Elapsed 1m 48s (remain 4m 10s) Loss: 0.0791(0.0955) Grad: 0.1306  LR: 0.00001982  \n",
      "Epoch: [1][287/953] Elapsed 1m 48s (remain 4m 10s) Loss: 0.0713(0.0955) Grad: 0.0905  LR: 0.00001982  \n",
      "Epoch: [1][288/953] Elapsed 1m 48s (remain 4m 10s) Loss: 0.0631(0.0953) Grad: 0.0850  LR: 0.00001982  \n",
      "Epoch: [1][289/953] Elapsed 1m 49s (remain 4m 9s) Loss: 0.0468(0.0952) Grad: 0.1860  LR: 0.00001982  \n",
      "Epoch: [1][290/953] Elapsed 1m 49s (remain 4m 9s) Loss: 0.0536(0.0950) Grad: 0.1130  LR: 0.00001982  \n",
      "Epoch: [1][291/953] Elapsed 1m 50s (remain 4m 9s) Loss: 0.0662(0.0949) Grad: 0.0794  LR: 0.00001982  \n",
      "Epoch: [1][292/953] Elapsed 1m 50s (remain 4m 8s) Loss: 0.0484(0.0948) Grad: 0.1036  LR: 0.00001981  \n",
      "Epoch: [1][293/953] Elapsed 1m 50s (remain 4m 8s) Loss: 0.0751(0.0947) Grad: 0.0861  LR: 0.00001981  \n",
      "Epoch: [1][294/953] Elapsed 1m 51s (remain 4m 8s) Loss: 0.0551(0.0946) Grad: 0.0957  LR: 0.00001981  \n",
      "Epoch: [1][295/953] Elapsed 1m 51s (remain 4m 7s) Loss: 0.0828(0.0945) Grad: 0.1080  LR: 0.00001981  \n",
      "Epoch: [1][296/953] Elapsed 1m 51s (remain 4m 7s) Loss: 0.1180(0.0946) Grad: 0.2792  LR: 0.00001981  \n",
      "Epoch: [1][297/953] Elapsed 1m 52s (remain 4m 6s) Loss: 0.0878(0.0946) Grad: 0.1103  LR: 0.00001981  \n",
      "Epoch: [1][298/953] Elapsed 1m 52s (remain 4m 6s) Loss: 0.0706(0.0945) Grad: 0.1037  LR: 0.00001981  \n",
      "Epoch: [1][299/953] Elapsed 1m 53s (remain 4m 6s) Loss: 0.0602(0.0944) Grad: 0.1368  LR: 0.00001981  \n",
      "Epoch: [1][300/953] Elapsed 1m 53s (remain 4m 5s) Loss: 0.0433(0.0942) Grad: 0.1427  LR: 0.00001980  \n",
      "Epoch: [1][301/953] Elapsed 1m 53s (remain 4m 5s) Loss: 0.0697(0.0941) Grad: 0.0914  LR: 0.00001980  \n",
      "Epoch: [1][302/953] Elapsed 1m 54s (remain 4m 5s) Loss: 0.0566(0.0940) Grad: 0.1076  LR: 0.00001980  \n",
      "Epoch: [1][303/953] Elapsed 1m 54s (remain 4m 4s) Loss: 0.0608(0.0939) Grad: 0.0975  LR: 0.00001980  \n",
      "Epoch: [1][304/953] Elapsed 1m 55s (remain 4m 4s) Loss: 0.0678(0.0938) Grad: 0.1094  LR: 0.00001980  \n",
      "Epoch: [1][305/953] Elapsed 1m 55s (remain 4m 4s) Loss: 0.0804(0.0938) Grad: 0.0924  LR: 0.00001980  \n",
      "Epoch: [1][306/953] Elapsed 1m 55s (remain 4m 3s) Loss: 0.0390(0.0936) Grad: 0.1861  LR: 0.00001980  \n",
      "Epoch: [1][307/953] Elapsed 1m 56s (remain 4m 3s) Loss: 0.0714(0.0935) Grad: 0.0896  LR: 0.00001979  \n",
      "Epoch: [1][308/953] Elapsed 1m 56s (remain 4m 2s) Loss: 0.0562(0.0934) Grad: 0.1101  LR: 0.00001979  \n",
      "Epoch: [1][309/953] Elapsed 1m 56s (remain 4m 2s) Loss: 0.0564(0.0933) Grad: 0.1128  LR: 0.00001979  \n",
      "Epoch: [1][310/953] Elapsed 1m 57s (remain 4m 2s) Loss: 0.0925(0.0933) Grad: 0.1886  LR: 0.00001979  \n",
      "Epoch: [1][311/953] Elapsed 1m 57s (remain 4m 1s) Loss: 0.0381(0.0931) Grad: 0.2380  LR: 0.00001979  \n",
      "Epoch: [1][312/953] Elapsed 1m 58s (remain 4m 1s) Loss: 0.0561(0.0930) Grad: 0.1335  LR: 0.00001979  \n",
      "Epoch: [1][313/953] Elapsed 1m 58s (remain 4m 1s) Loss: 0.0476(0.0928) Grad: 0.1601  LR: 0.00001979  \n",
      "Epoch: [1][314/953] Elapsed 1m 58s (remain 4m 0s) Loss: 0.0794(0.0928) Grad: 0.1217  LR: 0.00001979  \n",
      "Epoch: [1][315/953] Elapsed 1m 59s (remain 4m 0s) Loss: 0.0341(0.0926) Grad: 0.2247  LR: 0.00001978  \n",
      "Epoch: [1][316/953] Elapsed 1m 59s (remain 4m 0s) Loss: 0.0470(0.0925) Grad: 0.1625  LR: 0.00001978  \n",
      "Epoch: [1][317/953] Elapsed 2m 0s (remain 3m 59s) Loss: 0.0926(0.0925) Grad: 0.1814  LR: 0.00001978  \n",
      "Epoch: [1][318/953] Elapsed 2m 0s (remain 3m 59s) Loss: 0.0631(0.0924) Grad: 0.0992  LR: 0.00001978  \n",
      "Epoch: [1][319/953] Elapsed 2m 0s (remain 3m 58s) Loss: 0.0638(0.0923) Grad: 0.0861  LR: 0.00001978  \n",
      "Epoch: [1][320/953] Elapsed 2m 1s (remain 3m 58s) Loss: 0.0683(0.0922) Grad: 0.0838  LR: 0.00001978  \n",
      "Epoch: [1][321/953] Elapsed 2m 1s (remain 3m 58s) Loss: 0.0388(0.0921) Grad: 0.2173  LR: 0.00001978  \n",
      "Epoch: [1][322/953] Elapsed 2m 1s (remain 3m 57s) Loss: 0.0734(0.0920) Grad: 0.1140  LR: 0.00001977  \n",
      "Epoch: [1][323/953] Elapsed 2m 2s (remain 3m 57s) Loss: 0.0433(0.0918) Grad: 0.1567  LR: 0.00001977  \n",
      "Epoch: [1][324/953] Elapsed 2m 2s (remain 3m 57s) Loss: 0.0651(0.0918) Grad: 0.0950  LR: 0.00001977  \n",
      "Epoch: [1][325/953] Elapsed 2m 3s (remain 3m 56s) Loss: 0.0822(0.0917) Grad: 0.1494  LR: 0.00001977  \n",
      "Epoch: [1][326/953] Elapsed 2m 3s (remain 3m 56s) Loss: 0.0861(0.0917) Grad: 0.1515  LR: 0.00001977  \n",
      "Epoch: [1][327/953] Elapsed 2m 3s (remain 3m 56s) Loss: 0.0470(0.0916) Grad: 0.1434  LR: 0.00001977  \n",
      "Epoch: [1][328/953] Elapsed 2m 4s (remain 3m 55s) Loss: 0.0776(0.0915) Grad: 0.1206  LR: 0.00001977  \n",
      "Epoch: [1][329/953] Elapsed 2m 4s (remain 3m 55s) Loss: 0.1257(0.0916) Grad: 0.3900  LR: 0.00001976  \n",
      "Epoch: [1][330/953] Elapsed 2m 5s (remain 3m 54s) Loss: 0.0544(0.0915) Grad: 0.1314  LR: 0.00001976  \n",
      "Epoch: [1][331/953] Elapsed 2m 5s (remain 3m 54s) Loss: 0.0833(0.0915) Grad: 0.1096  LR: 0.00001976  \n",
      "Epoch: [1][332/953] Elapsed 2m 5s (remain 3m 54s) Loss: 0.0360(0.0913) Grad: 0.1553  LR: 0.00001976  \n",
      "Epoch: [1][333/953] Elapsed 2m 6s (remain 3m 53s) Loss: 0.0656(0.0913) Grad: 0.0945  LR: 0.00001976  \n",
      "Epoch: [1][334/953] Elapsed 2m 6s (remain 3m 53s) Loss: 0.0401(0.0911) Grad: 0.1455  LR: 0.00001976  \n",
      "Epoch: [1][335/953] Elapsed 2m 6s (remain 3m 53s) Loss: 0.0713(0.0910) Grad: 0.0924  LR: 0.00001976  \n",
      "Epoch: [1][336/953] Elapsed 2m 7s (remain 3m 52s) Loss: 0.0585(0.0910) Grad: 0.0983  LR: 0.00001975  \n",
      "Epoch: [1][337/953] Elapsed 2m 7s (remain 3m 52s) Loss: 0.0505(0.0908) Grad: 0.1420  LR: 0.00001975  \n",
      "Epoch: [1][338/953] Elapsed 2m 8s (remain 3m 52s) Loss: 0.0629(0.0907) Grad: 0.0803  LR: 0.00001975  \n",
      "Epoch: [1][339/953] Elapsed 2m 8s (remain 3m 51s) Loss: 0.0592(0.0907) Grad: 0.0925  LR: 0.00001975  \n",
      "Epoch: [1][340/953] Elapsed 2m 8s (remain 3m 51s) Loss: 0.0632(0.0906) Grad: 0.0914  LR: 0.00001975  \n",
      "Epoch: [1][341/953] Elapsed 2m 9s (remain 3m 50s) Loss: 0.0596(0.0905) Grad: 0.0870  LR: 0.00001975  \n",
      "Epoch: [1][342/953] Elapsed 2m 9s (remain 3m 50s) Loss: 0.0420(0.0903) Grad: 0.1851  LR: 0.00001975  \n",
      "Epoch: [1][343/953] Elapsed 2m 10s (remain 3m 50s) Loss: 0.0530(0.0902) Grad: 0.1395  LR: 0.00001974  \n",
      "Epoch: [1][344/953] Elapsed 2m 10s (remain 3m 49s) Loss: 0.1117(0.0903) Grad: 0.2623  LR: 0.00001974  \n",
      "Epoch: [1][345/953] Elapsed 2m 10s (remain 3m 49s) Loss: 0.1047(0.0903) Grad: 0.2525  LR: 0.00001974  \n",
      "Epoch: [1][346/953] Elapsed 2m 11s (remain 3m 49s) Loss: 0.0627(0.0903) Grad: 0.0852  LR: 0.00001974  \n",
      "Epoch: [1][347/953] Elapsed 2m 11s (remain 3m 48s) Loss: 0.0783(0.0902) Grad: 0.1254  LR: 0.00001974  \n",
      "Epoch: [1][348/953] Elapsed 2m 11s (remain 3m 48s) Loss: 0.0850(0.0902) Grad: 0.1495  LR: 0.00001974  \n",
      "Epoch: [1][349/953] Elapsed 2m 12s (remain 3m 48s) Loss: 0.0869(0.0902) Grad: 0.1476  LR: 0.00001974  \n",
      "Epoch: [1][350/953] Elapsed 2m 12s (remain 3m 47s) Loss: 0.0556(0.0901) Grad: 0.1121  LR: 0.00001973  \n",
      "Epoch: [1][351/953] Elapsed 2m 13s (remain 3m 47s) Loss: 0.0489(0.0900) Grad: 0.1509  LR: 0.00001973  \n",
      "Epoch: [1][352/953] Elapsed 2m 13s (remain 3m 46s) Loss: 0.0685(0.0899) Grad: 0.0985  LR: 0.00001973  \n",
      "Epoch: [1][353/953] Elapsed 2m 13s (remain 3m 46s) Loss: 0.0549(0.0898) Grad: 0.1341  LR: 0.00001973  \n",
      "Epoch: [1][354/953] Elapsed 2m 14s (remain 3m 46s) Loss: 0.0761(0.0898) Grad: 0.0918  LR: 0.00001973  \n",
      "Epoch: [1][355/953] Elapsed 2m 14s (remain 3m 45s) Loss: 0.0791(0.0898) Grad: 0.1167  LR: 0.00001973  \n",
      "Epoch: [1][356/953] Elapsed 2m 15s (remain 3m 45s) Loss: 0.0556(0.0897) Grad: 0.1506  LR: 0.00001972  \n",
      "Epoch: [1][357/953] Elapsed 2m 15s (remain 3m 45s) Loss: 0.0701(0.0896) Grad: 0.1131  LR: 0.00001972  \n",
      "Epoch: [1][358/953] Elapsed 2m 15s (remain 3m 44s) Loss: 0.0476(0.0895) Grad: 0.1528  LR: 0.00001972  \n",
      "Epoch: [1][359/953] Elapsed 2m 16s (remain 3m 44s) Loss: 0.1005(0.0895) Grad: 0.2056  LR: 0.00001972  \n",
      "Epoch: [1][360/953] Elapsed 2m 16s (remain 3m 43s) Loss: 0.0539(0.0894) Grad: 0.1049  LR: 0.00001972  \n",
      "Epoch: [1][361/953] Elapsed 2m 16s (remain 3m 43s) Loss: 0.1070(0.0895) Grad: 0.1944  LR: 0.00001972  \n",
      "Epoch: [1][362/953] Elapsed 2m 17s (remain 3m 43s) Loss: 0.0604(0.0894) Grad: 0.1785  LR: 0.00001972  \n",
      "Epoch: [1][363/953] Elapsed 2m 17s (remain 3m 42s) Loss: 0.0564(0.0893) Grad: 0.1100  LR: 0.00001971  \n",
      "Epoch: [1][364/953] Elapsed 2m 18s (remain 3m 42s) Loss: 0.0488(0.0892) Grad: 0.1696  LR: 0.00001971  \n",
      "Epoch: [1][365/953] Elapsed 2m 18s (remain 3m 42s) Loss: 0.0650(0.0891) Grad: 0.0992  LR: 0.00001971  \n",
      "Epoch: [1][366/953] Elapsed 2m 18s (remain 3m 41s) Loss: 0.0687(0.0891) Grad: 0.0968  LR: 0.00001971  \n",
      "Epoch: [1][367/953] Elapsed 2m 19s (remain 3m 41s) Loss: 0.1029(0.0891) Grad: 0.1764  LR: 0.00001971  \n",
      "Epoch: [1][368/953] Elapsed 2m 19s (remain 3m 40s) Loss: 0.1113(0.0892) Grad: 0.2788  LR: 0.00001971  \n",
      "Epoch: [1][369/953] Elapsed 2m 19s (remain 3m 40s) Loss: 0.0468(0.0890) Grad: 0.1605  LR: 0.00001970  \n",
      "Epoch: [1][370/953] Elapsed 2m 20s (remain 3m 40s) Loss: 0.0410(0.0889) Grad: 0.1693  LR: 0.00001970  \n",
      "Epoch: [1][371/953] Elapsed 2m 20s (remain 3m 39s) Loss: 0.0460(0.0888) Grad: 0.1878  LR: 0.00001970  \n",
      "Epoch: [1][372/953] Elapsed 2m 21s (remain 3m 39s) Loss: 0.0541(0.0887) Grad: 0.1101  LR: 0.00001970  \n",
      "Epoch: [1][373/953] Elapsed 2m 21s (remain 3m 39s) Loss: 0.0688(0.0887) Grad: 0.1017  LR: 0.00001970  \n",
      "Epoch: [1][374/953] Elapsed 2m 21s (remain 3m 38s) Loss: 0.0857(0.0887) Grad: 0.1157  LR: 0.00001970  \n",
      "Epoch: [1][375/953] Elapsed 2m 22s (remain 3m 38s) Loss: 0.0611(0.0886) Grad: 0.1144  LR: 0.00001969  \n",
      "Epoch: [1][376/953] Elapsed 2m 22s (remain 3m 37s) Loss: 0.0480(0.0885) Grad: 0.1710  LR: 0.00001969  \n",
      "Epoch: [1][377/953] Elapsed 2m 23s (remain 3m 37s) Loss: 0.1169(0.0885) Grad: 0.2833  LR: 0.00001969  \n",
      "Epoch: [1][378/953] Elapsed 2m 23s (remain 3m 37s) Loss: 0.0542(0.0885) Grad: 0.1186  LR: 0.00001969  \n",
      "Epoch: [1][379/953] Elapsed 2m 23s (remain 3m 36s) Loss: 0.0643(0.0884) Grad: 0.1029  LR: 0.00001969  \n",
      "Epoch: [1][380/953] Elapsed 2m 24s (remain 3m 36s) Loss: 0.0486(0.0883) Grad: 0.1126  LR: 0.00001969  \n",
      "Epoch: [1][381/953] Elapsed 2m 24s (remain 3m 36s) Loss: 0.0489(0.0882) Grad: 0.1606  LR: 0.00001968  \n",
      "Epoch: [1][382/953] Elapsed 2m 24s (remain 3m 35s) Loss: 0.0522(0.0881) Grad: 0.1544  LR: 0.00001968  \n",
      "Epoch: [1][383/953] Elapsed 2m 25s (remain 3m 35s) Loss: 0.1116(0.0882) Grad: 0.2299  LR: 0.00001968  \n",
      "Epoch: [1][384/953] Elapsed 2m 25s (remain 3m 34s) Loss: 0.0503(0.0881) Grad: 0.1465  LR: 0.00001968  \n",
      "Epoch: [1][385/953] Elapsed 2m 26s (remain 3m 34s) Loss: 0.0999(0.0881) Grad: 0.1620  LR: 0.00001968  \n",
      "Epoch: [1][386/953] Elapsed 2m 26s (remain 3m 34s) Loss: 0.0423(0.0880) Grad: 0.1828  LR: 0.00001968  \n",
      "Epoch: [1][387/953] Elapsed 2m 26s (remain 3m 33s) Loss: 0.0611(0.0879) Grad: 0.1251  LR: 0.00001967  \n",
      "Epoch: [1][388/953] Elapsed 2m 27s (remain 3m 33s) Loss: 0.0757(0.0879) Grad: 0.0961  LR: 0.00001967  \n",
      "Epoch: [1][389/953] Elapsed 2m 27s (remain 3m 33s) Loss: 0.0685(0.0878) Grad: 0.1144  LR: 0.00001967  \n",
      "Epoch: [1][390/953] Elapsed 2m 27s (remain 3m 32s) Loss: 0.0637(0.0878) Grad: 0.1022  LR: 0.00001967  \n",
      "Epoch: [1][391/953] Elapsed 2m 28s (remain 3m 32s) Loss: 0.0687(0.0877) Grad: 0.0862  LR: 0.00001967  \n",
      "Epoch: [1][392/953] Elapsed 2m 28s (remain 3m 31s) Loss: 0.0898(0.0877) Grad: 0.1387  LR: 0.00001967  \n",
      "Epoch: [1][393/953] Elapsed 2m 29s (remain 3m 31s) Loss: 0.0677(0.0877) Grad: 0.0961  LR: 0.00001966  \n",
      "Epoch: [1][394/953] Elapsed 2m 29s (remain 3m 31s) Loss: 0.0811(0.0876) Grad: 0.1285  LR: 0.00001966  \n",
      "Epoch: [1][395/953] Elapsed 2m 29s (remain 3m 30s) Loss: 0.0853(0.0876) Grad: 0.1427  LR: 0.00001966  \n",
      "Epoch: [1][396/953] Elapsed 2m 30s (remain 3m 30s) Loss: 0.1018(0.0877) Grad: 0.2184  LR: 0.00001966  \n",
      "Epoch: [1][397/953] Elapsed 2m 30s (remain 3m 30s) Loss: 0.0868(0.0877) Grad: 0.1572  LR: 0.00001966  \n",
      "Epoch: [1][398/953] Elapsed 2m 31s (remain 3m 29s) Loss: 0.1303(0.0878) Grad: 0.3303  LR: 0.00001966  \n",
      "Epoch: [1][399/953] Elapsed 2m 31s (remain 3m 29s) Loss: 0.0925(0.0878) Grad: 0.1371  LR: 0.00001965  \n",
      "Epoch: [1][400/953] Elapsed 2m 31s (remain 3m 28s) Loss: 0.0733(0.0878) Grad: 0.0928  LR: 0.00001965  \n",
      "Epoch: [1][401/953] Elapsed 2m 32s (remain 3m 28s) Loss: 0.0527(0.0877) Grad: 0.1427  LR: 0.00001965  \n",
      "Epoch: [1][402/953] Elapsed 2m 32s (remain 3m 28s) Loss: 0.0567(0.0876) Grad: 0.1556  LR: 0.00001965  \n",
      "Epoch: [1][403/953] Elapsed 2m 32s (remain 3m 27s) Loss: 0.0569(0.0875) Grad: 0.1811  LR: 0.00001965  \n",
      "Epoch: [1][404/953] Elapsed 2m 33s (remain 3m 27s) Loss: 0.0773(0.0875) Grad: 0.1350  LR: 0.00001965  \n",
      "Epoch: [1][405/953] Elapsed 2m 33s (remain 3m 27s) Loss: 0.0525(0.0874) Grad: 0.1979  LR: 0.00001964  \n",
      "Epoch: [1][406/953] Elapsed 2m 34s (remain 3m 26s) Loss: 0.0653(0.0873) Grad: 0.1748  LR: 0.00001964  \n",
      "Epoch: [1][407/953] Elapsed 2m 34s (remain 3m 26s) Loss: 0.0539(0.0873) Grad: 0.1807  LR: 0.00001964  \n",
      "Epoch: [1][408/953] Elapsed 2m 34s (remain 3m 26s) Loss: 0.0765(0.0872) Grad: 0.1066  LR: 0.00001964  \n",
      "Epoch: [1][409/953] Elapsed 2m 35s (remain 3m 25s) Loss: 0.0487(0.0871) Grad: 0.1654  LR: 0.00001964  \n",
      "Epoch: [1][410/953] Elapsed 2m 35s (remain 3m 25s) Loss: 0.0679(0.0871) Grad: 0.1136  LR: 0.00001964  \n",
      "Epoch: [1][411/953] Elapsed 2m 36s (remain 3m 24s) Loss: 0.0547(0.0870) Grad: 0.1473  LR: 0.00001963  \n",
      "Epoch: [1][412/953] Elapsed 2m 36s (remain 3m 24s) Loss: 0.0731(0.0870) Grad: 0.0794  LR: 0.00001963  \n",
      "Epoch: [1][413/953] Elapsed 2m 36s (remain 3m 24s) Loss: 0.0660(0.0869) Grad: 0.0898  LR: 0.00001963  \n",
      "Epoch: [1][414/953] Elapsed 2m 37s (remain 3m 23s) Loss: 0.0665(0.0869) Grad: 0.0839  LR: 0.00001963  \n",
      "Epoch: [1][415/953] Elapsed 2m 37s (remain 3m 23s) Loss: 0.0650(0.0868) Grad: 0.1225  LR: 0.00001963  \n",
      "Epoch: [1][416/953] Elapsed 2m 37s (remain 3m 23s) Loss: 0.0585(0.0868) Grad: 0.1385  LR: 0.00001962  \n",
      "Epoch: [1][417/953] Elapsed 2m 38s (remain 3m 22s) Loss: 0.0468(0.0867) Grad: 0.1400  LR: 0.00001962  \n",
      "Epoch: [1][418/953] Elapsed 2m 38s (remain 3m 22s) Loss: 0.0600(0.0866) Grad: 0.1337  LR: 0.00001962  \n",
      "Epoch: [1][419/953] Elapsed 2m 39s (remain 3m 21s) Loss: 0.0635(0.0865) Grad: 0.0988  LR: 0.00001962  \n",
      "Epoch: [1][420/953] Elapsed 2m 39s (remain 3m 21s) Loss: 0.0656(0.0865) Grad: 0.1214  LR: 0.00001962  \n",
      "Epoch: [1][421/953] Elapsed 2m 39s (remain 3m 21s) Loss: 0.0797(0.0865) Grad: 0.1156  LR: 0.00001962  \n",
      "Epoch: [1][422/953] Elapsed 2m 40s (remain 3m 20s) Loss: 0.0483(0.0864) Grad: 0.1352  LR: 0.00001961  \n",
      "Epoch: [1][423/953] Elapsed 2m 40s (remain 3m 20s) Loss: 0.0726(0.0864) Grad: 0.0988  LR: 0.00001961  \n",
      "Epoch: [1][424/953] Elapsed 2m 41s (remain 3m 20s) Loss: 0.0723(0.0863) Grad: 0.0955  LR: 0.00001961  \n",
      "Epoch: [1][425/953] Elapsed 2m 41s (remain 3m 19s) Loss: 0.0996(0.0864) Grad: 0.1852  LR: 0.00001961  \n",
      "Epoch: [1][426/953] Elapsed 2m 41s (remain 3m 19s) Loss: 0.0683(0.0863) Grad: 0.1140  LR: 0.00001961  \n",
      "Epoch: [1][427/953] Elapsed 2m 42s (remain 3m 18s) Loss: 0.0415(0.0862) Grad: 0.1388  LR: 0.00001960  \n",
      "Epoch: [1][428/953] Elapsed 2m 42s (remain 3m 18s) Loss: 0.0969(0.0862) Grad: 0.2042  LR: 0.00001960  \n",
      "Epoch: [1][429/953] Elapsed 2m 42s (remain 3m 18s) Loss: 0.0726(0.0862) Grad: 0.1238  LR: 0.00001960  \n",
      "Epoch: [1][430/953] Elapsed 2m 43s (remain 3m 17s) Loss: 0.0541(0.0861) Grad: 0.1286  LR: 0.00001960  \n",
      "Epoch: [1][431/953] Elapsed 2m 43s (remain 3m 17s) Loss: 0.1016(0.0862) Grad: 0.1730  LR: 0.00001960  \n",
      "Epoch: [1][432/953] Elapsed 2m 44s (remain 3m 17s) Loss: 0.0692(0.0861) Grad: 0.1031  LR: 0.00001960  \n",
      "Epoch: [1][433/953] Elapsed 2m 44s (remain 3m 16s) Loss: 0.0719(0.0861) Grad: 0.0852  LR: 0.00001959  \n",
      "Epoch: [1][434/953] Elapsed 2m 44s (remain 3m 16s) Loss: 0.0556(0.0860) Grad: 0.1207  LR: 0.00001959  \n",
      "Epoch: [1][435/953] Elapsed 2m 45s (remain 3m 15s) Loss: 0.0656(0.0860) Grad: 0.1070  LR: 0.00001959  \n",
      "Epoch: [1][436/953] Elapsed 2m 45s (remain 3m 15s) Loss: 0.0474(0.0859) Grad: 0.1309  LR: 0.00001959  \n",
      "Epoch: [1][437/953] Elapsed 2m 45s (remain 3m 15s) Loss: 0.0855(0.0859) Grad: 0.1407  LR: 0.00001959  \n",
      "Epoch: [1][438/953] Elapsed 2m 46s (remain 3m 14s) Loss: 0.0536(0.0858) Grad: 0.1065  LR: 0.00001958  \n",
      "Epoch: [1][439/953] Elapsed 2m 46s (remain 3m 14s) Loss: 0.0745(0.0858) Grad: 0.0888  LR: 0.00001958  \n",
      "Epoch: [1][440/953] Elapsed 2m 47s (remain 3m 14s) Loss: 0.0683(0.0857) Grad: 0.0830  LR: 0.00001958  \n",
      "Epoch: [1][441/953] Elapsed 2m 47s (remain 3m 13s) Loss: 0.0526(0.0857) Grad: 0.1227  LR: 0.00001958  \n",
      "Epoch: [1][442/953] Elapsed 2m 47s (remain 3m 13s) Loss: 0.0842(0.0857) Grad: 0.0887  LR: 0.00001958  \n",
      "Epoch: [1][443/953] Elapsed 2m 48s (remain 3m 12s) Loss: 0.0678(0.0856) Grad: 0.1068  LR: 0.00001957  \n",
      "Epoch: [1][444/953] Elapsed 2m 48s (remain 3m 12s) Loss: 0.0634(0.0856) Grad: 0.1115  LR: 0.00001957  \n",
      "Epoch: [1][445/953] Elapsed 2m 49s (remain 3m 12s) Loss: 0.0808(0.0856) Grad: 0.1415  LR: 0.00001957  \n",
      "Epoch: [1][446/953] Elapsed 2m 49s (remain 3m 11s) Loss: 0.0761(0.0855) Grad: 0.0972  LR: 0.00001957  \n",
      "Epoch: [1][447/953] Elapsed 2m 49s (remain 3m 11s) Loss: 0.0701(0.0855) Grad: 0.0918  LR: 0.00001957  \n",
      "Epoch: [1][448/953] Elapsed 2m 50s (remain 3m 11s) Loss: 0.0478(0.0854) Grad: 0.1578  LR: 0.00001957  \n",
      "Epoch: [1][449/953] Elapsed 2m 50s (remain 3m 10s) Loss: 0.0509(0.0854) Grad: 0.1534  LR: 0.00001956  \n",
      "Epoch: [1][450/953] Elapsed 2m 50s (remain 3m 10s) Loss: 0.0673(0.0853) Grad: 0.1068  LR: 0.00001956  \n",
      "Epoch: [1][451/953] Elapsed 2m 51s (remain 3m 9s) Loss: 0.0729(0.0853) Grad: 0.0974  LR: 0.00001956  \n",
      "Epoch: [1][452/953] Elapsed 2m 51s (remain 3m 9s) Loss: 0.0620(0.0852) Grad: 0.1062  LR: 0.00001956  \n",
      "Epoch: [1][453/953] Elapsed 2m 52s (remain 3m 9s) Loss: 0.0679(0.0852) Grad: 0.0897  LR: 0.00001956  \n",
      "Epoch: [1][454/953] Elapsed 2m 52s (remain 3m 8s) Loss: 0.0855(0.0852) Grad: 0.1079  LR: 0.00001955  \n",
      "Epoch: [1][455/953] Elapsed 2m 52s (remain 3m 8s) Loss: 0.0858(0.0852) Grad: 0.1536  LR: 0.00001955  \n",
      "Epoch: [1][456/953] Elapsed 2m 53s (remain 3m 8s) Loss: 0.0580(0.0851) Grad: 0.1027  LR: 0.00001955  \n",
      "Epoch: [1][457/953] Elapsed 2m 53s (remain 3m 7s) Loss: 0.0508(0.0851) Grad: 0.1743  LR: 0.00001955  \n",
      "Epoch: [1][458/953] Elapsed 2m 53s (remain 3m 7s) Loss: 0.0440(0.0850) Grad: 0.1869  LR: 0.00001955  \n",
      "Epoch: [1][459/953] Elapsed 2m 54s (remain 3m 6s) Loss: 0.0825(0.0850) Grad: 0.1138  LR: 0.00001954  \n",
      "Epoch: [1][460/953] Elapsed 2m 54s (remain 3m 6s) Loss: 0.0712(0.0849) Grad: 0.0973  LR: 0.00001954  \n",
      "Epoch: [1][461/953] Elapsed 2m 55s (remain 3m 6s) Loss: 0.0683(0.0849) Grad: 0.1012  LR: 0.00001954  \n",
      "Epoch: [1][462/953] Elapsed 2m 55s (remain 3m 5s) Loss: 0.0844(0.0849) Grad: 0.1073  LR: 0.00001954  \n",
      "Epoch: [1][463/953] Elapsed 2m 55s (remain 3m 5s) Loss: 0.0634(0.0849) Grad: 0.1016  LR: 0.00001954  \n",
      "Epoch: [1][464/953] Elapsed 2m 56s (remain 3m 5s) Loss: 0.0743(0.0848) Grad: 0.0971  LR: 0.00001953  \n",
      "Epoch: [1][465/953] Elapsed 2m 56s (remain 3m 4s) Loss: 0.0571(0.0848) Grad: 0.1114  LR: 0.00001953  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1620\\3529880504.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrn_fold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0m_oof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[0moof_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moof_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_oof_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mLOGGER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"========== fold: {fold} result ==========\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1620\\4055035133.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(folds, fold)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m# eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1620\\1794449400.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# choose the loss of label != -1, and calculate the mean of all none -1 loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = create_labels_for_scoring(oof_df)\n",
    "        predictions = oof_df[[i for i in range(CFG.max_len)]].values\n",
    "        char_probs = get_char_probs(oof_df['pn_history'].values, predictions, CFG.tokenizer)\n",
    "        results = get_results(char_probs, th=0.5)\n",
    "        preds = get_predictions(results)\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "        \n",
    "    if CFG.wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
