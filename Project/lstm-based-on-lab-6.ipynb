{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# others\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import time\n",
    "\n",
    "# dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import Flowers102\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# read file \n",
    "import pandas as pd\n",
    "\n",
    "# label\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use TweetEval emotion recognition dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '../../Data/tweeteval/datasets/emotion/'\n",
    "# mapping_file = os.path.join(root, 'mapping.txt')\n",
    "# test_labels_file = os.path.join(root, 'test_labels.txt')\n",
    "# test_text_file = os.path.join(root, 'test_text.txt')\n",
    "# train_labels_file = os.path.join(root, 'train_labels.txt')\n",
    "# train_text_file = os.path.join(root, 'train_text.txt')\n",
    "# val_labels_file = os.path.join(root, 'val_labels.txt')\n",
    "# val_text_file = os.path.join(root, 'val_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_pd = pd.read_csv(mapping_file, sep='\\t', header=None)\n",
    "# test_label_pd = pd.read_csv(test_labels_file, sep='\\t', header=None)\n",
    "# test_dataset = open(test_text_file).read().split('\\n')[:-1] # remove last empty line \n",
    "# train_label_pd = pd.read_csv(train_labels_file, sep='\\t', header=None)\n",
    "# train_dataset = open(train_text_file).read().split('\\n')[:-1] # remove last empty line\n",
    "# val_label_pd = pd.read_csv(val_labels_file, sep='\\t', header=None)\n",
    "# val_dataset = open(val_text_file).read().split('\\n')[:-1] # remove last empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training data\n",
    "- Given: Notes with ranges and labels\n",
    "- Transform into label + lists of tokens with [does token describe label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data/'\n",
    "features_path = os.path.join(root, 'features.csv')\n",
    "patient_notes_path = os.path.join(root, 'patient_notes.csv')\n",
    "sample_submission_path = os.path.join(root, 'sample_submission.csv')\n",
    "test_path = os.path.join(root, 'test.csv')\n",
    "train_path = os.path.join(root, 'train.csv')\n",
    "features = pd.read_csv(features_path, sep=',', header=0)\n",
    "patient_notes = pd.read_csv(patient_notes_path, sep=',', header=0)\n",
    "train_raw = pd.read_csv(train_path, sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>208</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>407</td>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>501</td>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>802</td>\n",
       "      <td>8</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>913</td>\n",
       "      <td>9</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_num  case_num feature_text\n",
       "25           112         1       Female\n",
       "34           208         2       Female\n",
       "66           407         4       Female\n",
       "70           501         5       Female\n",
       "99           700         7       Female\n",
       "110          802         8       Female\n",
       "139          913         9       Female"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unusual_numbers = features[\"feature_num\"].value_counts()[features[\"feature_num\"].value_counts() != 1]\n",
    "# unusual_numbers\n",
    "features[features[\"feature_text\"] == \"Female\"]\n",
    "# features[\"feature_num\"] == "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intro \n",
    "- `case_num`: 0~9, each num belongs their groups ... ? \n",
    "- `pn_num`: the id in patient_notes.csv which is 'pn_history', present the note of each case \n",
    "- `feature_num`: the id in features.csv which is 'feature_num', present the feature of each case \n",
    "- `location`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def df_string2list_of_ints(df_string: str):\n",
    "    df_string = df_string.strip(\"[]\")\n",
    "    if df_string == \"\":\n",
    "        return []\n",
    "    entries = re.split(\",|;\", df_string)\n",
    "    entries = [entry.strip(\" '\") for entry in entries]\n",
    "    ranges = [tuple(int(num_as_str) for num_as_str in entry.split(\" \")) for entry in entries]\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>['203 217']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>['70 91', '176 183']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>['222 258']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14295</th>\n",
       "      <td>95333_912</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>912</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14296</th>\n",
       "      <td>95333_913</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>913</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14297</th>\n",
       "      <td>95333_914</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>914</td>\n",
       "      <td>['photobia']</td>\n",
       "      <td>['274 282']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14298</th>\n",
       "      <td>95333_915</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>915</td>\n",
       "      <td>['no sick contacts']</td>\n",
       "      <td>['421 437']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14299</th>\n",
       "      <td>95333_916</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>916</td>\n",
       "      <td>['Subjective fever']</td>\n",
       "      <td>['314 330']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  case_num  pn_num  feature_num  \\\n",
       "0      00016_000         0      16            0   \n",
       "1      00016_001         0      16            1   \n",
       "2      00016_002         0      16            2   \n",
       "3      00016_003         0      16            3   \n",
       "4      00016_004         0      16            4   \n",
       "...          ...       ...     ...          ...   \n",
       "14295  95333_912         9   95333          912   \n",
       "14296  95333_913         9   95333          913   \n",
       "14297  95333_914         9   95333          914   \n",
       "14298  95333_915         9   95333          915   \n",
       "14299  95333_916         9   95333          916   \n",
       "\n",
       "                                     annotation              location  \n",
       "0              ['dad with recent heart attcak']           ['696 724']  \n",
       "1                 ['mom with \"thyroid disease']           ['668 693']  \n",
       "2                            ['chest pressure']           ['203 217']  \n",
       "3          ['intermittent episodes', 'episode']  ['70 91', '176 183']  \n",
       "4      ['felt as if he were going to pass out']           ['222 258']  \n",
       "...                                         ...                   ...  \n",
       "14295                                        []                    []  \n",
       "14296                                        []                    []  \n",
       "14297                              ['photobia']           ['274 282']  \n",
       "14298                      ['no sick contacts']           ['421 437']  \n",
       "14299                      ['Subjective fever']           ['314 330']  \n",
       "\n",
       "[14300 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                                 annotation                location  \\\n",
       "0          ['dad with recent heart attcak']            [(696, 724)]   \n",
       "1             ['mom with \"thyroid disease']            [(668, 693)]   \n",
       "2                        ['chest pressure']            [(203, 217)]   \n",
       "3      ['intermittent episodes', 'episode']  [(70, 91), (176, 183)]   \n",
       "4  ['felt as if he were going to pass out']            [(222, 258)]   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = train_raw.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "data_merged = data_merged.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "data_merged[\"location\"] = data_merged[\"location\"].apply(df_string2list_of_ints)\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history                location  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...            [(696, 724)]  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...            [(668, 693)]  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...            [(203, 217)]  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  [(70, 91), (176, 183)]  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...            [(222, 258)]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_merged[[\"feature_text\", \"pn_history\", \"location\", ]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter training data with no location\n",
    "train = train[train[\"location\"].apply(lambda row: len(row) != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset= 9901\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of dataset= {len(train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "- Use spaCy to split the notes into words.\n",
    "\n",
    "Before start using spaCy\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "# from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leonard\\.conda\\envs\\UniAILab\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "# tokenizer.encode_plus(\"hello i am Drunk\", return_offsets_mapping=True, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# def build_vocab_from_lines(lines: Iterable[str]):\n",
    "#     text_to_count_tokens = ' '.join(lines)\n",
    "#     doc = tokenizer.tokenize(text_to_count_tokens)\n",
    "#     # Get the most frequent words, filtering out stop words and punctuation.\n",
    "#     word_freq = Counter(token.text.lower() for token in doc if \\\n",
    "#                         not token.is_punct and \\\n",
    "#                             not token.is_stop and \\\n",
    "#                                 not token.is_space)\n",
    "#     return word_freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pathjoin\n",
    "import pickle\n",
    "cache_dir = \"cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1045, 2572, 7144, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (6, 7), (8, 10), (11, 16), (0, 0)]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"hello i am Drunk\", return_offsets_mapping=True, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "BERT_FP = ('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embed_matrix():\n",
    "    bert = BertModel.from_pretrained(BERT_FP)\n",
    "    bert_embeddings = list(bert.children())[0]\n",
    "    bert_word_embeddings = list(bert_embeddings.children())[0]\n",
    "    mat = bert_word_embeddings.weight.data.numpy()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_bert_embed_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache, lru_cache\n",
    "\n",
    "def embed_seq(s: Iterable[int]):\n",
    "    return np.array([onehot_word(word_id) @ embedding_matrix for word_id in s])\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def embed_word(word_id: int):\n",
    "    return onehot_word(word_id) @ embedding_matrix\n",
    "\n",
    "def onehot_word(a: int):\n",
    "    oh = np.zeros(30522, dtype=int)\n",
    "    oh[a] = 1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized patient histories loaded from cache.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_pn_histories.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_pn_histories = pickle.load(f)\n",
    "    print(\"Tokenized patient histories loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized patient histories. Tokenizing...\")\n",
    "    tokenized_pn_histories: Dict[str, List[Dict]] = {}\n",
    "    for pn_history in tqdm(train[\"pn_history\"]):\n",
    "        indexed_words = []\n",
    "        if pn_history in tokenized_pn_histories:\n",
    "            continue\n",
    "\n",
    "        tokenized = tokenizer.encode_plus(pn_history, return_offsets_mapping=True, add_special_tokens=True)\n",
    "\n",
    "        for word, offset_mapping in zip(tokenized[\"input_ids\"], tokenized[\"offset_mapping\"]):\n",
    "            embedded_token = embed_word(word)\n",
    "\n",
    "            indexed_words.append({\n",
    "                \"word_id\": word,\n",
    "                \"embedded\": embedded_token,\n",
    "                \"range\": offset_mapping\n",
    "            })\n",
    "                    \n",
    "        tokenized_pn_histories[pn_history] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_pn_histories, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure:\n",
    "tokenized_pn_histories\n",
    "hist_id -> [tokens]\n",
    "token -> ['word_id', 'embedded', 'range']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_id': 6522,\n",
       " 'embedded': array([-0.01908113, -0.04173963, -0.06673449, -0.01592105,  0.04393673,\n",
       "        -0.0045615 , -0.07181855, -0.03923002, -0.06660692, -0.05208729,\n",
       "        -0.05683259, -0.07923082, -0.07103121, -0.03811198,  0.04058741,\n",
       "        -0.07675945, -0.03249264,  0.01277649,  0.00591493, -0.02265794,\n",
       "        -0.07537558, -0.0377069 , -0.02679249, -0.02630543, -0.10752729,\n",
       "        -0.07387543, -0.03104966, -0.05502456, -0.01574389,  0.03902053,\n",
       "        -0.03393734, -0.04892192, -0.01713544, -0.01373354, -0.03406208,\n",
       "        -0.03928473, -0.01480401, -0.0745343 , -0.04668886,  0.04388529,\n",
       "         0.04298296, -0.02789518, -0.06555539, -0.01023453, -0.05270426,\n",
       "        -0.01351104, -0.01696531, -0.02604903, -0.06805495, -0.0114427 ,\n",
       "        -0.07451096, -0.08470455, -0.06320265, -0.07705162,  0.00924673,\n",
       "        -0.06076133, -0.03972978, -0.00976295,  0.04022013,  0.00853729,\n",
       "        -0.03172083, -0.07831176, -0.04030865, -0.01483213, -0.02548516,\n",
       "        -0.02037099,  0.02583888, -0.00601115,  0.0005211 , -0.0503305 ,\n",
       "         0.01535861, -0.06918605,  0.06422725, -0.04100475, -0.01852632,\n",
       "        -0.00795093, -0.01691452,  0.12692392,  0.0025903 , -0.08476964,\n",
       "        -0.05988051,  0.0401165 , -0.01006817,  0.01409894,  0.02960487,\n",
       "        -0.00199787, -0.0436109 , -0.01663729,  0.01638095,  0.00484425,\n",
       "         0.03679116, -0.02578565, -0.06917788, -0.05262199, -0.01385328,\n",
       "        -0.07637186,  0.03965394, -0.04200633, -0.12093006,  0.01642366,\n",
       "         0.05532187,  0.00137352, -0.02469023,  0.01579396, -0.03945262,\n",
       "        -0.04573373, -0.03275239, -0.05293219,  0.01127648, -0.03078214,\n",
       "        -0.05783765,  0.01816563,  0.00907583, -0.07086688, -0.01004548,\n",
       "        -0.01934193, -0.00457785, -0.05536553, -0.0459183 , -0.0672041 ,\n",
       "         0.03481179,  0.0206055 , -0.01818438, -0.00539505, -0.07162139,\n",
       "        -0.07882681, -0.02764613, -0.09358473, -0.06332196, -0.06640781,\n",
       "        -0.08489089, -0.00042765,  0.12575683, -0.05091361, -0.05330716,\n",
       "         0.01216682, -0.0151412 , -0.00678979, -0.01782138,  0.1514968 ,\n",
       "        -0.05412668, -0.08116943,  0.02427371, -0.03710703, -0.01445157,\n",
       "        -0.02355842,  0.01155926, -0.03040035, -0.01847903, -0.04440971,\n",
       "        -0.00966275,  0.00402587, -0.05210217,  0.04715672, -0.0018505 ,\n",
       "        -0.03760519, -0.0181003 , -0.06754413, -0.06897832,  0.01820193,\n",
       "        -0.04256851,  0.01031271, -0.10166597, -0.06834054, -0.08312954,\n",
       "        -0.09930322,  0.01107772, -0.01796713,  0.03765639, -0.03941826,\n",
       "        -0.01618453,  0.00628192, -0.0043477 , -0.03597885, -0.00878117,\n",
       "         0.08767164, -0.01497303, -0.02665254, -0.01302524, -0.02829309,\n",
       "        -0.08718653, -0.00174683, -0.0600834 , -0.05688876, -0.02207683,\n",
       "        -0.00959872, -0.01209632, -0.05893016, -0.0134788 , -0.05507741,\n",
       "        -0.0325698 , -0.05177484, -0.08775931, -0.0305383 , -0.00073222,\n",
       "        -0.07700045, -0.04340889, -0.00715729, -0.04786425,  0.0315089 ,\n",
       "        -0.07755212, -0.02716055,  0.04729325, -0.08755802, -0.02731228,\n",
       "         0.00718454, -0.03118062, -0.07835725, -0.10915846,  0.00159168,\n",
       "        -0.00731354, -0.01225757, -0.01914476,  0.03351985,  0.01089427,\n",
       "        -0.05832738, -0.14224371, -0.03576584,  0.04456902, -0.000757  ,\n",
       "         0.03093938, -0.02885435, -0.00280817,  0.01134229, -0.04378517,\n",
       "         0.0615575 ,  0.02901632, -0.0430181 ,  0.01795693, -0.00810694,\n",
       "        -0.06790127, -0.00443572,  0.01635651,  0.00989045, -0.04803739,\n",
       "         0.0139738 , -0.06465024, -0.07707804, -0.02952593, -0.09800228,\n",
       "        -0.03218525,  0.00706195, -0.01531379, -0.03954516, -0.01203465,\n",
       "         0.043837  ,  0.00101034, -0.01836536,  0.00685722, -0.07278425,\n",
       "        -0.04377524,  0.01495099, -0.07974492,  0.01325171, -0.02169541,\n",
       "         0.01997804,  0.01421765, -0.03940663, -0.06355347,  0.03975793,\n",
       "        -0.02328743,  0.0563975 ,  0.04225071, -0.03109835, -0.01666352,\n",
       "        -0.03430907, -0.00860342,  0.01001961,  0.00359407,  0.01288842,\n",
       "         0.00498054, -0.00509374, -0.04782482, -0.04231593, -0.01432745,\n",
       "        -0.09286752, -0.04726689, -0.02346229, -0.05374473,  0.01145206,\n",
       "         0.03026417, -0.06029093, -0.00253558,  0.06202162, -0.05841234,\n",
       "        -0.03516613, -0.00256508, -0.08468466, -0.02153719, -0.0368582 ,\n",
       "        -0.03436837,  0.05374306,  0.04055554, -0.07779582, -0.00427109,\n",
       "        -0.05840474, -0.0285328 , -0.02278991, -0.15128602,  0.00052356,\n",
       "        -0.08083797, -0.0130174 , -0.03916908, -0.01433287, -0.14204586,\n",
       "         0.04276562,  0.03892587, -0.02843608,  0.01542204, -0.1145288 ,\n",
       "        -0.02506554,  0.03367257, -0.00213193,  0.01320693, -0.04444836,\n",
       "         0.01883331, -0.05896155, -0.06115995, -0.04090078, -0.01691943,\n",
       "         0.00405402,  0.00108538, -0.09178237, -0.05631986, -0.06067519,\n",
       "        -0.02987276, -0.04702154, -0.02576351, -0.0612037 , -0.05243647,\n",
       "        -0.01006082,  0.01848766,  0.03871314,  0.02257213, -0.07605862,\n",
       "         0.00673083, -0.0318241 ,  0.01370405, -0.02269986, -0.00097142,\n",
       "        -0.02942256,  0.0227625 ,  0.00409969,  0.01073872,  0.08637159,\n",
       "         0.04685065,  0.02199768, -0.03398025, -0.03657096, -0.05069414,\n",
       "        -0.07068926, -0.03799183, -0.07610273,  0.07176458, -0.03762812,\n",
       "         0.02382901, -0.02638155, -0.00514337, -0.02068941, -0.06532674,\n",
       "        -0.02516313, -0.08015107, -0.05505192, -0.06204863,  0.05124347,\n",
       "         0.01688904,  0.01903924, -0.07061321, -0.01526833,  0.01070135,\n",
       "         0.01444305, -0.03720876, -0.04877125, -0.0189193 , -0.0124364 ,\n",
       "        -0.02761597, -0.06310479, -0.06761326, -0.01406777, -0.05796858,\n",
       "        -0.0277304 , -0.03851173,  0.00873912, -0.1048217 , -0.04117056,\n",
       "        -0.04438694, -0.01083885, -0.05739474,  0.00505072, -0.00536615,\n",
       "        -0.01123734, -0.04380842,  0.06836498, -0.04949834,  0.033135  ,\n",
       "        -0.07762786, -0.05575214, -0.02798773, -0.04192498, -0.04895433,\n",
       "        -0.06437089, -0.11018419, -0.00538384,  0.01429083, -0.02152648,\n",
       "        -0.0496478 , -0.07264582, -0.04929105, -0.01951207, -0.0619363 ,\n",
       "        -0.01403196, -0.13095725, -0.07425182, -0.02866054,  0.00218311,\n",
       "        -0.02678812, -0.02157573, -0.04306534, -0.00745715, -0.03779166,\n",
       "        -0.05122556,  0.02723394,  0.00819311, -0.0768657 ,  0.0272549 ,\n",
       "        -0.01246105,  0.01943599, -0.06547591,  0.01579106, -0.06771848,\n",
       "        -0.07174174, -0.00147516, -0.04715826, -0.04099685, -0.06936637,\n",
       "         0.01602412, -0.06892688, -0.05970216,  0.01043891, -0.01967197,\n",
       "        -0.0097134 , -0.0019278 , -0.07333077,  0.08527321,  0.01241145,\n",
       "        -0.08843168, -0.02522356,  0.01829377, -0.02715925, -0.04659085,\n",
       "         0.04570746, -0.03056694, -0.00275488, -0.12433532, -0.0351824 ,\n",
       "        -0.06592382, -0.03078032,  0.00180635, -0.02113222,  0.09001572,\n",
       "        -0.01052239, -0.02229475, -0.03569242,  0.00036761, -0.01738564,\n",
       "        -0.03722301, -0.00113262,  0.05362143, -0.03955411,  0.002896  ,\n",
       "        -0.05952486, -0.05586326, -0.08437575,  0.01152003, -0.03250872,\n",
       "        -0.03138869,  0.01692799, -0.02602969, -0.11407799, -0.09128868,\n",
       "        -0.07265399, -0.00359682,  0.12208072, -0.03638796,  0.01464378,\n",
       "        -0.0503557 ,  0.0402507 , -0.06297771,  0.01648491, -0.00340791,\n",
       "        -0.04787293, -0.08782237, -0.06150886, -0.06769664, -0.00179957,\n",
       "        -0.03849053,  0.09978071,  0.00915475, -0.07090563, -0.10180055,\n",
       "        -0.07990018,  0.00577071,  0.00488389, -0.07092109, -0.01440103,\n",
       "        -0.05694594,  0.00914358, -0.04349818, -0.01060246, -0.01127336,\n",
       "         0.01695217, -0.0403586 ,  0.01587945, -0.08662798, -0.01474116,\n",
       "        -0.06371089, -0.04907921, -0.0448628 ,  0.05979871, -0.02591555,\n",
       "         0.03503687, -0.02138414, -0.0339639 ,  0.00137897,  0.01739392,\n",
       "        -0.01392002, -0.04831953, -0.06016075, -0.06491395,  0.01704529,\n",
       "        -0.04203914, -0.04868194,  0.03360818, -0.02246774, -0.02588319,\n",
       "        -0.02498071, -0.06587233, -0.02461761, -0.08513182,  0.08144202,\n",
       "        -0.04022377,  0.01476349,  0.0111707 , -0.03091095,  0.03112956,\n",
       "        -0.00713935,  0.01671844, -0.10406578, -0.04446519, -0.03654759,\n",
       "        -0.00090015, -0.0822074 , -0.04481355,  0.01534062,  0.03918026,\n",
       "        -0.01107791,  0.01082302, -0.00779742, -0.03331521, -0.0068609 ,\n",
       "         0.02225946, -0.03062034, -0.00548296, -0.04015785,  0.00021749,\n",
       "        -0.03255514, -0.02342268,  0.00763948, -0.08413568,  0.00275202,\n",
       "         0.02455448, -0.02556607,  0.00588004, -0.04904316, -0.00150067,\n",
       "        -0.02676761, -0.11770628,  0.01419074, -0.01508235, -0.03976258,\n",
       "        -0.02756159, -0.08268955, -0.0486187 , -0.06423166,  0.01544593,\n",
       "        -0.02693414,  0.01646638, -0.0501403 , -0.03779647,  0.01287141,\n",
       "        -0.00923243, -0.00107006,  0.00207343, -0.04434131, -0.00433736,\n",
       "        -0.0507568 , -0.05762206, -0.04867698, -0.00996203, -0.00558443,\n",
       "        -0.06657334,  0.01561775, -0.05053767, -0.03807011, -0.01189713,\n",
       "        -0.06210907, -0.0495418 , -0.07157552, -0.09053706,  0.00181156,\n",
       "        -0.04218731, -0.04662285, -0.12897606,  0.03579992,  0.0100256 ,\n",
       "         0.02326034, -0.06064779, -0.04980562, -0.06982776, -0.01257232,\n",
       "        -0.05897349, -0.04919791, -0.04425377,  0.02722542, -0.03964664,\n",
       "        -0.00589339, -0.02847223, -0.05516527,  0.0109106 , -0.03825077,\n",
       "        -0.01748873, -0.04376196, -0.0285569 , -0.02454232, -0.0121718 ,\n",
       "        -0.05331823, -0.03772478,  0.00095005, -0.02478909, -0.02408331,\n",
       "         0.03763863, -0.00808368, -0.00888659,  0.02268378, -0.01658686,\n",
       "        -0.0513445 , -0.01339481, -0.00470968,  0.00539002, -0.0560668 ,\n",
       "         0.0124986 , -0.05672762, -0.0138257 , -0.05866195,  0.0203    ,\n",
       "         0.03280495, -0.05672722, -0.05720117, -0.01101085, -0.00529359,\n",
       "         0.00630592, -0.05287591, -0.09404698,  0.00321512, -0.00732199,\n",
       "        -0.1058289 ,  0.04019904, -0.05588099,  0.01275758, -0.02172038,\n",
       "         0.08089678, -0.08789966, -0.04367155, -0.09806436,  0.02455979,\n",
       "         0.025256  , -0.07345985, -0.04881034, -0.08482597,  0.01101647,\n",
       "         0.00815555, -0.08573112, -0.02448465, -0.07433309, -0.00483828,\n",
       "        -0.0585055 , -0.00990027, -0.08444568, -0.00440183, -0.05598024,\n",
       "        -0.07475943, -0.02485162,  0.00843887, -0.08426636, -0.02037607,\n",
       "        -0.04357914, -0.03103529, -0.03505394, -0.03477909, -0.01211137,\n",
       "        -0.05613294,  0.00179894, -0.05551952, -0.07097323, -0.02187284,\n",
       "        -0.08530426,  0.02231892,  0.00967894, -0.02867814, -0.06385659,\n",
       "         0.00504741,  0.0078729 , -0.04916536, -0.04518002,  0.0471171 ,\n",
       "         0.03352923,  0.03048048, -0.01245142, -0.01099646, -0.00735218,\n",
       "         0.0011658 , -0.03855402, -0.03913379, -0.00843964, -0.03549154,\n",
       "         0.03258345, -0.07661324, -0.03411501, -0.05275038, -0.02779922,\n",
       "        -0.00328957, -0.04803751,  0.03381702, -0.03121832, -0.10256522,\n",
       "        -0.09645248, -0.03838626, -0.04535397, -0.06665562, -0.0674329 ,\n",
       "         0.01928262, -0.04901787, -0.0272354 , -0.07807704,  0.01493715,\n",
       "        -0.0357038 ,  0.02745784, -0.04218743,  0.00024408,  0.02782501,\n",
       "         0.01505823, -0.01638191, -0.02046041,  0.01921135, -0.02198615,\n",
       "        -0.04741193, -0.04992861, -0.0194096 , -0.06105652, -0.00595623,\n",
       "        -0.01816186,  0.0177033 ,  0.00843524,  0.00802204, -0.04932498,\n",
       "        -0.02861249, -0.09120153, -0.03054466]),\n",
       " 'range': (0, 2)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(tokenized_pn_histories.values())[0][0].keys()\n",
    "list(tokenized_pn_histories.values())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized features loaded from cache.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_features.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_features = pickle.load(f)\n",
    "    print(\"Tokenized features loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized features. Tokenizing...\")\n",
    "    tokenized_features: Dict[str, List[str]] = {}\n",
    "    for feature_text in tqdm(train[\"feature_text\"]):\n",
    "        indexed_words = []\n",
    "        if feature_text in tokenized_features:\n",
    "            continue\n",
    "\n",
    "        tokenized = tokenizer.encode_plus(feature_text, add_special_tokens=True)\n",
    "\n",
    "        for word in tokenized[\"input_ids\"]:\n",
    "            embedded_token = embed_word(word)\n",
    "\n",
    "            indexed_words.append({\n",
    "                \"word_id\": word,\n",
    "                \"embedded\": embedded_token,\n",
    "            })\n",
    "                \n",
    "        tokenized_features[feature_text] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_features, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_id': 2155,\n",
       " 'embedded': array([ 1.52311306e-02,  9.42442659e-03, -3.38619053e-02, -2.42799595e-02,\n",
       "         2.34256983e-02, -8.17641430e-03, -2.93032657e-02, -5.23912981e-02,\n",
       "         3.92839797e-02,  2.82708416e-03, -1.66527238e-02, -2.51747109e-02,\n",
       "        -3.52113359e-02,  3.62591706e-02, -7.53024034e-03,  2.13038772e-02,\n",
       "        -2.84320372e-03, -5.02108894e-02, -3.15106139e-02, -1.20872613e-02,\n",
       "        -3.53940949e-02, -3.89684625e-02, -9.54788644e-03,  1.48558039e-02,\n",
       "        -1.46783004e-02,  4.23340984e-02, -1.99949723e-02,  8.76626652e-03,\n",
       "        -2.45130006e-02, -5.38146645e-02,  2.60462761e-02, -1.56000787e-02,\n",
       "        -3.20293419e-02, -1.68701317e-02, -5.71791418e-02,  6.56096172e-03,\n",
       "        -2.73210574e-02, -9.99859255e-03, -2.09460687e-02,  1.66101735e-02,\n",
       "         2.68947575e-02,  1.13374833e-03, -1.66690629e-02,  1.36123579e-02,\n",
       "         2.36188024e-02, -7.47381104e-03,  1.41381836e-02,  1.25802001e-02,\n",
       "         2.57711634e-02,  1.62442252e-02, -3.19506996e-03, -6.07429678e-03,\n",
       "        -3.58607359e-02, -6.43704087e-02, -3.92574770e-03, -1.00825774e-02,\n",
       "         2.75388900e-02, -3.10948975e-02,  2.38278732e-02,  7.44539779e-03,\n",
       "        -4.51974235e-02, -2.97444407e-02, -4.98110019e-02,  2.10578293e-02,\n",
       "        -8.30610245e-02, -9.97002819e-04, -8.61699730e-02,  2.23975535e-02,\n",
       "        -3.63851711e-02,  2.59253103e-02,  1.60876010e-02, -9.07502025e-02,\n",
       "        -5.43588540e-04, -9.99948084e-02, -2.06221528e-02, -3.46545305e-04,\n",
       "        -3.78179066e-02, -7.17385709e-02,  5.57350414e-03, -4.78198193e-02,\n",
       "        -6.21052459e-02, -2.02608649e-02, -8.12239870e-02,  8.56962427e-03,\n",
       "        -5.33923600e-03, -3.35443653e-02,  2.26709917e-02,  1.13433441e-02,\n",
       "        -8.49064216e-02, -1.88676231e-02, -8.39527976e-03, -3.40117491e-03,\n",
       "        -7.06991032e-02,  6.59702858e-03, -1.87366959e-02, -6.42724708e-02,\n",
       "        -4.58222963e-02, -3.97029705e-02, -3.07402085e-03, -2.72165854e-02,\n",
       "        -2.03661732e-02,  1.61965732e-02, -2.54113898e-02, -8.35221335e-02,\n",
       "        -1.46965766e-02, -7.56659508e-02,  2.90891603e-02, -7.51430122e-03,\n",
       "         1.30209271e-02,  5.33524826e-02, -2.45514722e-03, -1.76951755e-02,\n",
       "         1.70296934e-02,  7.50798220e-03, -5.08431718e-03, -1.42686833e-02,\n",
       "        -4.51855659e-02,  3.96616245e-03,  4.22960185e-02, -4.65673115e-03,\n",
       "         1.94535088e-02, -4.98280525e-02,  1.47982426e-02,  3.21139093e-03,\n",
       "        -3.62973884e-02, -3.59461233e-02,  1.67836889e-03, -6.97912276e-02,\n",
       "        -1.32286549e-02, -3.76444422e-02,  4.71290462e-02,  1.99587867e-02,\n",
       "        -4.86811101e-02, -4.27232571e-02,  1.33466849e-03,  2.61137411e-02,\n",
       "         2.37338878e-02,  3.08593665e-03,  2.86678188e-02, -5.49637191e-02,\n",
       "         8.24517012e-03, -5.23012038e-03,  3.77820097e-02, -1.67999149e-03,\n",
       "        -4.83294018e-02, -4.34270166e-02, -3.65219153e-02,  9.22037847e-03,\n",
       "         8.11870769e-03,  7.81739503e-03, -6.72895610e-02, -1.86866131e-02,\n",
       "        -2.00832319e-02, -4.90455143e-02,  1.05020113e-03, -4.55542430e-02,\n",
       "        -4.85595502e-02, -3.35119255e-02, -2.23896205e-02, -6.92902580e-02,\n",
       "         3.83430012e-02,  1.91386443e-05, -5.41733727e-02, -4.62411642e-02,\n",
       "        -2.11368464e-02, -2.94983741e-02,  1.82185769e-02,  3.38594019e-02,\n",
       "        -3.82412262e-02, -2.97777075e-03,  6.72160927e-03, -7.17436150e-03,\n",
       "        -4.06495575e-03, -3.94228101e-02, -5.38021140e-02, -6.17605262e-02,\n",
       "        -5.65859154e-02,  8.64125788e-03, -4.81633469e-02, -9.36725512e-02,\n",
       "        -3.73072028e-02,  4.95297788e-03,  4.02747374e-03, -4.07767780e-02,\n",
       "        -7.65677774e-03, -2.06898395e-02, -1.17031718e-03,  2.28958782e-02,\n",
       "        -1.12383971e-02,  6.36798739e-02, -9.20545086e-02,  5.81580065e-02,\n",
       "        -1.82220433e-02, -4.25849259e-02, -1.29954927e-02,  1.97418220e-02,\n",
       "        -4.16004136e-02,  2.24671829e-02, -3.77326868e-02, -3.30510885e-02,\n",
       "        -1.21155763e-02, -2.03517266e-02, -4.36056405e-02, -7.61120394e-02,\n",
       "        -8.93824548e-02,  1.31214634e-02,  2.85673253e-02,  2.52485983e-02,\n",
       "        -5.57261147e-03,  4.16146964e-02,  1.91640407e-02, -3.57294232e-02,\n",
       "         1.64403860e-02, -4.65510525e-02, -2.96277218e-02, -4.86823817e-04,\n",
       "        -7.15841204e-02, -6.31128326e-02, -4.75690514e-02, -5.93900159e-02,\n",
       "         2.08735801e-02,  2.44462863e-02, -1.45831341e-02, -1.28136994e-02,\n",
       "        -3.97576131e-02, -7.13715702e-02,  1.87176038e-02, -5.16580753e-02,\n",
       "        -5.43301925e-02, -1.17413262e-02, -7.23965243e-02, -8.83746892e-02,\n",
       "        -9.87459160e-03,  1.01658544e-02,  1.61665827e-02, -5.68849482e-02,\n",
       "        -1.27311926e-02,  2.18595620e-02, -5.29118069e-02, -9.06441733e-02,\n",
       "         1.15458379e-02, -3.99737284e-02, -1.26529252e-02, -1.56024471e-02,\n",
       "        -5.21961926e-03, -2.42009237e-02, -2.17853766e-03,  2.80902442e-02,\n",
       "         5.73536905e-04, -1.55107141e-01,  8.53505824e-03,  4.97472510e-02,\n",
       "         4.20570038e-02,  3.38615887e-02, -4.43216274e-03,  3.12609039e-02,\n",
       "         6.20675413e-03, -2.02233028e-02, -5.60681894e-03,  1.64948292e-02,\n",
       "         3.04654222e-02, -4.50957753e-02, -8.02531186e-03, -3.92436385e-02,\n",
       "        -6.30453750e-02,  3.55113596e-02, -6.67691007e-02, -3.63236144e-02,\n",
       "        -4.18387204e-02, -4.26423550e-02, -1.70283001e-02,  2.36203521e-02,\n",
       "        -4.08612043e-02, -7.70878568e-02,  5.72917722e-02, -6.99767992e-02,\n",
       "        -4.64395657e-02,  1.89192742e-02, -7.71297365e-02, -4.31481525e-02,\n",
       "        -2.68922225e-02, -5.83421886e-02,  3.16341892e-02, -6.81618825e-02,\n",
       "        -1.01459846e-02, -1.33461431e-02,  1.34614259e-02,  3.69603820e-02,\n",
       "         7.83465523e-03, -4.28813845e-02,  5.92431845e-03, -5.19340970e-02,\n",
       "         2.69206036e-02, -5.37696294e-02, -4.07555327e-03,  3.93235050e-02,\n",
       "        -7.77556598e-02, -6.68021821e-05, -5.37035391e-02,  1.57563388e-02,\n",
       "        -6.58299774e-02, -4.68439497e-02,  1.98894329e-02, -7.45963529e-02,\n",
       "         2.90387124e-03, -1.93812866e-02, -2.45766323e-02,  3.95529531e-02,\n",
       "        -3.68425213e-02, -1.70048475e-02, -9.45194140e-02, -3.51017457e-03,\n",
       "        -4.69196737e-02, -4.34695184e-03,  2.08559576e-02, -2.31875926e-02,\n",
       "        -2.35556606e-02, -9.85213276e-03,  1.67959761e-02, -5.32903746e-02,\n",
       "        -5.05688861e-02, -3.64725338e-03, -4.53368388e-02, -2.04105787e-02,\n",
       "         1.63314883e-02, -5.16547672e-02,  6.41106814e-02,  1.10853575e-02,\n",
       "        -3.57060246e-02,  4.42247316e-02,  2.65915319e-03, -4.81954701e-02,\n",
       "         2.80114841e-02, -4.38882597e-02,  2.98616104e-02,  8.07774533e-03,\n",
       "        -7.38246813e-02, -2.56575122e-02, -1.20891235e-03, -5.35254218e-02,\n",
       "        -3.03421132e-02, -3.46376821e-02, -1.99767146e-02, -8.55871439e-02,\n",
       "         2.25002579e-02, -8.78942460e-02, -2.18789186e-02,  2.54477244e-02,\n",
       "         1.60067272e-03,  1.91864464e-02, -5.95970266e-03,  1.39965536e-02,\n",
       "        -1.05711684e-01, -7.85354525e-02, -4.32069413e-02, -3.77054606e-03,\n",
       "        -2.20834091e-02, -6.89397007e-02, -6.04365431e-02, -2.98009124e-02,\n",
       "        -5.05183153e-02,  2.33694855e-02, -4.99159750e-03,  1.63934361e-02,\n",
       "         1.02080768e-02,  3.52836959e-03,  6.18576771e-03,  5.88758476e-02,\n",
       "        -1.12545071e-02, -4.26817685e-03,  4.06682380e-02,  2.28572860e-02,\n",
       "        -6.76611438e-02, -2.92034447e-02, -7.90591761e-02,  5.47752250e-03,\n",
       "        -6.78537320e-03, -2.99374294e-02, -3.11338827e-02, -4.36666347e-02,\n",
       "         3.24213393e-02, -2.93021034e-02,  1.93920881e-02,  6.73353719e-03,\n",
       "        -2.53700726e-02,  3.42251137e-02, -7.28035867e-02,  9.03593563e-03,\n",
       "         4.22166623e-02,  5.74758910e-02, -4.18832451e-02,  1.00277308e-02,\n",
       "        -1.27890510e-02, -6.17674440e-02, -6.05846308e-02, -7.12366924e-02,\n",
       "        -7.56696565e-03, -1.11925546e-02,  2.78304443e-02, -7.62914354e-03,\n",
       "         3.03969625e-02, -6.32165149e-02, -2.98878085e-02, -1.06464615e-02,\n",
       "        -1.34275574e-02,  5.33745997e-03,  1.58859137e-02, -3.51887234e-02,\n",
       "        -7.19104987e-03, -1.49495322e-02,  3.07643297e-03,  2.02511586e-02,\n",
       "         6.58996054e-04, -4.89874445e-02,  1.69149507e-02,  4.35995124e-02,\n",
       "        -4.81884778e-02, -8.31077155e-03, -7.14417845e-02,  7.55870044e-02,\n",
       "         4.10913453e-02, -6.23122752e-02,  3.53792831e-02, -7.20357075e-02,\n",
       "        -2.18317490e-02,  4.16731164e-02, -7.10124969e-02, -4.97945212e-02,\n",
       "        -5.90372048e-02, -2.46399026e-02, -9.29307714e-02,  1.45874368e-02,\n",
       "        -1.20130526e-02, -3.22679393e-02, -1.55276097e-02,  1.94596555e-02,\n",
       "         4.00568731e-02, -6.19752593e-02, -2.73674540e-02, -3.16938274e-02,\n",
       "         5.63009568e-02,  2.54132338e-02, -1.75560557e-03, -6.70711473e-02,\n",
       "        -2.62224693e-02, -5.79263605e-02,  3.85424905e-02, -6.09059446e-03,\n",
       "        -3.65220360e-03, -6.24917075e-02, -4.70685139e-02, -4.15379219e-02,\n",
       "         9.23450571e-03, -3.37729529e-02, -4.45652194e-02,  2.26356592e-02,\n",
       "        -5.98876774e-02,  1.05139697e-02, -1.96131822e-02, -2.15638778e-03,\n",
       "         1.75591093e-02, -1.05781227e-01, -1.51436462e-03, -6.12972789e-02,\n",
       "        -1.92073025e-02, -2.87937354e-02, -1.04767969e-02, -2.19317749e-02,\n",
       "         1.54626770e-02, -5.31485565e-02,  1.73501242e-02, -3.59393656e-02,\n",
       "        -3.91451716e-02, -2.48707961e-02, -1.15518114e-02, -1.33844232e-02,\n",
       "         7.91026279e-03, -4.40833196e-02, -3.13281491e-02,  5.17701777e-03,\n",
       "         2.41726916e-02,  2.15686187e-02, -3.01312525e-02,  1.59254251e-03,\n",
       "         3.13285142e-02,  2.56159510e-02,  7.95306917e-03, -1.81422122e-02,\n",
       "         3.82506289e-03,  2.92187426e-02,  9.22858436e-03, -4.38426621e-02,\n",
       "         5.14102355e-03, -9.82083473e-03,  2.44829375e-02, -6.46073595e-02,\n",
       "        -1.09620303e-01, -8.12270388e-04,  1.21808061e-02, -1.81697942e-02,\n",
       "        -2.80488543e-02, -4.14994322e-02, -3.73201706e-02, -3.00048701e-02,\n",
       "        -3.60519662e-02, -1.62596609e-02, -6.43436164e-02,  7.89933372e-03,\n",
       "        -1.22455703e-02, -1.78920366e-02, -7.34208375e-02, -5.51692732e-02,\n",
       "         4.76007164e-02, -8.00941065e-02, -9.89862625e-03,  2.44176947e-02,\n",
       "         1.24313831e-02, -6.44868314e-02, -5.55185378e-02,  8.74918327e-03,\n",
       "        -3.83896679e-02,  1.24899922e-02,  3.05021331e-02, -9.15587135e-03,\n",
       "         4.76321355e-02,  5.24125993e-02, -1.22695873e-02,  2.09699739e-02,\n",
       "         1.96429305e-02, -3.37100476e-02, -5.85683808e-02,  1.11338934e-02,\n",
       "        -2.87697408e-02, -3.24240476e-02, -1.05963545e-02, -3.99678275e-02,\n",
       "         7.84924533e-03,  6.65762648e-02,  3.21839377e-02,  4.31494005e-02,\n",
       "         1.05961384e-02, -4.09080945e-02, -1.06012495e-02, -4.37951734e-04,\n",
       "        -7.11712660e-03, -4.40713204e-02,  8.71266704e-03,  2.52674017e-02,\n",
       "        -2.56595351e-02,  5.41124074e-03,  6.63584098e-03, -2.13722922e-02,\n",
       "         4.16363962e-02, -9.27571114e-03, -3.08887772e-02, -2.52176039e-02,\n",
       "        -3.21201188e-03, -3.74785042e-03, -5.54295555e-02, -5.98303303e-02,\n",
       "        -4.50135842e-02,  7.26621412e-03,  8.26125685e-03, -4.43532392e-02,\n",
       "        -5.36668599e-02, -5.59826866e-02,  5.83275296e-02, -2.26354469e-02,\n",
       "        -2.24626046e-02, -3.48942401e-03, -1.97326597e-02,  9.84343607e-03,\n",
       "         6.60307892e-03,  2.85363346e-02, -3.41117755e-02, -6.81525543e-02,\n",
       "        -9.20211300e-02, -6.34001791e-02, -3.61268036e-02, -1.66131256e-04,\n",
       "        -8.19247663e-02,  2.08414402e-02, -1.68942530e-02,  7.92026147e-03,\n",
       "         9.15625319e-03,  1.62156578e-02,  8.38278234e-03,  4.91934642e-03,\n",
       "        -7.17306137e-02,  3.89475003e-02, -9.35496092e-02, -7.20771700e-02,\n",
       "        -6.46768957e-02, -2.28228681e-02, -2.91856695e-02,  6.64982805e-03,\n",
       "         1.91643536e-02,  3.34108099e-02, -4.16697450e-02, -2.07496732e-02,\n",
       "        -6.27234951e-02, -2.01628916e-03, -4.06005140e-03, -2.08371785e-02,\n",
       "        -3.25768478e-02,  1.99883450e-02, -3.55855599e-02, -5.78509131e-03,\n",
       "         4.68035862e-02, -5.50547428e-02, -2.74876710e-02,  3.32259922e-03,\n",
       "        -4.06967737e-02,  3.76991555e-02, -7.08693266e-02,  3.71663552e-03,\n",
       "         5.03133703e-03, -1.43978521e-02, -2.82731354e-02,  1.85701102e-02,\n",
       "        -3.86685915e-02,  2.22261343e-02,  3.91379185e-03,  7.54645979e-03,\n",
       "        -3.72246169e-02, -5.37433587e-02,  7.55747547e-03, -6.95776492e-02,\n",
       "        -2.79285535e-02, -5.85377216e-02, -4.50596446e-03, -2.69717770e-04,\n",
       "         3.78758498e-02, -3.47519815e-02, -3.93273011e-02, -5.14439456e-02,\n",
       "        -4.74330969e-02, -1.09439073e-02, -3.70996445e-02, -1.93865709e-02,\n",
       "        -1.96613688e-02,  9.40618396e-04, -1.56773217e-02,  3.64401611e-03,\n",
       "        -3.43837254e-02, -3.89860645e-02, -2.49423403e-02,  8.36504716e-03,\n",
       "        -2.78186500e-02,  1.71869493e-03,  5.78296464e-03, -2.53306678e-03,\n",
       "        -3.93334217e-02, -3.97457965e-02, -5.49873188e-02, -4.62207459e-02,\n",
       "        -1.58211980e-02, -2.50746030e-02, -3.13455053e-02, -5.77586843e-03,\n",
       "        -2.93670651e-02, -1.24803996e-02,  4.18163743e-03,  1.41403750e-02,\n",
       "         1.68143697e-02, -5.35635836e-02,  2.87874937e-02, -1.67598464e-02,\n",
       "        -1.32627971e-02,  9.33323335e-03, -8.31953064e-02, -9.45595501e-04,\n",
       "        -4.44996450e-03,  4.18075733e-02, -4.24354225e-02, -3.35405394e-02,\n",
       "         1.97008271e-02, -5.04458733e-02, -2.89398041e-02,  5.09284139e-02,\n",
       "         4.76725884e-02, -1.43112019e-02, -3.01140681e-04, -6.54487610e-02,\n",
       "         7.51618156e-03,  2.27209907e-02, -3.55842635e-02, -2.56626513e-02,\n",
       "        -6.88615302e-03,  1.77982356e-02, -4.24528345e-02, -1.65193086e-03,\n",
       "        -4.28550541e-02, -2.94640753e-02,  8.80818442e-03, -1.37469852e-02,\n",
       "        -9.18758214e-02, -1.80169865e-02, -8.91804397e-02, -6.12931810e-02,\n",
       "        -1.28519749e-02, -6.08912157e-03,  4.85325381e-02, -6.62526116e-02,\n",
       "        -4.07447889e-02, -2.91593820e-02, -5.36921842e-04,  3.26046161e-03,\n",
       "         1.61990337e-02, -2.81461086e-02, -2.81694718e-02,  8.23842920e-03,\n",
       "        -6.10045390e-03,  1.00754609e-03, -2.93709431e-02, -5.30086383e-02,\n",
       "        -5.88793345e-02, -3.39528397e-02, -1.27446912e-02, -2.56063547e-02,\n",
       "         3.94139905e-03, -5.62214255e-02,  9.49775148e-03,  2.24863831e-03,\n",
       "        -5.57465330e-02, -2.01091785e-02,  1.34388525e-02, -1.14753349e-02,\n",
       "         1.08914003e-02, -5.01763411e-02, -3.63819189e-02,  3.38192880e-02,\n",
       "         4.13231961e-02, -4.98285294e-02, -3.86047736e-02,  4.57839528e-03,\n",
       "         4.91425917e-02, -3.65576930e-02,  6.80767139e-03, -1.43696666e-02,\n",
       "        -2.49997601e-02,  6.44037826e-03,  8.70285928e-03, -7.69861937e-02,\n",
       "        -1.04244985e-02, -2.14265324e-02, -3.94667238e-02,  3.64254937e-02,\n",
       "        -3.65097187e-02,  5.31419925e-02,  4.42693047e-02, -2.81242188e-03,\n",
       "        -3.75478789e-02, -5.29529899e-02,  1.93552990e-02,  3.82753951e-03,\n",
       "        -2.99465526e-02, -1.02200909e-02,  1.28179248e-02,  1.41479671e-02,\n",
       "         2.69795326e-03,  4.07936750e-03, -3.60189155e-02, -1.88907720e-02,\n",
       "        -6.15102472e-03, -1.76733863e-02,  3.19157355e-03,  5.39453290e-02])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenized_features.values())[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follow the example described here. Use the same architecture, but:\n",
    "  - only use the last output of the LSTM in the loss function\n",
    "  - use an embedding dim of 128\n",
    "  - use a hidden dim of 256.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature-relevancy of tokens via char ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9901it [00:08, 1114.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_preprocessed = dict()\n",
    "for i, (feature_text, pn_history, location) in tqdm(train.iterrows()):\n",
    "    tokenized_history = tokenized_pn_histories[pn_history]\n",
    "    tokens_with_scores = []\n",
    "    for token in tokenized_history:\n",
    "        percentages = []\n",
    "        for feature_relevant_range in location:\n",
    "            token_start, token_end = token[\"range\"]\n",
    "            range_start, range_end = feature_relevant_range[0], feature_relevant_range[1]\n",
    "            \n",
    "            percentage_of_token_in_range = max(min(token_end, range_end)+1 - max(token_start, range_start), 0) / (token_end+1 - token_start)\n",
    "            percentages.append(percentage_of_token_in_range)\n",
    "            # if percentage_of_token_in_range > 0:\n",
    "            #     print(percentage_of_token_in_range, token, feature_relevant_range)\n",
    "        \n",
    "\n",
    "        tokens_with_scores.append({\"token\": token,\n",
    "                                   \"score\": int(max(percentages) > 0.9)})\n",
    "\n",
    "    train_data_preprocessed[i] = {\n",
    "                                    \"pn_history_tokens\": [ts[\"token\"] for ts in tokens_with_scores],\n",
    "                                    \"scores\": torch.tensor([ts[\"score\"] for ts in tokens_with_scores]),\n",
    "                                    \"feature_tokens\": tokenized_features[feature_text],\n",
    "                                    \"locations\": location\n",
    "                                   }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering 2 out of 9901 datapoints because they don't contain any positive scores.\n"
     ]
    }
   ],
   "source": [
    "num_no_positives = sum([1 for dp in train_data_preprocessed.values() if sum(dp[\"scores\"]) == 0])\n",
    "print(f\"filtering {num_no_positives} out of {len(train_data_preprocessed)} datapoints because they don't contain any positive scores.\")\n",
    "train_data_preprocessed = {key: dp for key, dp in train_data_preprocessed.items() if sum(dp[\"scores\"]) != 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of the Model\n",
    "Layers in LSTM Model:\n",
    "1. embed feature tokens\n",
    "2. lstm feature -> constant size vector\n",
    "\n",
    "3. pass to 2nd lstm\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_matrix.shape[1]\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTokenScorer(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout=0.0):\n",
    "        super(LSTMTokenScorer, self).__init__()\n",
    "\n",
    "        self.pn_history_hidden_dim = hidden_dim\n",
    "\n",
    "        # self.bert_embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix))\n",
    "\n",
    "        self.feature_lstm = nn.LSTM(embedding_dim, embedding_dim, bidirectional=False, dropout=dropout) # the feature is now one tensor of size [embedding_dim].\n",
    "\n",
    "        self.total_lstm = nn.LSTM(embedding_dim * 2, self.pn_history_hidden_dim, bidirectional=False, dropout=dropout)\n",
    "        \n",
    "        self.hidden2score = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, pn_history, feature):\n",
    "        # feature_embeds = self.bert_embedding(feature)\n",
    "        feature_lstm_out, _ = self.feature_lstm(feature.view(len(feature), 1, -1)) # the feature is now one tensor of size [embedding_dim].\n",
    "        feature_reduced = torch.squeeze(feature_lstm_out[-1]) #.view(1, -1)\n",
    "        feature_multiplied = feature_reduced.repeat((len(pn_history), 1)) # duplicate feature vector to be same size as embedded pn_history vector.\n",
    "\n",
    "        # pn_history_embeds = self.bert_embedding(pn_history)\n",
    "        pn_history_and_features = torch.concat((feature_multiplied, pn_history), dim=1)\n",
    "\n",
    "        pn_history_reduced, _ = self.total_lstm(pn_history_and_features)\n",
    "        pred_score_raw = torch.squeeze(self.hidden2score(pn_history_reduced))\n",
    "        pred_score = self.sigmoid(pred_score_raw)\n",
    "        return pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_scores = [d[\"scores\"].numpy() for d in train_data_preprocessed.values()]\n",
    "# avg_neg_div_pos = np.mean([(scores.shape[0] - np.sum(scores)) / np.sum(scores) for scores in all_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model with vocab sizes, including placeholder indices\n",
    "model = LSTMTokenScorer(EMBEDDING_DIM, HIDDEN_DIM)\n",
    "# loss_function = nn.BCELoss()\n",
    "pos_weight = torch.full((1,), 30)\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)#make positive class more valuable\n",
    "\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "from sklearn.metrics import f1_score\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pn_history_tokens', 'scores', 'feature_tokens', 'locations'])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_preprocessed[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4970, 0.4967, 0.4954, 0.4939, 0.4944, 0.4945, 0.4947, 0.4922, 0.4924,\n",
       "        0.4926, 0.4924, 0.4919, 0.4924, 0.4921, 0.4915, 0.4925, 0.4927, 0.4924,\n",
       "        0.4932, 0.4924, 0.4905, 0.4935, 0.4927, 0.4933, 0.4940, 0.4909, 0.4916,\n",
       "        0.4909, 0.4912, 0.4913, 0.4929, 0.4930, 0.4932, 0.4935, 0.4933, 0.4943,\n",
       "        0.4931, 0.4934, 0.4936, 0.4915, 0.4933, 0.4923, 0.4919, 0.4944, 0.4942,\n",
       "        0.4944, 0.4948, 0.4949, 0.4932, 0.4933, 0.4915, 0.4914, 0.4919, 0.4931,\n",
       "        0.4930, 0.4926, 0.4930, 0.4937, 0.4941, 0.4923, 0.4918, 0.4924, 0.4933,\n",
       "        0.4929, 0.4933, 0.4928, 0.4902, 0.4902, 0.4911, 0.4924, 0.4919, 0.4916,\n",
       "        0.4921, 0.4918, 0.4942, 0.4930, 0.4922, 0.4922, 0.4926, 0.4944, 0.4928,\n",
       "        0.4932, 0.4928, 0.4938, 0.4930, 0.4933, 0.4927, 0.4931, 0.4930, 0.4933,\n",
       "        0.4933, 0.4955, 0.4936, 0.4936, 0.4920, 0.4919, 0.4908, 0.4929, 0.4932,\n",
       "        0.4954, 0.4939, 0.4919, 0.4927, 0.4920, 0.4938, 0.4919, 0.4915, 0.4919,\n",
       "        0.4918, 0.4935, 0.4935, 0.4928, 0.4925, 0.4909, 0.4910, 0.4907, 0.4922,\n",
       "        0.4929, 0.4933, 0.4934, 0.4929, 0.4933, 0.4919, 0.4916, 0.4928, 0.4893,\n",
       "        0.4899, 0.4918, 0.4918, 0.4927, 0.4944, 0.4940, 0.4939, 0.4927, 0.4929,\n",
       "        0.4931, 0.4934, 0.4925, 0.4921, 0.4925, 0.4910, 0.4922, 0.4924, 0.4922,\n",
       "        0.4913, 0.4923, 0.4930, 0.4923, 0.4920, 0.4907, 0.4915, 0.4934, 0.4902,\n",
       "        0.4906, 0.4901, 0.4918, 0.4930, 0.4929, 0.4916, 0.4928, 0.4902, 0.4912,\n",
       "        0.4908, 0.4925, 0.4918, 0.4941, 0.4933, 0.4915, 0.4925, 0.4919, 0.4924,\n",
       "        0.4925, 0.4913, 0.4926, 0.4933, 0.4930, 0.4937, 0.4932, 0.4927, 0.4933,\n",
       "        0.4936, 0.4936, 0.4932, 0.4926, 0.4939, 0.4920, 0.4927, 0.4940, 0.4932,\n",
       "        0.4933, 0.4905, 0.4924, 0.4910, 0.4909, 0.4919, 0.4927, 0.4935, 0.4943,\n",
       "        0.4963, 0.4933, 0.4937, 0.4933, 0.4926, 0.4925, 0.4932, 0.4921, 0.4942,\n",
       "        0.4930, 0.4933, 0.4932, 0.4927, 0.4913, 0.4922, 0.4919, 0.4919, 0.4947,\n",
       "        0.4937, 0.4914, 0.4936, 0.4930, 0.4935, 0.4930, 0.4912, 0.4930, 0.4893,\n",
       "        0.4915, 0.4910, 0.4936, 0.4926, 0.4926, 0.4911, 0.4923, 0.4935, 0.4924,\n",
       "        0.4925, 0.4918, 0.4917, 0.4916, 0.4950, 0.4943, 0.4923, 0.4948, 0.4939],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_tokens = train_data_preprocessed[0][\"feature_tokens\"]\n",
    "pn_history_tokens = train_data_preprocessed[0][\"pn_history_tokens\"]\n",
    "\n",
    "feature_tensor = torch.tensor(np.array([t[\"embedded\"] for t in feature_tokens]), dtype=torch.float)\n",
    "pn_history_tensor = torch.tensor(np.array([t[\"embedded\"] for t in pn_history_tokens]), dtype=torch.float)\n",
    "\n",
    "model(pn_history_tensor, feature_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds: List[List[Tuple[int, int]]], truths: List[List[Tuple[int, int]]]):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_micro_f1([[(1,3)]], [[(1,3)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scored_ranges2spans(rnges: Tuple[int, int], scores: int) -> List[Tuple[int, int]]:\n",
    "    thresh = 0.9\n",
    "    spans: List[Tuple[int, int]] = []\n",
    "    active_span_start = None\n",
    "    active_span_end = None\n",
    "    for rng, score in zip(rnges, scores):\n",
    "        if active_span_start is None:\n",
    "            if score > thresh:\n",
    "                active_span_start = rng[0]\n",
    "                active_span_end = rng[1]\n",
    "        else: # prev. words are already part of span\n",
    "            if score > thresh:\n",
    "                active_span_end = rng[1]\n",
    "            else:\n",
    "                spans.append((active_span_start, active_span_end))\n",
    "                active_span_start = None\n",
    "                active_span_end = None\n",
    "    if active_span_start is not None:\n",
    "        spans.append((active_span_start, active_span_end))\n",
    "    return spans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 81.3326 F1: 0.0000 Time elapsed: 7 sec.\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 81.2431 F1: 0.0000 Time elapsed: 14 sec.\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 79.8205 F1: 0.0000 Time elapsed: 20 sec.\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 79.6145 F1: 0.0000 Time elapsed: 26 sec.\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 80.6711 F1: 0.0000 Time elapsed: 32 sec.\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 77.6813 F1: 0.0000 Time elapsed: 38 sec.\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 76.6331 F1: 0.0000 Time elapsed: 44 sec.\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 80.0578 F1: 0.0000 Time elapsed: 50 sec.\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 82.0060 F1: 0.0000 Time elapsed: 56 sec.\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 78.6728 F1: 0.0000 Time elapsed: 62 sec.\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 79.4513 F1: 0.0000 Time elapsed: 68 sec.\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 83.3140 F1: 0.0000 Time elapsed: 73 sec.\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 79.0433 F1: 0.0000 Time elapsed: 80 sec.\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 81.7898 F1: 0.0000 Time elapsed: 86 sec.\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 77.0373 F1: 0.0000 Time elapsed: 92 sec.\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 76.5830 F1: 0.0000 Time elapsed: 98 sec.\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 80.5319 F1: 0.0000 Time elapsed: 104 sec.\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 75.6364 F1: 0.0000 Time elapsed: 109 sec.\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 76.4949 F1: 0.0000 Time elapsed: 115 sec.\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "LSTM output:\n",
      "tensor([0.3823, 0.3661, 0.3581, 0.3536, 0.3528, 0.3504, 0.3497, 0.3483, 0.3461,\n",
      "        0.3455, 0.3470, 0.3458, 0.3484, 0.3484, 0.3491, 0.3501, 0.3490, 0.3483,\n",
      "        0.3489, 0.3492, 0.3473, 0.3469, 0.3486, 0.3490, 0.3508, 0.3489, 0.3513,\n",
      "        0.3490, 0.3495, 0.3507, 0.3507, 0.3524, 0.3498, 0.3488, 0.3518, 0.3495,\n",
      "        0.3493, 0.3483, 0.3507, 0.3501, 0.3479, 0.3455, 0.3467, 0.3479, 0.3472,\n",
      "        0.3483, 0.3492, 0.3503, 0.3487, 0.3477, 0.3478, 0.3499, 0.3496, 0.3484,\n",
      "        0.3475, 0.3469, 0.3482, 0.3482, 0.3492, 0.3501, 0.3481, 0.3478, 0.3475,\n",
      "        0.3463, 0.3477, 0.3491, 0.3500, 0.3502, 0.3507, 0.3488, 0.3481, 0.3487,\n",
      "        0.3475, 0.3448, 0.3472, 0.3470, 0.3488, 0.3492, 0.3493, 0.3483, 0.3506,\n",
      "        0.3502, 0.3491, 0.3488, 0.3481, 0.3495, 0.3481, 0.3494, 0.3465, 0.3482,\n",
      "        0.3464, 0.3477, 0.3454, 0.3464, 0.3453, 0.3476, 0.3455, 0.3452, 0.3477,\n",
      "        0.3441, 0.3446, 0.3470, 0.3472, 0.3496, 0.3499, 0.3493, 0.3481, 0.3480,\n",
      "        0.3456, 0.3475, 0.3483, 0.3493, 0.3464, 0.3458, 0.3477, 0.3455, 0.3477,\n",
      "        0.3463, 0.3479, 0.3455, 0.3460, 0.3476, 0.3453, 0.3448, 0.3448, 0.3471,\n",
      "        0.3481, 0.3459, 0.3473, 0.3469, 0.3474, 0.3469, 0.3460, 0.3429, 0.3434,\n",
      "        0.3437, 0.3476, 0.3478, 0.3497, 0.3508, 0.3487, 0.3491, 0.3488, 0.3477,\n",
      "        0.3476, 0.3476, 0.3484, 0.3472, 0.3463, 0.3477, 0.3454, 0.3481, 0.3487,\n",
      "        0.3503, 0.3492, 0.3467, 0.3507, 0.3477, 0.3486, 0.3467, 0.3476, 0.3484,\n",
      "        0.3482, 0.3478, 0.3511, 0.3490, 0.3498, 0.3475, 0.3487, 0.3489, 0.3477,\n",
      "        0.3489, 0.3484, 0.3488, 0.3476, 0.3495, 0.3497, 0.3511],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "Truth:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "train Loss: 77.5903 F1: 0.0000 Time elapsed: 121 sec.\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 79.8268 F1: 0.0000 Time elapsed: 127 sec.\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 79.4066 F1: 0.0000 Time elapsed: 133 sec.\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 79.5360 F1: 0.0000 Time elapsed: 139 sec.\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 75.2104 F1: 0.0000 Time elapsed: 146 sec.\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 78.6512 F1: 0.0000 Time elapsed: 151 sec.\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 78.9318 F1: 0.0000 Time elapsed: 158 sec.\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 80.5273 F1: 0.0000 Time elapsed: 164 sec.\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 76.7556 F1: 0.0000 Time elapsed: 170 sec.\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 80.8036 F1: 0.0000 Time elapsed: 176 sec.\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 80.7270 F1: 0.0000 Time elapsed: 184 sec.\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 78.8786 F1: 0.0000 Time elapsed: 191 sec.\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 77.8826 F1: 0.0000 Time elapsed: 197 sec.\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 78.4422 F1: 0.0000 Time elapsed: 203 sec.\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 76.9139 F1: 0.0000 Time elapsed: 209 sec.\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 78.8511 F1: 0.0000 Time elapsed: 216 sec.\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 79.2433 F1: 0.0000 Time elapsed: 222 sec.\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 82.1028 F1: 0.0000 Time elapsed: 228 sec.\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 77.8872 F1: 0.0000 Time elapsed: 234 sec.\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 78.2480 F1: 0.0000 Time elapsed: 240 sec.\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "LSTM output:\n",
      "tensor([0.3820, 0.3666, 0.3595, 0.3549, 0.3496, 0.3469, 0.3470, 0.3462, 0.3475,\n",
      "        0.3457, 0.3471, 0.3474, 0.3473, 0.3471, 0.3479, 0.3469, 0.3479, 0.3457,\n",
      "        0.3476, 0.3477, 0.3455, 0.3471, 0.3479, 0.3466, 0.3466, 0.3464, 0.3491,\n",
      "        0.3465, 0.3463, 0.3474, 0.3487, 0.3474, 0.3468, 0.3442, 0.3467, 0.3434,\n",
      "        0.3460, 0.3474, 0.3479, 0.3489, 0.3466, 0.3455, 0.3465, 0.3461, 0.3454,\n",
      "        0.3442, 0.3453, 0.3465, 0.3480, 0.3482, 0.3484, 0.3479, 0.3469, 0.3451,\n",
      "        0.3471, 0.3471, 0.3466, 0.3460, 0.3476, 0.3478, 0.3456, 0.3457, 0.3458,\n",
      "        0.3465, 0.3471, 0.3459, 0.3465, 0.3475, 0.3469, 0.3485, 0.3473, 0.3490,\n",
      "        0.3461, 0.3462, 0.3448, 0.3446, 0.3463, 0.3441, 0.3427, 0.3455, 0.3438,\n",
      "        0.3440, 0.3467, 0.3447, 0.3453, 0.3462, 0.3443, 0.3408, 0.3439, 0.3474,\n",
      "        0.3481, 0.3452, 0.3455, 0.3478, 0.3454, 0.3469, 0.3446, 0.3478, 0.3478,\n",
      "        0.3474, 0.3456, 0.3475, 0.3450, 0.3470, 0.3474, 0.3475, 0.3467, 0.3466,\n",
      "        0.3457, 0.3434, 0.3462, 0.3474, 0.3449, 0.3441, 0.3468, 0.3484, 0.3494,\n",
      "        0.3471, 0.3475, 0.3477, 0.3490, 0.3490, 0.3495, 0.3486, 0.3463, 0.3452,\n",
      "        0.3468, 0.3444, 0.3447, 0.3459, 0.3449, 0.3445, 0.3459, 0.3470, 0.3459,\n",
      "        0.3444, 0.3454, 0.3470, 0.3466, 0.3470, 0.3454, 0.3471, 0.3447, 0.3436,\n",
      "        0.3470, 0.3467, 0.3483, 0.3488, 0.3467, 0.3464, 0.3470, 0.3444, 0.3471,\n",
      "        0.3470, 0.3448, 0.3440, 0.3467, 0.3435, 0.3426, 0.3443, 0.3471, 0.3494,\n",
      "        0.3517, 0.3474, 0.3455, 0.3471, 0.3465, 0.3477, 0.3477, 0.3467, 0.3480,\n",
      "        0.3492, 0.3488, 0.3485, 0.3487, 0.3497, 0.3499, 0.3518, 0.3496, 0.3500,\n",
      "        0.3493, 0.3476, 0.3480, 0.3472, 0.3485, 0.3496, 0.3489, 0.3495, 0.3491,\n",
      "        0.3485, 0.3479, 0.3469, 0.3471, 0.3494], grad_fn=<SigmoidBackward0>)\n",
      "Truth:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "train Loss: 82.6700 F1: 0.0000 Time elapsed: 247 sec.\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 76.3994 F1: 0.0000 Time elapsed: 254 sec.\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 76.5466 F1: 0.0000 Time elapsed: 260 sec.\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 79.7761 F1: 0.0000 Time elapsed: 266 sec.\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 77.7398 F1: 0.0000 Time elapsed: 272 sec.\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 77.1509 F1: 0.0000 Time elapsed: 278 sec.\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 82.0862 F1: 0.0000 Time elapsed: 285 sec.\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 81.0002 F1: 0.0000 Time elapsed: 293 sec.\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 76.8890 F1: 0.0000 Time elapsed: 301 sec.\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 79.7297 F1: 0.0000 Time elapsed: 308 sec.\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 75.8905 F1: 0.0000 Time elapsed: 314 sec.\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 76.7713 F1: 0.0000 Time elapsed: 322 sec.\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 73.2242 F1: 0.0000 Time elapsed: 329 sec.\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 81.4768 F1: 0.0000 Time elapsed: 337 sec.\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 80.0581 F1: 0.0000 Time elapsed: 343 sec.\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 78.5294 F1: 0.0000 Time elapsed: 350 sec.\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 76.2095 F1: 0.0000 Time elapsed: 357 sec.\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 80.6807 F1: 0.0000 Time elapsed: 364 sec.\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 79.9143 F1: 0.0000 Time elapsed: 375 sec.\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 77.9333 F1: 0.0000 Time elapsed: 384 sec.\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "LSTM output:\n",
      "tensor([0.3824, 0.3683, 0.3593, 0.3548, 0.3528, 0.3494, 0.3493, 0.3487, 0.3487,\n",
      "        0.3503, 0.3496, 0.3489, 0.3470, 0.3481, 0.3470, 0.3470, 0.3451, 0.3476,\n",
      "        0.3465, 0.3481, 0.3487, 0.3496, 0.3491, 0.3490, 0.3489, 0.3468, 0.3479,\n",
      "        0.3499, 0.3498, 0.3497, 0.3505, 0.3486, 0.3493, 0.3498, 0.3478, 0.3488,\n",
      "        0.3511, 0.3468, 0.3486, 0.3493, 0.3478, 0.3488, 0.3479, 0.3474, 0.3482,\n",
      "        0.3491, 0.3504, 0.3480, 0.3492, 0.3495, 0.3461, 0.3431, 0.3470, 0.3499,\n",
      "        0.3503, 0.3495, 0.3499, 0.3466, 0.3484, 0.3494, 0.3487, 0.3494, 0.3472,\n",
      "        0.3488, 0.3476, 0.3488, 0.3466, 0.3483, 0.3460, 0.3456, 0.3483, 0.3500,\n",
      "        0.3490, 0.3471, 0.3439, 0.3438, 0.3448, 0.3473, 0.3479, 0.3461, 0.3453,\n",
      "        0.3453, 0.3459, 0.3456, 0.3474, 0.3453, 0.3452, 0.3454, 0.3454, 0.3480,\n",
      "        0.3460, 0.3464, 0.3468, 0.3477, 0.3489, 0.3475, 0.3470, 0.3484, 0.3490,\n",
      "        0.3468, 0.3481, 0.3489, 0.3460, 0.3470, 0.3474, 0.3488, 0.3476, 0.3451,\n",
      "        0.3466, 0.3475, 0.3447, 0.3448, 0.3443, 0.3470, 0.3457, 0.3479, 0.3492,\n",
      "        0.3498, 0.3478, 0.3489, 0.3497, 0.3485, 0.3467, 0.3477, 0.3506, 0.3495,\n",
      "        0.3491, 0.3486, 0.3482, 0.3516, 0.3495, 0.3504, 0.3480, 0.3478, 0.3488,\n",
      "        0.3478, 0.3509, 0.3485, 0.3493, 0.3490, 0.3501, 0.3506, 0.3481, 0.3495,\n",
      "        0.3476, 0.3488, 0.3491, 0.3492, 0.3502, 0.3493, 0.3496, 0.3487, 0.3467,\n",
      "        0.3458, 0.3481, 0.3488, 0.3466, 0.3457, 0.3455, 0.3477, 0.3481, 0.3461,\n",
      "        0.3462, 0.3492, 0.3453, 0.3484, 0.3493, 0.3485, 0.3468, 0.3488, 0.3480,\n",
      "        0.3470, 0.3438, 0.3442, 0.3444, 0.3451, 0.3460, 0.3477, 0.3471, 0.3463,\n",
      "        0.3434, 0.3440, 0.3443, 0.3485, 0.3491, 0.3497, 0.3496, 0.3513, 0.3503,\n",
      "        0.3493, 0.3463, 0.3430, 0.3441, 0.3472, 0.3486, 0.3490, 0.3482, 0.3477,\n",
      "        0.3472, 0.3475, 0.3498, 0.3498, 0.3484, 0.3482, 0.3492, 0.3485, 0.3467,\n",
      "        0.3471, 0.3485, 0.3501, 0.3506, 0.3486, 0.3467, 0.3484, 0.3491, 0.3463,\n",
      "        0.3457, 0.3457, 0.3457, 0.3476, 0.3476, 0.3484, 0.3479, 0.3493, 0.3493,\n",
      "        0.3522, 0.3527], grad_fn=<SigmoidBackward0>)\n",
      "Truth:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "train Loss: 78.4340 F1: 0.0000 Time elapsed: 391 sec.\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 79.3606 F1: 0.0000 Time elapsed: 398 sec.\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 77.7521 F1: 0.0000 Time elapsed: 404 sec.\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 80.3791 F1: 0.0000 Time elapsed: 410 sec.\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 79.3420 F1: 0.0000 Time elapsed: 416 sec.\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 82.1829 F1: 0.0000 Time elapsed: 423 sec.\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 75.7679 F1: 0.0000 Time elapsed: 429 sec.\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 78.2918 F1: 0.0000 Time elapsed: 435 sec.\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 79.0298 F1: 0.0000 Time elapsed: 442 sec.\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 78.7381 F1: 0.0000 Time elapsed: 448 sec.\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 76.2586 F1: 0.0000 Time elapsed: 455 sec.\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 78.6699 F1: 0.0000 Time elapsed: 461 sec.\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 82.4356 F1: 0.0000 Time elapsed: 467 sec.\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 79.5341 F1: 0.0000 Time elapsed: 473 sec.\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 75.3764 F1: 0.0000 Time elapsed: 480 sec.\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 74.7734 F1: 0.0000 Time elapsed: 486 sec.\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 77.5967 F1: 0.0000 Time elapsed: 493 sec.\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 81.9612 F1: 0.0000 Time elapsed: 499 sec.\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 79.8035 F1: 0.0000 Time elapsed: 506 sec.\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 78.2896 F1: 0.0000 Time elapsed: 512 sec.\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "LSTM output:\n",
      "tensor([0.3818, 0.3655, 0.3574, 0.3540, 0.3515, 0.3509, 0.3487, 0.3480, 0.3493,\n",
      "        0.3499, 0.3491, 0.3466, 0.3468, 0.3485, 0.3486, 0.3473, 0.3483, 0.3485,\n",
      "        0.3488, 0.3484, 0.3506, 0.3501, 0.3504, 0.3487, 0.3494, 0.3496, 0.3500,\n",
      "        0.3488, 0.3479, 0.3464, 0.3478, 0.3484, 0.3483, 0.3479, 0.3482, 0.3459,\n",
      "        0.3472, 0.3478, 0.3484, 0.3481, 0.3456, 0.3471, 0.3478, 0.3467, 0.3476,\n",
      "        0.3455, 0.3457, 0.3470, 0.3479, 0.3457, 0.3468, 0.3476, 0.3478, 0.3488,\n",
      "        0.3484, 0.3465, 0.3477, 0.3485, 0.3488, 0.3489, 0.3478, 0.3476, 0.3476,\n",
      "        0.3465, 0.3472, 0.3467, 0.3457, 0.3469, 0.3484, 0.3471, 0.3486, 0.3491,\n",
      "        0.3498, 0.3497, 0.3496, 0.3490, 0.3482, 0.3479, 0.3478, 0.3482, 0.3464,\n",
      "        0.3446, 0.3448, 0.3463, 0.3448, 0.3450, 0.3475, 0.3482, 0.3475, 0.3458,\n",
      "        0.3462, 0.3476, 0.3456, 0.3437, 0.3454, 0.3459, 0.3467, 0.3472, 0.3460,\n",
      "        0.3478, 0.3477, 0.3458, 0.3462, 0.3459, 0.3477, 0.3477, 0.3492, 0.3467,\n",
      "        0.3454, 0.3464, 0.3476, 0.3475, 0.3447, 0.3484, 0.3490, 0.3471, 0.3477,\n",
      "        0.3447, 0.3457, 0.3460, 0.3475, 0.3467, 0.3450, 0.3446, 0.3463, 0.3456,\n",
      "        0.3469, 0.3445, 0.3462, 0.3453, 0.3442, 0.3460, 0.3435, 0.3459, 0.3461,\n",
      "        0.3475, 0.3485, 0.3474, 0.3452, 0.3467, 0.3468, 0.3464, 0.3476, 0.3459,\n",
      "        0.3475, 0.3449, 0.3457, 0.3464, 0.3466, 0.3466, 0.3475, 0.3471, 0.3449,\n",
      "        0.3454, 0.3448, 0.3469, 0.3447, 0.3457, 0.3447, 0.3468, 0.3483, 0.3465,\n",
      "        0.3472, 0.3475, 0.3488, 0.3488, 0.3480, 0.3490, 0.3481, 0.3477, 0.3466,\n",
      "        0.3468, 0.3451, 0.3469, 0.3445, 0.3434, 0.3436, 0.3446, 0.3458, 0.3453,\n",
      "        0.3462, 0.3459, 0.3465, 0.3469, 0.3482, 0.3480, 0.3447, 0.3451, 0.3477,\n",
      "        0.3462, 0.3480, 0.3450, 0.3465, 0.3483, 0.3481, 0.3476, 0.3488, 0.3477,\n",
      "        0.3481, 0.3483, 0.3470, 0.3453, 0.3462, 0.3451, 0.3445, 0.3459, 0.3476,\n",
      "        0.3461, 0.3444, 0.3453, 0.3465, 0.3479, 0.3481, 0.3491, 0.3481, 0.3465,\n",
      "        0.3449, 0.3460, 0.3481, 0.3470, 0.3481, 0.3484, 0.3480, 0.3448, 0.3457,\n",
      "        0.3478, 0.3497, 0.3477, 0.3475, 0.3471, 0.3487, 0.3485, 0.3472, 0.3483,\n",
      "        0.3481, 0.3461, 0.3478, 0.3467, 0.3486, 0.3496, 0.3463, 0.3465, 0.3469,\n",
      "        0.3478, 0.3484, 0.3502], grad_fn=<SigmoidBackward0>)\n",
      "Truth:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "train Loss: 78.7467 F1: 0.0000 Time elapsed: 519 sec.\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 81.5877 F1: 0.0000 Time elapsed: 526 sec.\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 79.1135 F1: 0.0000 Time elapsed: 532 sec.\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 78.8094 F1: 0.0000 Time elapsed: 539 sec.\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 75.2207 F1: 0.0000 Time elapsed: 545 sec.\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 78.9113 F1: 0.0000 Time elapsed: 551 sec.\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 78.4369 F1: 0.0000 Time elapsed: 557 sec.\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 79.9683 F1: 0.0000 Time elapsed: 563 sec.\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 80.9577 F1: 0.0000 Time elapsed: 570 sec.\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 77.7145 F1: 0.0000 Time elapsed: 577 sec.\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 79.1022 F1: 0.0000 Time elapsed: 583 sec.\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 76.0163 F1: 0.0000 Time elapsed: 591 sec.\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 77.4206 F1: 0.0000 Time elapsed: 597 sec.\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 76.6840 F1: 0.0000 Time elapsed: 604 sec.\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 76.2065 F1: 0.0000 Time elapsed: 613 sec.\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 75.3696 F1: 0.0000 Time elapsed: 621 sec.\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 82.4041 F1: 0.0000 Time elapsed: 629 sec.\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 79.8974 F1: 0.0000 Time elapsed: 635 sec.\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 79.2234 F1: 0.0000 Time elapsed: 643 sec.\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 74.8643 F1: 0.0000 Time elapsed: 649 sec.\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "LSTM output:\n",
      "tensor([0.3824, 0.3664, 0.3591, 0.3555, 0.3539, 0.3512, 0.3510, 0.3482, 0.3488,\n",
      "        0.3493, 0.3492, 0.3501, 0.3487, 0.3516, 0.3507, 0.3501, 0.3508, 0.3507,\n",
      "        0.3508, 0.3506, 0.3470, 0.3464, 0.3473, 0.3480, 0.3471, 0.3482, 0.3493,\n",
      "        0.3497, 0.3501, 0.3463, 0.3461, 0.3467, 0.3503, 0.3504, 0.3495, 0.3500,\n",
      "        0.3477, 0.3488, 0.3480, 0.3466, 0.3488, 0.3502, 0.3496, 0.3471, 0.3459,\n",
      "        0.3457, 0.3474, 0.3461, 0.3456, 0.3470, 0.3481, 0.3467, 0.3478, 0.3470,\n",
      "        0.3501, 0.3504, 0.3474, 0.3489, 0.3460, 0.3481, 0.3497, 0.3504, 0.3522,\n",
      "        0.3514, 0.3501, 0.3494, 0.3484, 0.3494, 0.3474, 0.3482, 0.3478, 0.3468,\n",
      "        0.3476, 0.3502, 0.3490, 0.3488, 0.3486, 0.3498, 0.3490, 0.3502, 0.3495,\n",
      "        0.3499, 0.3495, 0.3481, 0.3494, 0.3467, 0.3455, 0.3456, 0.3482, 0.3513,\n",
      "        0.3510, 0.3514, 0.3511, 0.3504, 0.3497, 0.3477, 0.3480, 0.3496, 0.3498,\n",
      "        0.3501, 0.3498, 0.3487, 0.3487, 0.3458, 0.3449, 0.3449, 0.3468, 0.3478,\n",
      "        0.3460, 0.3459, 0.3464, 0.3474, 0.3451, 0.3463, 0.3454, 0.3451, 0.3472,\n",
      "        0.3451, 0.3478, 0.3482, 0.3493, 0.3467, 0.3434, 0.3444, 0.3472, 0.3487,\n",
      "        0.3480, 0.3476, 0.3471, 0.3475, 0.3498, 0.3492, 0.3490, 0.3480, 0.3477,\n",
      "        0.3492, 0.3488, 0.3495, 0.3487, 0.3477, 0.3482, 0.3475, 0.3490, 0.3523,\n",
      "        0.3514, 0.3499, 0.3482, 0.3484, 0.3492, 0.3484, 0.3461, 0.3472, 0.3458,\n",
      "        0.3472, 0.3485, 0.3484, 0.3497, 0.3478, 0.3484, 0.3474, 0.3475, 0.3498,\n",
      "        0.3500, 0.3470, 0.3439, 0.3436, 0.3483, 0.3495, 0.3484, 0.3470, 0.3485,\n",
      "        0.3484, 0.3473, 0.3487, 0.3457, 0.3477, 0.3468, 0.3444, 0.3466, 0.3472,\n",
      "        0.3444, 0.3445, 0.3471, 0.3499], grad_fn=<SigmoidBackward0>)\n",
      "Truth:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "train Loss: 76.8938 F1: 0.0000 Time elapsed: 655 sec.\n",
      "\n",
      "Training complete in 10m 55s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "logfile_name = \"training_log.txt\"\n",
    "\n",
    "def log(logtext: str = \"\") -> None:\n",
    "    print(logtext)\n",
    "    with open(logfile_name, \"a\", encoding=\"utf8\") as f:\n",
    "        f.write(str(logtext) + \"\\n\")\n",
    "    \n",
    "\n",
    "def train_model(model: LSTMTokenScorer, criterion, optimizer, scheduler, num_epochs=1):\n",
    "    since = time.time()\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        best_loss = 9999999999999\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            log(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            log('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train']: #, 'test'\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else: \n",
    "                    model.eval()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_f1_average = 0\n",
    "\n",
    "                batch = random.choices(list(train_data_preprocessed.values()), k=64)\n",
    "\n",
    "                # Iterate over data.\n",
    "                for i, datum_preprocessed in enumerate(batch):\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    pn_history_tokens = datum_preprocessed[\"pn_history_tokens\"]\n",
    "                    scores = datum_preprocessed[\"scores\"]\n",
    "                    feature_tokens = datum_preprocessed[\"feature_tokens\"]\n",
    "\n",
    "                    feature_tensor = torch.tensor(np.array([t[\"embedded\"] for t in feature_tokens]), dtype=torch.float)\n",
    "                    pn_history_tensor = torch.tensor(np.array([t[\"embedded\"] for t in pn_history_tokens]), dtype=torch.float)\n",
    "\n",
    "                    # track history only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(pn_history_tensor, feature_tensor)\n",
    "                        \n",
    "                        loss = criterion(outputs.float(), scores.float())\n",
    "\n",
    "                        # pred = (outputs > 0.5).int()\n",
    "                        # print(scores.float())\n",
    "                        # print(outputs.detach().float())\n",
    "                        try:\n",
    "                            f1 = f1_score(scores.int(), outputs.detach().int())\n",
    "                        except Exception as e:\n",
    "                            log(\"F1 score calc failed:\")\n",
    "                            log(\"Scores:\")\n",
    "                            log(scores.int())\n",
    "                            log(\"\\nOutputs\")\n",
    "                            log(outputs.detach().int())\n",
    "                            log(\"\\n\")\n",
    "                            raise Exception(e)\n",
    "                        \n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item()\n",
    "                    # if torch.equal(pred, scores):\n",
    "                    running_f1_average = ((running_f1_average * i) + f1) / (i+1)\n",
    "                    \n",
    "                    if i == len(batch) - 1 and epoch % 20 == 19:\n",
    "                        log(\"LSTM output:\")\n",
    "                        log(outputs)\n",
    "                        log(\"Truth:\")\n",
    "                        log(scores)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss # / dataset_sizes[phase]\n",
    "                epoch_f1 = running_f1_average # / dataset_sizes[phase]\n",
    "                log(f'{phase} Loss: {epoch_loss:.4f} F1: {epoch_f1:.4f} Time elapsed: {round((time.time() - since))} sec.')\n",
    "                \n",
    "                # deep copy the model\n",
    "                if phase == 'test' and epoch_loss < best_loss: #epoch_acc > best_acc:\n",
    "                    best_acc = epoch_f1\n",
    "                    best_loss = epoch_loss\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            log()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        log(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        log(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model\n",
    "\n",
    "open(logfile_name, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "model = train_model(model, loss_function, optimizer, exp_lr_scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0, 0, 0], dtype=torch.float)\n",
    "b = torch.tensor([0, 0, 1], dtype=torch.float)\n",
    "c = torch.tensor([1, 1, 0], dtype=torch.float)\n",
    "d = torch.tensor([0, 1, 0], dtype=torch.float)\n",
    "\n",
    "nn.functional.binary_cross_entropy_with_logits(a, b, pos_weight=torch.full((3,), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
