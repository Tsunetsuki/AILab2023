{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# others\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import time\n",
    "\n",
    "# dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import Flowers102\n",
    "\n",
    "# read file \n",
    "import pandas as pd\n",
    "\n",
    "# label\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use TweetEval emotion recognition dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '../../Data/tweeteval/datasets/emotion/'\n",
    "# mapping_file = os.path.join(root, 'mapping.txt')\n",
    "# test_labels_file = os.path.join(root, 'test_labels.txt')\n",
    "# test_text_file = os.path.join(root, 'test_text.txt')\n",
    "# train_labels_file = os.path.join(root, 'train_labels.txt')\n",
    "# train_text_file = os.path.join(root, 'train_text.txt')\n",
    "# val_labels_file = os.path.join(root, 'val_labels.txt')\n",
    "# val_text_file = os.path.join(root, 'val_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_pd = pd.read_csv(mapping_file, sep='\\t', header=None)\n",
    "# test_label_pd = pd.read_csv(test_labels_file, sep='\\t', header=None)\n",
    "# test_dataset = open(test_text_file).read().split('\\n')[:-1] # remove last empty line \n",
    "# train_label_pd = pd.read_csv(train_labels_file, sep='\\t', header=None)\n",
    "# train_dataset = open(train_text_file).read().split('\\n')[:-1] # remove last empty line\n",
    "# val_label_pd = pd.read_csv(val_labels_file, sep='\\t', header=None)\n",
    "# val_dataset = open(val_text_file).read().split('\\n')[:-1] # remove last empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training data\n",
    "- Given: Notes with ranges and labels\n",
    "- Transform into label + lists of tokens with [does token describe label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data/'\n",
    "features_path = os.path.join(root, 'features.csv')\n",
    "patient_notes_path = os.path.join(root, 'patient_notes.csv')\n",
    "sample_submission_path = os.path.join(root, 'sample_submission.csv')\n",
    "test_path = os.path.join(root, 'test.csv')\n",
    "train_path = os.path.join(root, 'train.csv')\n",
    "features = pd.read_csv(features_path, sep=',', header=0)\n",
    "patient_notes = pd.read_csv(patient_notes_path, sep=',', header=0)\n",
    "train_raw = pd.read_csv(train_path, sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>208</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>407</td>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>501</td>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>802</td>\n",
       "      <td>8</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>913</td>\n",
       "      <td>9</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_num  case_num feature_text\n",
       "25           112         1       Female\n",
       "34           208         2       Female\n",
       "66           407         4       Female\n",
       "70           501         5       Female\n",
       "99           700         7       Female\n",
       "110          802         8       Female\n",
       "139          913         9       Female"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unusual_numbers = features[\"feature_num\"].value_counts()[features[\"feature_num\"].value_counts() != 1]\n",
    "# unusual_numbers\n",
    "features[features[\"feature_text\"] == \"Female\"]\n",
    "# features[\"feature_num\"] == "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intro \n",
    "- `case_num`: 0~9, each num belongs their groups ... ? \n",
    "- `pn_num`: the id in patient_notes.csv which is 'pn_history', present the note of each case \n",
    "- `feature_num`: the id in features.csv which is 'feature_num', present the feature of each case \n",
    "- `location`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def df_string2list_of_ints(df_string: str):\n",
    "    df_string = df_string.strip(\"[]\")\n",
    "    if df_string == \"\":\n",
    "        return []\n",
    "    entries = re.split(\",|;\", df_string)\n",
    "    entries = [entry.strip(\" '\") for entry in entries]\n",
    "    ranges = [tuple(int(num_as_str) for num_as_str in entry.split(\" \")) for entry in entries]\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>['203 217']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>['70 91', '176 183']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>['222 258']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14295</th>\n",
       "      <td>95333_912</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>912</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14296</th>\n",
       "      <td>95333_913</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>913</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14297</th>\n",
       "      <td>95333_914</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>914</td>\n",
       "      <td>['photobia']</td>\n",
       "      <td>['274 282']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14298</th>\n",
       "      <td>95333_915</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>915</td>\n",
       "      <td>['no sick contacts']</td>\n",
       "      <td>['421 437']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14299</th>\n",
       "      <td>95333_916</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>916</td>\n",
       "      <td>['Subjective fever']</td>\n",
       "      <td>['314 330']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  case_num  pn_num  feature_num  \\\n",
       "0      00016_000         0      16            0   \n",
       "1      00016_001         0      16            1   \n",
       "2      00016_002         0      16            2   \n",
       "3      00016_003         0      16            3   \n",
       "4      00016_004         0      16            4   \n",
       "...          ...       ...     ...          ...   \n",
       "14295  95333_912         9   95333          912   \n",
       "14296  95333_913         9   95333          913   \n",
       "14297  95333_914         9   95333          914   \n",
       "14298  95333_915         9   95333          915   \n",
       "14299  95333_916         9   95333          916   \n",
       "\n",
       "                                     annotation              location  \n",
       "0              ['dad with recent heart attcak']           ['696 724']  \n",
       "1                 ['mom with \"thyroid disease']           ['668 693']  \n",
       "2                            ['chest pressure']           ['203 217']  \n",
       "3          ['intermittent episodes', 'episode']  ['70 91', '176 183']  \n",
       "4      ['felt as if he were going to pass out']           ['222 258']  \n",
       "...                                         ...                   ...  \n",
       "14295                                        []                    []  \n",
       "14296                                        []                    []  \n",
       "14297                              ['photobia']           ['274 282']  \n",
       "14298                      ['no sick contacts']           ['421 437']  \n",
       "14299                      ['Subjective fever']           ['314 330']  \n",
       "\n",
       "[14300 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                                 annotation                location  \\\n",
       "0          ['dad with recent heart attcak']            [(696, 724)]   \n",
       "1             ['mom with \"thyroid disease']            [(668, 693)]   \n",
       "2                        ['chest pressure']            [(203, 217)]   \n",
       "3      ['intermittent episodes', 'episode']  [(70, 91), (176, 183)]   \n",
       "4  ['felt as if he were going to pass out']            [(222, 258)]   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = train_raw.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "data_merged = data_merged.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "data_merged[\"location\"] = data_merged[\"location\"].apply(df_string2list_of_ints)\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history                location  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...            [(696, 724)]  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...            [(668, 693)]  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...            [(203, 217)]  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  [(70, 91), (176, 183)]  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...            [(222, 258)]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_merged[[\"feature_text\", \"pn_history\", \"location\", ]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter training data with no location\n",
    "train = train[train[\"location\"].apply(lambda row: len(row) != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset= 9901\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of dataset= {len(train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "- Use spaCy to split the notes into words.\n",
    "\n",
    "Before start using spaCy\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from collections import Counter\n",
    "\n",
    "# use spacy to tokenize the sentence with english model \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable\n",
    "\n",
    "def build_vocab_from_lines(lines: Iterable[str]):\n",
    "    text_to_count_tokens = ' '.join(lines)\n",
    "    doc = nlp(text_to_count_tokens)\n",
    "    # Get the most frequent words, filtering out stop words and punctuation.\n",
    "    word_freq = Counter(token.text.lower() for token in doc if \\\n",
    "                        not token.is_punct and \\\n",
    "                            not token.is_stop and \\\n",
    "                                not token.is_space)\n",
    "    return word_freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no cached vocabulary. Creating...\n",
      "Top 10 words:  pain, 2, denies, ago, 3, pmh, months, changes, 4, use\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary by getting the most common words across (unique) patient histories\n",
    "import pickle\n",
    "import os\n",
    "from os.path import join as pathjoin\n",
    "\n",
    "cache_dir = \"cache\"\n",
    "cache_file = pathjoin(cache_dir, \"vocab.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        pn_history_vocab = pickle.load(f)\n",
    "    print(\"Vocabulary loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached vocabulary. Creating...\")\n",
    "    \n",
    "    most_common_words = build_vocab_from_lines(train[\"pn_history\"].drop_duplicates())[:5000]\n",
    "    \n",
    "    pn_history_vocab = {word[0]: idx for idx, word in enumerate(most_common_words)}\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(pn_history_vocab, f)\n",
    "\n",
    "print(\"Top 10 words: \", \", \".join(list(pn_history_vocab)[:10]))\n",
    "# [(k, v) for k, v in vocab.items() if v == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no cached feature vocabulary. Creating...\n",
      "Top 10 words:  symptoms, ago, year, history, use, months, pain, family, recent, irregular\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary by getting the most common words across (unique) patient histories\n",
    "import pickle\n",
    "import os\n",
    "from os.path import join as pathjoin\n",
    "\n",
    "cache_dir = \"cache\"\n",
    "cache_file = pathjoin(cache_dir, \"feature_vocab.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        feature_vocab = pickle.load(f)\n",
    "    print(\"Feature vocabulary loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached feature vocabulary. Creating...\")\n",
    "    \n",
    "    most_common_words = build_vocab_from_lines(train[\"feature_text\"].drop_duplicates())[:5000]\n",
    "    \n",
    "    feature_vocab = {word[0]: idx for idx, word in enumerate(most_common_words)}\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(feature_vocab, f)\n",
    "\n",
    "print(\"Top 10 words: \", \", \".join(list(feature_vocab)[:10]))\n",
    "# [(k, v) for k, v in vocab.items() if v == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no cached tokenized patient histories. Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9901 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9901/9901 [00:25<00:00, 387.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "placeholder_index = 5000\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_pn_histories.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_pn_histories = pickle.load(f)\n",
    "    print(\"Tokenized patient histories loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized patient histories. Tokenizing...\")\n",
    "    tokenized_pn_histories: Dict[str, List[str]] = {}\n",
    "    for pn_history in tqdm(train[\"pn_history\"]):\n",
    "        indexed_words = []\n",
    "        if pn_history in tokenized_pn_histories:\n",
    "            continue\n",
    "        for token in nlp(pn_history):\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "                word = token.text.lower()\n",
    "                start_idx = token.idx\n",
    "                end_idx = token.idx + len(token.text)\n",
    "\n",
    "                word_as_number = pn_history_vocab[word] if word in pn_history_vocab else placeholder_index\n",
    "                \n",
    "                indexed_words.append({\n",
    "                    \"word_idx\": word_as_number,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": end_idx\n",
    "                })\n",
    "                    \n",
    "        tokenized_pn_histories[pn_history] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_pn_histories, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no cached tokenized features. Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9901/9901 [00:00<00:00, 10313.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "placeholder_index = len(feature_vocab)\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_features.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_features = pickle.load(f)\n",
    "    print(\"Tokenized features loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized features. Tokenizing...\")\n",
    "    tokenized_features: Dict[str, List[str]] = {}\n",
    "    for feature_text in tqdm(train[\"feature_text\"]):\n",
    "        indexed_words = []\n",
    "        if feature_text in tokenized_features:\n",
    "            continue\n",
    "        for token in nlp(feature_text):\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "                word = token.text.lower()\n",
    "                word_as_number = feature_vocab[word] if word in feature_vocab else placeholder_index\n",
    "                \n",
    "                indexed_words.append(word_as_number)\n",
    "                    \n",
    "        tokenized_features[feature_text] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_features, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Family-history-of-MI-OR-Family-history-of-myocardial-infarction': [7,\n",
       "  3,\n",
       "  70,\n",
       "  7,\n",
       "  3,\n",
       "  71,\n",
       "  72],\n",
       " 'Family-history-of-thyroid-disorder': [7, 3, 31, 73],\n",
       " 'Chest-pressure': [15, 74],\n",
       " 'Intermittent-symptoms': [32, 0],\n",
       " 'Lightheaded': [75],\n",
       " 'Adderall-use': [76, 4],\n",
       " 'heart-pounding-OR-heart-racing': [16, 77, 16, 33],\n",
       " 'Few-months-duration': [5, 11],\n",
       " '17-year': [78, 2],\n",
       " 'Male': [79],\n",
       " 'Shortness-of-breath': [17, 12],\n",
       " 'No-hair-changes-OR-no-nail-changes-OR-no-temperature-intolerance': [80,\n",
       "  34,\n",
       "  81,\n",
       "  34,\n",
       "  82,\n",
       "  83],\n",
       " 'Caffeine-use': [18, 4],\n",
       " 'No-vaginal-discharge': [35, 84],\n",
       " 'Not-sexually-active': [36, 37],\n",
       " '20-year': [85, 2],\n",
       " 'Recurrent-bouts-over-past-6-months': [86, 87, 88, 5],\n",
       " 'Right-sided-LQ-abdominal-pain-OR-Right-lower-quadrant-abdominal-pain': [38,\n",
       "  89,\n",
       "  90,\n",
       "  39,\n",
       "  6,\n",
       "  38,\n",
       "  91,\n",
       "  92,\n",
       "  39,\n",
       "  6],\n",
       " 'No-urinary-symptoms': [93, 0],\n",
       " 'Normal-LMP-2-weeks-ago-OR-Normal-last-menstrual-period-2-weeks-ago': [19,\n",
       "  40,\n",
       "  13,\n",
       "  1,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  13,\n",
       "  1],\n",
       " '8-to-10-hours-of-acute-pain': [94, 95, 96, 97, 6],\n",
       " 'Female': [98],\n",
       " 'Prior-episodes-of-diarrhea': [41, 22, 99],\n",
       " 'No-bloody-bowel-movements': [100, 42, 43],\n",
       " 'Diminished-appetite': [44, 23],\n",
       " 'Weight-loss': [24, 45],\n",
       " 'Vaginal-dryness': [35, 101],\n",
       " 'Irregular-menses': [9, 102],\n",
       " 'Recent-nausea-vomiting-OR-Recent-flulike-symptoms': [8, 25, 46, 8, 103, 0],\n",
       " 'LMP-2-months-ago-or-Last-menstrual-period-2-months-ago': [40,\n",
       "  5,\n",
       "  1,\n",
       "  20,\n",
       "  21,\n",
       "  5,\n",
       "  1],\n",
       " 'Hot-flashes': [47, 104],\n",
       " 'Irregular-flow-OR-Irregular-frequency-OR-Irregular-intervals': [9,\n",
       "  105,\n",
       "  9,\n",
       "  48,\n",
       "  9,\n",
       "  106],\n",
       " 'Onset-3-years-ago': [107, 49, 1],\n",
       " 'Heavy-sweating': [14, 108],\n",
       " '44-year': [109, 2],\n",
       " 'Sexually-active': [36, 37],\n",
       " 'Last-Pap-smear-I-year-ago': [110, 111, 2, 1],\n",
       " 'Prior-normal-periods': [41, 19, 26],\n",
       " 'IUD': [112],\n",
       " 'Sleep-disturbance-OR-Early-awakenings': [50, 113, 51, 114],\n",
       " 'No-premenstrual-symptoms': [115, 0],\n",
       " 'Stress': [27],\n",
       " 'FHx-of-PUD-OR-Family-history-of-peptic-ulcer-disease': [52,\n",
       "  116,\n",
       "  7,\n",
       "  3,\n",
       "  117,\n",
       "  118,\n",
       "  119],\n",
       " 'Epigastric-discomfort': [120, 121],\n",
       " 'Darker-bowel-movements': [122, 42, 43],\n",
       " 'NSAID-use-OR-Nonsteroidal-anti-inflammatory-drug-use': [123,\n",
       "  4,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  53,\n",
       "  4],\n",
       " 'burning-OR-gnawing-OR-burning-and-gnawing': [54, 55, 54, 55],\n",
       " 'getting-worse-OR-progressive-OR-symptoms-now-daily': [127, 56, 128, 0, 129],\n",
       " '2-to-3-beers-a-week': [130, 131, 132, 133],\n",
       " 'Intermittent': [32],\n",
       " 'Minimal-to-no-change-with-Tums': [134, 135, 136],\n",
       " 'Nausea': [25],\n",
       " '35-year': [137, 2],\n",
       " 'Post-prandial-bloating-OR-fullness-with-meals': [138, 139, 140, 141, 142],\n",
       " 'duration-2-months': [143, 5],\n",
       " 'Awakens-at-night': [144, 145],\n",
       " 'No-blood-in-stool': [146, 147],\n",
       " 'Lack-of-other-thyroid-symptoms': [148, 31, 0],\n",
       " 'anxious-OR-nervous': [149, 150],\n",
       " 'No-depressed-mood': [151, 152],\n",
       " 'Weight-stable': [24, 153],\n",
       " 'Insomnia': [154],\n",
       " '45-year': [155, 2],\n",
       " 'Stress-due-to-caring-for-elderly-parents': [27, 156, 157, 158],\n",
       " 'Decreased-appetite': [159, 23],\n",
       " 'Heavy-caffeine-use': [14, 18, 4],\n",
       " 'Associated-SOB-OR-Associated-shortness-of-breath': [10, 160, 10, 17, 12],\n",
       " 'Episodes-of-heart-racing': [22, 16, 33],\n",
       " 'Recent-visit-to-emergency-department-with-negative-workup': [8,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165],\n",
       " 'No-chest-pain': [15, 6],\n",
       " 'No-illicit-drug-use': [166, 53, 4],\n",
       " 'Increased-frequency-recently': [28, 48, 167],\n",
       " 'Episodes-last-15-to-30-minutes': [22, 168, 169, 170],\n",
       " 'Feels-hot-OR-Feels-clammy': [57, 47, 57, 171],\n",
       " 'Episode-of-hand-numbness-OR-Episode-of-finger-numbness': [58,\n",
       "  172,\n",
       "  59,\n",
       "  58,\n",
       "  173,\n",
       "  59],\n",
       " 'Increased-stress': [28, 27],\n",
       " '26-year': [174, 2],\n",
       " 'Onset-5-years-ago': [175, 49, 1],\n",
       " 'Associated-feeling-of-impending-doom': [10, 60, 176, 177],\n",
       " 'Fatigue-OR-Difficulty-concentrating': [61, 29, 178],\n",
       " 'No-caffeine-use': [18, 4],\n",
       " 'Associated-nausea': [10, 25],\n",
       " 'Associated-throat-tightness': [10, 62, 179],\n",
       " 'Subjective-fevers': [63, 180],\n",
       " 'Recent-upper-respiratory-symptoms': [8, 181, 182, 0],\n",
       " 'Worse-with-deep-breath-OR-pleuritic': [56, 183, 12, 184],\n",
       " 'Exercise-induced-asthma': [185, 186, 64],\n",
       " 'Chest-pain': [15, 6],\n",
       " 'Duration-x-1-day': [11, 187, 65],\n",
       " 'No-shortness-of-breath': [17, 12],\n",
       " 'No-relief-with-asthma-inhaler': [30, 64, 188],\n",
       " 'Sharp-OR-stabbing-OR-7-to-8-out-of-10-on-pain-scale': [189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  6,\n",
       "  194],\n",
       " 'Recent-heavy-lifting-at-work-OR-recent-rock-climbing': [8,\n",
       "  14,\n",
       "  195,\n",
       "  196,\n",
       "  8,\n",
       "  197,\n",
       "  198],\n",
       " 'heavy-periods-OR-irregular-periods': [14, 26, 9, 26],\n",
       " 'Last-menstrual-period-2-months-ago': [20, 21, 5, 1],\n",
       " 'Unprotected-Sex': [199, 200],\n",
       " 'symptoms-for-6-months': [0, 201, 5],\n",
       " 'Weight-Gain': [24, 202],\n",
       " 'Fatigue': [61],\n",
       " 'Infertility-HX-OR-Infertility-history': [66, 203, 66, 3],\n",
       " 'Increased-appetite': [28, 23],\n",
       " 'Son-died-3-weeks-ago': [204, 205, 13, 1],\n",
       " 'tossing-and-turning': [206, 207],\n",
       " '67-year': [208, 2],\n",
       " 'Difficulty-falling-asleep': [29, 209, 210],\n",
       " 'duration-3-weeks': [211, 13],\n",
       " 'Sleeping-medication-ineffective': [212, 213, 214],\n",
       " 'Diminished-energy-OR-feeling-drained': [44, 215, 60, 216],\n",
       " 'loss-of-interest': [45, 217],\n",
       " 'Visual-hallucination-once': [218, 67],\n",
       " 'Early-wakening': [51, 219],\n",
       " 'No-suicidal-ideations': [220, 221],\n",
       " 'Difficulty-with-sleep': [29, 50],\n",
       " 'FHx-of-depression-OR-Family-history-of-depression': [52, 68, 7, 3, 68],\n",
       " 'Auditory-hallucination-once': [222, 67],\n",
       " 'Unsuccessful-napping': [223, 224],\n",
       " 'Hallucinations-after-taking-Ambien': [225, 226, 227],\n",
       " 'No-relief-with-Motrin-OR-no-relief-with-tylenol': [30, 228, 30, 229],\n",
       " '1-day-duration-OR-2-days-duration': [230, 65, 11, 231, 232, 11],\n",
       " 'Vomiting': [46],\n",
       " 'Shares-an-apartment': [233, 234],\n",
       " 'Photophobia': [235],\n",
       " 'No-known-illness-contacts': [236, 237, 238],\n",
       " 'Subjective-fever': [63, 239],\n",
       " 'viral-symptoms-OR-rhinorrhea-OR-scratchy-throat': [240, 0, 241, 242, 62],\n",
       " 'Family-history-of-migraines': [7, 3, 243],\n",
       " 'Myalgias': [244],\n",
       " 'Global-headache-OR-diffuse-headache': [245, 69, 246, 69],\n",
       " 'No-rash': [247],\n",
       " 'Neck-pain': [248, 6],\n",
       " 'Meningococcal-vaccine-status-unknown': [249, 250, 251, 252]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follow the example described here. Use the same architecture, but:\n",
    "  - only use the last output of the LSTM in the loss function\n",
    "  - use an embedding dim of 128\n",
    "  - use a hidden dim of 256.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature-relevancy of tokens via char ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9901it [00:03, 2538.07it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_preprocessed = dict()\n",
    "for i, (feature_text, pn_history, location) in tqdm(train.iterrows()):\n",
    "    tokenized_history = tokenized_pn_histories[pn_history]\n",
    "    tokens_with_scores = []\n",
    "    for token in tokenized_history:\n",
    "        for feature_relevant_range in location:\n",
    "            token_start, token_end = token[\"start\"], token[\"end\"]\n",
    "            range_start, range_end = feature_relevant_range[0], feature_relevant_range[1]\n",
    "            \n",
    "            percentage_of_token_in_range = max(min(token_end, range_end)+1 - max(token_start, range_start), 0) / (token_end+1 - token_start)\n",
    "            # if percentage_of_token_in_range > 0:\n",
    "            #     print(percentage_of_token_in_range, token, feature_relevant_range)\n",
    "            tokens_with_scores.append({\"word\": token[\"word_idx\"], \"score\": int(percentage_of_token_in_range > 0.9)})\n",
    "    \n",
    "    train_data_preprocessed[i] = {\n",
    "                                    \"scored_tokens\": tokens_with_scores,\n",
    "                                    \"feature_tokens\": tokenized_features[feature_text]\n",
    "                                   }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 70, 7, 3, 71, 72]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"data format:\")\n",
    "# train_data_preprocessed[0].keys()\n",
    "train_data_preprocessed[0][\"feature_tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO Bring feature label into training data for LSTM!\n",
    "- must encode the feature text into the LSTM input data to train. How to do it?\n",
    "- 2 vocabs\n",
    "\n",
    "Layers in LSTM Model:\n",
    "1. embed feature tokens\n",
    "2. lstm feature -> constant size vector\n",
    "\n",
    "3. pass to 2nd lstm\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([107, 256])\n",
      "torch.Size([107, 256])\n",
      "torch.Size([107])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5014, 0.5105, 0.5071, 0.5105, 0.5323, 0.5162, 0.4995, 0.5367, 0.5266,\n",
       "        0.5095, 0.5028, 0.4818, 0.4828, 0.5033, 0.5186, 0.5095, 0.5109, 0.5034,\n",
       "        0.5127, 0.5118, 0.4902, 0.4751, 0.4797, 0.4935, 0.5050, 0.5163, 0.5069,\n",
       "        0.5152, 0.5110, 0.5332, 0.5232, 0.5181, 0.5392, 0.5083, 0.5222, 0.5075,\n",
       "        0.4836, 0.4921, 0.5332, 0.4921, 0.4878, 0.5184, 0.5092, 0.4917, 0.4806,\n",
       "        0.4667, 0.4855, 0.4982, 0.4869, 0.4797, 0.4967, 0.5022, 0.4671, 0.4767,\n",
       "        0.4784, 0.5073, 0.5139, 0.5001, 0.4961, 0.4930, 0.4729, 0.4856, 0.5171,\n",
       "        0.5032, 0.4991, 0.5205, 0.4938, 0.5067, 0.5076, 0.5240, 0.5043, 0.4922,\n",
       "        0.4686, 0.4819, 0.4888, 0.4945, 0.4631, 0.4911, 0.5203, 0.5070, 0.5049,\n",
       "        0.5200, 0.4939, 0.4839, 0.4629, 0.5073, 0.5268, 0.5432, 0.5292, 0.4912,\n",
       "        0.5311, 0.5200, 0.4951, 0.4980, 0.4738, 0.4842, 0.5143, 0.4822, 0.5070,\n",
       "        0.4914, 0.4882, 0.5058, 0.4945, 0.4993, 0.5078, 0.5015, 0.4955],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTMTokenScorer(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, pn_hist_vocab_size, feature_vocab_size, dropout=0.0):\n",
    "        super(LSTMTokenScorer, self).__init__()\n",
    "\n",
    "        self.pn_history_hidden_dim = hidden_dim\n",
    "\n",
    "        self.feature_embeddings = nn.Embedding(feature_vocab_size, embedding_dim)\n",
    "        self.feature_lstm = nn.LSTM(embedding_dim, embedding_dim, dropout=dropout) # the feature is now one tensor of size [embedding_dim].\n",
    "\n",
    "        self.pn_history_embeddings = nn.Embedding(pn_hist_vocab_size, embedding_dim)\n",
    "        \n",
    "        self.total_lstm = nn.LSTM(embedding_dim * 2, self.pn_history_hidden_dim, dropout=dropout)\n",
    "        \n",
    "        self.hidden2score = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, pn_history, feature):\n",
    "        feature_embeds = self.feature_embeddings(feature)\n",
    "        feature_lstm_out, _ = self.feature_lstm(feature_embeds.view(len(feature), 1, -1)) # the feature is now one tensor of size [embedding_dim].\n",
    "        feature_reduced = feature_lstm_out[-1][0] #.view(1, -1)\n",
    "        \n",
    "        feature_multiplied = feature_reduced.repeat((len(pn_history), 1)) # duplicate feature vector to be same size as embedded pn_history vector.\n",
    "\n",
    "        pn_history_embeds = self.pn_history_embeddings(pn_history)\n",
    "        pn_history_and_features = torch.concat((feature_multiplied, pn_history_embeds), dim=1)\n",
    "        print(pn_history_and_features.size())\n",
    "\n",
    "        pn_history_reduced, _ = self.total_lstm(pn_history_and_features)\n",
    "        print(pn_history_reduced.size())\n",
    "        pred_score_raw = torch.squeeze(self.hidden2score(pn_history_reduced))\n",
    "        pred_score = self.sigmoid(pred_score_raw)\n",
    "        print(pred_score.size())\n",
    "        return pred_score\n",
    "\n",
    "# make model with vocab sizes, including placeholder indices\n",
    "model = LSTMTokenScorer(EMBEDDING_DIM, HIDDEN_DIM, len(pn_history_vocab)+1, len(feature_vocab)+1)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "\n",
    "feature_tokens = train_data_preprocessed[0][\"feature_tokens\"]\n",
    "scored_tokens = train_data_preprocessed[0][\"scored_tokens\"]\n",
    "pn_history_tokens = [st[\"word\"] for st in scored_tokens]\n",
    "feature_tokens = torch.tensor(feature_tokens)\n",
    "pn_history_tokens = torch.tensor(pn_history_tokens)\n",
    "feature_tokens\n",
    "\n",
    "model(pn_history_tokens, feature_tokens)\n",
    "# model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.repeat((2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=1):\n",
    "    since = time.time()\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'test']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else: \n",
    "                    model.eval()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for input, label in zip(dataloaders[phase], resultloaders[phase]):\n",
    "                    inputs_vector = prepare_sentence_sequence(input, word_to_ix)\n",
    "                    labels_vector = one_hot_encode(label, tag_to_ix)\n",
    "                    \n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs_vector) # 取得針對每個emotion的預測結果tensor (e.g. tensor([[-1.3948, -1.4476, -1.3804, -1.3261]]))\n",
    "                        pred = torch.argmax(outputs).item() # 取得最大值的index (e.g. 2)\n",
    "                        loss = criterion(outputs[0], labels_vector) # 外面還有一層，只需取得內層 [-1.3948, -1.4476, -1.3804, -1.3261] 與 [0, 0, 1, 0] 的計算loss\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item()\n",
    "                    if pred == label:\n",
    "                        running_corrects += 1\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Time elapsed: {round((time.time() - since))} sec.')\n",
    "                \n",
    "                # deep copy the model\n",
    "                if phase == 'test' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, loss_function, optimizer, exp_lr_scheduler, num_epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
