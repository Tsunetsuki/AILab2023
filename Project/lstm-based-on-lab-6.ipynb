{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# others\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import time\n",
    "\n",
    "# dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import Flowers102\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# read file \n",
    "import pandas as pd\n",
    "\n",
    "# label\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use TweetEval emotion recognition dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '../../Data/tweeteval/datasets/emotion/'\n",
    "# mapping_file = os.path.join(root, 'mapping.txt')\n",
    "# test_labels_file = os.path.join(root, 'test_labels.txt')\n",
    "# test_text_file = os.path.join(root, 'test_text.txt')\n",
    "# train_labels_file = os.path.join(root, 'train_labels.txt')\n",
    "# train_text_file = os.path.join(root, 'train_text.txt')\n",
    "# val_labels_file = os.path.join(root, 'val_labels.txt')\n",
    "# val_text_file = os.path.join(root, 'val_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_pd = pd.read_csv(mapping_file, sep='\\t', header=None)\n",
    "# test_label_pd = pd.read_csv(test_labels_file, sep='\\t', header=None)\n",
    "# test_dataset = open(test_text_file).read().split('\\n')[:-1] # remove last empty line \n",
    "# train_label_pd = pd.read_csv(train_labels_file, sep='\\t', header=None)\n",
    "# train_dataset = open(train_text_file).read().split('\\n')[:-1] # remove last empty line\n",
    "# val_label_pd = pd.read_csv(val_labels_file, sep='\\t', header=None)\n",
    "# val_dataset = open(val_text_file).read().split('\\n')[:-1] # remove last empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training data\n",
    "- Given: Notes with ranges and labels\n",
    "- Transform into label + lists of tokens with [does token describe label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data/'\n",
    "features_path = os.path.join(root, 'features.csv')\n",
    "patient_notes_path = os.path.join(root, 'patient_notes.csv')\n",
    "sample_submission_path = os.path.join(root, 'sample_submission.csv')\n",
    "test_path = os.path.join(root, 'test.csv')\n",
    "train_path = os.path.join(root, 'train.csv')\n",
    "features = pd.read_csv(features_path, sep=',', header=0)\n",
    "patient_notes = pd.read_csv(patient_notes_path, sep=',', header=0)\n",
    "train_raw = pd.read_csv(train_path, sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>208</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>407</td>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>501</td>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>802</td>\n",
       "      <td>8</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>913</td>\n",
       "      <td>9</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_num  case_num feature_text\n",
       "25           112         1       Female\n",
       "34           208         2       Female\n",
       "66           407         4       Female\n",
       "70           501         5       Female\n",
       "99           700         7       Female\n",
       "110          802         8       Female\n",
       "139          913         9       Female"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unusual_numbers = features[\"feature_num\"].value_counts()[features[\"feature_num\"].value_counts() != 1]\n",
    "# unusual_numbers\n",
    "features[features[\"feature_text\"] == \"Female\"]\n",
    "# features[\"feature_num\"] == "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intro \n",
    "- `case_num`: 0~9, each num belongs their groups ... ? \n",
    "- `pn_num`: the id in patient_notes.csv which is 'pn_history', present the note of each case \n",
    "- `feature_num`: the id in features.csv which is 'feature_num', present the feature of each case \n",
    "- `location`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def df_string2list_of_ints(df_string: str):\n",
    "    df_string = df_string.strip(\"[]\")\n",
    "    if df_string == \"\":\n",
    "        return []\n",
    "    entries = re.split(\",|;\", df_string)\n",
    "    entries = [entry.strip(\" '\") for entry in entries]\n",
    "    ranges = [tuple(int(num_as_str) for num_as_str in entry.split(\" \")) for entry in entries]\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>['203 217']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>['70 91', '176 183']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>['222 258']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14295</th>\n",
       "      <td>95333_912</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>912</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14296</th>\n",
       "      <td>95333_913</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>913</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14297</th>\n",
       "      <td>95333_914</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>914</td>\n",
       "      <td>['photobia']</td>\n",
       "      <td>['274 282']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14298</th>\n",
       "      <td>95333_915</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>915</td>\n",
       "      <td>['no sick contacts']</td>\n",
       "      <td>['421 437']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14299</th>\n",
       "      <td>95333_916</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>916</td>\n",
       "      <td>['Subjective fever']</td>\n",
       "      <td>['314 330']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  case_num  pn_num  feature_num  \\\n",
       "0      00016_000         0      16            0   \n",
       "1      00016_001         0      16            1   \n",
       "2      00016_002         0      16            2   \n",
       "3      00016_003         0      16            3   \n",
       "4      00016_004         0      16            4   \n",
       "...          ...       ...     ...          ...   \n",
       "14295  95333_912         9   95333          912   \n",
       "14296  95333_913         9   95333          913   \n",
       "14297  95333_914         9   95333          914   \n",
       "14298  95333_915         9   95333          915   \n",
       "14299  95333_916         9   95333          916   \n",
       "\n",
       "                                     annotation              location  \n",
       "0              ['dad with recent heart attcak']           ['696 724']  \n",
       "1                 ['mom with \"thyroid disease']           ['668 693']  \n",
       "2                            ['chest pressure']           ['203 217']  \n",
       "3          ['intermittent episodes', 'episode']  ['70 91', '176 183']  \n",
       "4      ['felt as if he were going to pass out']           ['222 258']  \n",
       "...                                         ...                   ...  \n",
       "14295                                        []                    []  \n",
       "14296                                        []                    []  \n",
       "14297                              ['photobia']           ['274 282']  \n",
       "14298                      ['no sick contacts']           ['421 437']  \n",
       "14299                      ['Subjective fever']           ['314 330']  \n",
       "\n",
       "[14300 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                                 annotation                location  \\\n",
       "0          ['dad with recent heart attcak']            [(696, 724)]   \n",
       "1             ['mom with \"thyroid disease']            [(668, 693)]   \n",
       "2                        ['chest pressure']            [(203, 217)]   \n",
       "3      ['intermittent episodes', 'episode']  [(70, 91), (176, 183)]   \n",
       "4  ['felt as if he were going to pass out']            [(222, 258)]   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = train_raw.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "data_merged = data_merged.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "data_merged[\"location\"] = data_merged[\"location\"].apply(df_string2list_of_ints)\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history                location  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...            [(696, 724)]  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...            [(668, 693)]  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...            [(203, 217)]  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  [(70, 91), (176, 183)]  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...            [(222, 258)]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_merged[[\"feature_text\", \"pn_history\", \"location\", ]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter training data with no location\n",
    "train = train[train[\"location\"].apply(lambda row: len(row) != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset= 9901\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of dataset= {len(train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "- Use spaCy to split the notes into words.\n",
    "\n",
    "Before start using spaCy\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from collections import Counter\n",
    "\n",
    "# use spacy to tokenize the sentence with english model \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable\n",
    "\n",
    "def build_vocab_from_lines(lines: Iterable[str]):\n",
    "    text_to_count_tokens = ' '.join(lines)\n",
    "    doc = nlp(text_to_count_tokens)\n",
    "    # Get the most frequent words, filtering out stop words and punctuation.\n",
    "    word_freq = Counter(token.text.lower() for token in doc if \\\n",
    "                        not token.is_punct and \\\n",
    "                            not token.is_stop and \\\n",
    "                                not token.is_space)\n",
    "    return word_freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pathjoin\n",
    "import pickle\n",
    "cache_dir = \"cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no cached tokenized patient histories. Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9901/9901 [00:48<00:00, 204.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_pn_histories.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_pn_histories = pickle.load(f)\n",
    "    print(\"Tokenized patient histories loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized patient histories. Tokenizing...\")\n",
    "    tokenized_pn_histories: Dict[str, List[str]] = {}\n",
    "    for pn_history in tqdm(train[\"pn_history\"]):\n",
    "        indexed_words = []\n",
    "        if pn_history in tokenized_pn_histories:\n",
    "            continue\n",
    "        for token in nlp(pn_history):\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "                word = token.text.lower()\n",
    "                start_idx = token.idx\n",
    "                end_idx = token.idx + len(token.text)\n",
    "\n",
    "                indexed_words.append({\n",
    "                    \"word\": word,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": end_idx\n",
    "                })\n",
    "                    \n",
    "        tokenized_pn_histories[pn_history] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_pn_histories, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no cached tokenized features. Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9901/9901 [00:02<00:00, 4810.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_features.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_features = pickle.load(f)\n",
    "    print(\"Tokenized features loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized features. Tokenizing...\")\n",
    "    tokenized_features: Dict[str, List[str]] = {}\n",
    "    for feature_text in tqdm(train[\"feature_text\"]):\n",
    "        indexed_words = []\n",
    "        if feature_text in tokenized_features:\n",
    "            continue\n",
    "        for token in nlp(feature_text):\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "                word = token.text.lower()\n",
    "                start_idx = token.idx\n",
    "                end_idx = token.idx + len(token.text)\n",
    "\n",
    "                indexed_words.append({\n",
    "                    \"word\": word,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": end_idx\n",
    "                })\n",
    "                \n",
    "        tokenized_features[feature_text] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_features, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follow the example described here. Use the same architecture, but:\n",
    "  - only use the last output of the LSTM in the loss function\n",
    "  - use an embedding dim of 128\n",
    "  - use a hidden dim of 256.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature-relevancy of tokens via char ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9901it [00:05, 1753.05it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_preprocessed = dict()\n",
    "for i, (feature_text, pn_history, location) in tqdm(train.iterrows()):\n",
    "    tokenized_history = tokenized_pn_histories[pn_history]\n",
    "    tokens_with_scores = []\n",
    "    for token in tokenized_history:\n",
    "        for feature_relevant_range in location:\n",
    "            token_start, token_end = token[\"start\"], token[\"end\"]\n",
    "            range_start, range_end = feature_relevant_range[0], feature_relevant_range[1]\n",
    "            \n",
    "            percentage_of_token_in_range = max(min(token_end, range_end)+1 - max(token_start, range_start), 0) / (token_end+1 - token_start)\n",
    "            # if percentage_of_token_in_range > 0:\n",
    "            #     print(percentage_of_token_in_range, token, feature_relevant_range)\n",
    "            tokens_with_scores.append({\"token\": token, \"score\": int(percentage_of_token_in_range > 0.9)})\n",
    "    \n",
    "    train_data_preprocessed[i] = {\n",
    "                                    \"pn_history_tokens\": [ts[\"token\"] for ts in tokens_with_scores],\n",
    "                                    \"scores\": torch.tensor([ts[\"score\"] for ts in tokens_with_scores]),\n",
    "                                    \"feature_tokens\": tokenized_features[feature_text],\n",
    "                                    \"locations\": location\n",
    "                                   }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pn_history_tokens': [{'word': 'hpi', 'start': 0, 'end': 3},\n",
       "  {'word': '17yo', 'start': 5, 'end': 9},\n",
       "  {'word': 'm', 'start': 10, 'end': 11},\n",
       "  {'word': 'presents', 'start': 12, 'end': 20},\n",
       "  {'word': 'palpitations', 'start': 26, 'end': 38},\n",
       "  {'word': 'patient', 'start': 40, 'end': 47},\n",
       "  {'word': 'reports', 'start': 48, 'end': 55},\n",
       "  {'word': '3', 'start': 56, 'end': 57},\n",
       "  {'word': '4', 'start': 58, 'end': 59},\n",
       "  {'word': 'months', 'start': 60, 'end': 66},\n",
       "  {'word': 'intermittent', 'start': 70, 'end': 82},\n",
       "  {'word': 'episodes', 'start': 83, 'end': 91},\n",
       "  {'word': 'heart', 'start': 96, 'end': 101},\n",
       "  {'word': 'beating', 'start': 102, 'end': 109},\n",
       "  {'word': 'pounding', 'start': 110, 'end': 118},\n",
       "  {'word': 'chest', 'start': 129, 'end': 134},\n",
       "  {'word': '2', 'start': 137, 'end': 138},\n",
       "  {'word': 'days', 'start': 139, 'end': 143},\n",
       "  {'word': 'ago', 'start': 144, 'end': 147},\n",
       "  {'word': 'soccer', 'start': 157, 'end': 163},\n",
       "  {'word': 'game', 'start': 164, 'end': 168},\n",
       "  {'word': 'episode', 'start': 176, 'end': 183},\n",
       "  {'word': 'time', 'start': 194, 'end': 198},\n",
       "  {'word': 'chest', 'start': 203, 'end': 208},\n",
       "  {'word': 'pressure', 'start': 209, 'end': 217},\n",
       "  {'word': 'felt', 'start': 222, 'end': 226},\n",
       "  {'word': 'going', 'start': 241, 'end': 246},\n",
       "  {'word': 'pass', 'start': 250, 'end': 254},\n",
       "  {'word': 'lose', 'start': 268, 'end': 272},\n",
       "  {'word': 'conciousness', 'start': 273, 'end': 285},\n",
       "  {'word': 'note', 'start': 291, 'end': 295},\n",
       "  {'word': 'patient', 'start': 296, 'end': 303},\n",
       "  {'word': 'endorses', 'start': 304, 'end': 312},\n",
       "  {'word': 'abusing', 'start': 313, 'end': 320},\n",
       "  {'word': 'adderall', 'start': 321, 'end': 329},\n",
       "  {'word': 'primarily', 'start': 331, 'end': 340},\n",
       "  {'word': 'study', 'start': 344, 'end': 349},\n",
       "  {'word': '1', 'start': 351, 'end': 352},\n",
       "  {'word': '3', 'start': 353, 'end': 354},\n",
       "  {'word': 'times', 'start': 355, 'end': 360},\n",
       "  {'word': 'week', 'start': 365, 'end': 369},\n",
       "  {'word': 'recent', 'start': 379, 'end': 385},\n",
       "  {'word': 'soccer', 'start': 386, 'end': 392},\n",
       "  {'word': 'game', 'start': 393, 'end': 397},\n",
       "  {'word': 'took', 'start': 399, 'end': 403},\n",
       "  {'word': 'adderrall', 'start': 404, 'end': 413},\n",
       "  {'word': 'night', 'start': 414, 'end': 419},\n",
       "  {'word': 'morning', 'start': 431, 'end': 438},\n",
       "  {'word': 'game', 'start': 442, 'end': 446},\n",
       "  {'word': 'denies', 'start': 448, 'end': 454},\n",
       "  {'word': 'shortness', 'start': 455, 'end': 464},\n",
       "  {'word': 'breath', 'start': 468, 'end': 474},\n",
       "  {'word': 'diaphoresis', 'start': 476, 'end': 487},\n",
       "  {'word': 'fevers', 'start': 489, 'end': 495},\n",
       "  {'word': 'chills', 'start': 497, 'end': 503},\n",
       "  {'word': 'headache', 'start': 505, 'end': 513},\n",
       "  {'word': 'fatigue', 'start': 515, 'end': 522},\n",
       "  {'word': 'changes', 'start': 524, 'end': 531},\n",
       "  {'word': 'sleep', 'start': 535, 'end': 540},\n",
       "  {'word': 'changes', 'start': 542, 'end': 549},\n",
       "  {'word': 'vision', 'start': 553, 'end': 559},\n",
       "  {'word': 'hearing', 'start': 560, 'end': 567},\n",
       "  {'word': 'abdominal', 'start': 569, 'end': 578},\n",
       "  {'word': 'paun', 'start': 579, 'end': 583},\n",
       "  {'word': 'changes', 'start': 585, 'end': 592},\n",
       "  {'word': 'bowel', 'start': 596, 'end': 601},\n",
       "  {'word': 'urinary', 'start': 605, 'end': 612},\n",
       "  {'word': 'habits', 'start': 613, 'end': 619},\n",
       "  {'word': 'pmhx', 'start': 623, 'end': 627},\n",
       "  {'word': 'rx', 'start': 635, 'end': 637},\n",
       "  {'word': 'uses', 'start': 639, 'end': 643},\n",
       "  {'word': 'friends', 'start': 644, 'end': 651},\n",
       "  {'word': 'adderrall', 'start': 652, 'end': 661},\n",
       "  {'word': 'fhx', 'start': 663, 'end': 666},\n",
       "  {'word': 'mom', 'start': 668, 'end': 671},\n",
       "  {'word': 'thyroid', 'start': 678, 'end': 685},\n",
       "  {'word': 'disease', 'start': 686, 'end': 693},\n",
       "  {'word': 'dad', 'start': 696, 'end': 699},\n",
       "  {'word': 'recent', 'start': 705, 'end': 711},\n",
       "  {'word': 'heart', 'start': 712, 'end': 717},\n",
       "  {'word': 'attcak', 'start': 718, 'end': 724},\n",
       "  {'word': 'immunizations', 'start': 737, 'end': 750},\n",
       "  {'word': 'date', 'start': 758, 'end': 762},\n",
       "  {'word': 'shx', 'start': 764, 'end': 767},\n",
       "  {'word': 'freshmen', 'start': 769, 'end': 777},\n",
       "  {'word': 'college', 'start': 781, 'end': 788},\n",
       "  {'word': 'endorses', 'start': 790, 'end': 798},\n",
       "  {'word': '3', 'start': 799, 'end': 800},\n",
       "  {'word': '4', 'start': 801, 'end': 802},\n",
       "  {'word': 'drinks', 'start': 803, 'end': 809},\n",
       "  {'word': '3', 'start': 810, 'end': 811},\n",
       "  {'word': 'nights', 'start': 812, 'end': 818},\n",
       "  {'word': 'week', 'start': 821, 'end': 825},\n",
       "  {'word': 'weekends', 'start': 830, 'end': 838},\n",
       "  {'word': 'denies', 'start': 841, 'end': 847},\n",
       "  {'word': 'tabacco', 'start': 848, 'end': 855},\n",
       "  {'word': 'endorses', 'start': 857, 'end': 865},\n",
       "  {'word': 'trying', 'start': 866, 'end': 872},\n",
       "  {'word': 'marijuana', 'start': 873, 'end': 882},\n",
       "  {'word': 'sexually', 'start': 884, 'end': 892},\n",
       "  {'word': 'active', 'start': 893, 'end': 899},\n",
       "  {'word': 'girlfriend', 'start': 905, 'end': 915},\n",
       "  {'word': 'x', 'start': 916, 'end': 917},\n",
       "  {'word': '1', 'start': 918, 'end': 919},\n",
       "  {'word': 'year', 'start': 920, 'end': 924},\n",
       "  {'word': 'uses', 'start': 926, 'end': 930},\n",
       "  {'word': 'condoms', 'start': 931, 'end': 938}],\n",
       " 'scores': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'feature_tokens': [{'word': 'family', 'start': 0, 'end': 6},\n",
       "  {'word': 'history', 'start': 7, 'end': 14},\n",
       "  {'word': 'mi', 'start': 18, 'end': 20},\n",
       "  {'word': 'family', 'start': 24, 'end': 30},\n",
       "  {'word': 'history', 'start': 31, 'end': 38},\n",
       "  {'word': 'myocardial', 'start': 42, 'end': 52},\n",
       "  {'word': 'infarction', 'start': 53, 'end': 63}],\n",
       " 'locations': [(696, 724)]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_preprocessed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering 77 out of 9901 datapoints because they don't contain any positive scores.\n"
     ]
    }
   ],
   "source": [
    "num_no_positives = sum([1 for dp in train_data_preprocessed.values() if sum(dp[\"scores\"]) == 0])\n",
    "print(f\"filtering {num_no_positives} out of {len(train_data_preprocessed)} datapoints because they don't contain any positive scores.\")\n",
    "train_data_preprocessed = {key: dp for key, dp in train_data_preprocessed.items() if sum(dp[\"scores\"]) != 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of the Model\n",
    "Layers in LSTM Model:\n",
    "1. embed feature tokens\n",
    "2. lstm feature -> constant size vector\n",
    "\n",
    "3. pass to 2nd lstm\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTokenScorer(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, pn_hist_vocab_size, feature_vocab_size, dropout=0.0):\n",
    "        super(LSTMTokenScorer, self).__init__()\n",
    "\n",
    "        self.pn_history_hidden_dim = hidden_dim\n",
    "\n",
    "        self.feature_embeddings = nn.Embedding(feature_vocab_size, embedding_dim)\n",
    "        self.feature_lstm = nn.LSTM(embedding_dim, embedding_dim, dropout=dropout) # the feature is now one tensor of size [embedding_dim].\n",
    "\n",
    "        self.pn_history_embeddings = nn.Embedding(pn_hist_vocab_size, embedding_dim)\n",
    "        \n",
    "        self.total_lstm = nn.LSTM(embedding_dim * 2, self.pn_history_hidden_dim, dropout=dropout)\n",
    "        \n",
    "        self.hidden2score = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, pn_history, feature):\n",
    "        feature_embeds = self.feature_embeddings(feature)\n",
    "        feature_lstm_out, _ = self.feature_lstm(feature_embeds.view(len(feature), 1, -1)) # the feature is now one tensor of size [embedding_dim].\n",
    "        feature_reduced = torch.squeeze(feature_lstm_out[-1]) #.view(1, -1)\n",
    "        feature_multiplied = feature_reduced.repeat((len(pn_history), 1)) # duplicate feature vector to be same size as embedded pn_history vector.\n",
    "\n",
    "        pn_history_embeds = self.pn_history_embeddings(pn_history)\n",
    "        pn_history_and_features = torch.concat((feature_multiplied, pn_history_embeds), dim=1)\n",
    "\n",
    "        pn_history_reduced, _ = self.total_lstm(pn_history_and_features)\n",
    "        pred_score_raw = torch.squeeze(self.hidden2score(pn_history_reduced))\n",
    "        pred_score = self.sigmoid(pred_score_raw)\n",
    "        return pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = [d[\"scores\"].numpy() for d in train_data_preprocessed.values()]\n",
    "avg_neg_div_pos = np.mean([(scores.shape[0] - np.sum(scores)) / np.sum(scores) for scores in all_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model with vocab sizes, including placeholder indices\n",
    "model = LSTMTokenScorer(EMBEDDING_DIM, HIDDEN_DIM, len(pn_history_vocab)+1, len(feature_vocab)+1)\n",
    "loss_function = nn.BCELoss()\n",
    "# loss_function = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(avg_neg_div_pos))\n",
    "# loss_function = lambda pred, target, vec_size: nn.functional.binary_cross_entropy_with_logits(pred.float(), target.float(), pos_weight=torch.full((vec_size,), avg_neg_div_pos))\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tokens = train_data_preprocessed[0][\"feature_tokens\"]\n",
    "pn_history_tokens = train_data_preprocessed[0][\"pn_history_tokens\"]\n",
    "\n",
    "model(pn_history_tokens, feature_tokens)\n",
    "# model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot_encode(val):\n",
    "#     if val == 0:\n",
    "#         return torch.tensor([1, 0], dtype=torch.float)\n",
    "#     elif val == 1:\n",
    "#         return torch.tensor([0, 1], dtype=torch.float)\n",
    "#     raise Exception(\"one hot encode got invalid value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds: List[List[Tuple[int, int]]], truths: List[List[Tuple[int, int]]]):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_micro_f1([[(1,3)]], [[(1,3)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "logfile_name = \"training_log.txt\"\n",
    "\n",
    "def log(logtext: str = \"\") -> None:\n",
    "    print(logtext)\n",
    "    with open(logfile_name, \"a\", encoding=\"utf8\") as f:\n",
    "        f.write(str(logtext) + \"\\n\")\n",
    "    \n",
    "\n",
    "def train_model(model: LSTMTokenScorer, criterion, optimizer, scheduler, num_epochs=1):\n",
    "    since = time.time()\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        best_loss = 9999999999999\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            log(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            log('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train']: #, 'test'\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else: \n",
    "                    model.eval()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                batch = random.choices(list(train_data_preprocessed.values()), k=64)\n",
    "\n",
    "                # Iterate over data.\n",
    "                for i, datum_preprocessed in enumerate(batch):\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    pn_history_tokens = datum_preprocessed[\"pn_history_tokens\"]\n",
    "                    scores = datum_preprocessed[\"scores\"]\n",
    "                    feature_tokens = datum_preprocessed[\"feature_tokens\"]\n",
    "\n",
    "                    # track history only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(pn_history_tokens, feature_tokens)\n",
    "                        loss = criterion(outputs.float(), scores.float())\n",
    "\n",
    "                        pred = (outputs > 0.9).int()\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item()\n",
    "                    if torch.equal(pred, scores):\n",
    "                        running_corrects += 1\n",
    "                    \n",
    "                    if i == len(batch) - 1 and epoch % 20 == 19:\n",
    "                        log(\"LSTM output:\")\n",
    "                        log(outputs)\n",
    "                        log(\"Truth:\")\n",
    "                        log(scores)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss # / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects # / dataset_sizes[phase]\n",
    "                log(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Time elapsed: {round((time.time() - since))} sec.')\n",
    "                \n",
    "                # deep copy the model\n",
    "                if phase == 'test' and epoch_loss < best_loss: #epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_loss = epoch_loss\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            log()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        log(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        log(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model\n",
    "\n",
    "open(logfile_name, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "model = train_model(model, loss_function, optimizer, exp_lr_scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0, 0, 0], dtype=torch.float)\n",
    "b = torch.tensor([0, 0, 1], dtype=torch.float)\n",
    "c = torch.tensor([1, 1, 0], dtype=torch.float)\n",
    "d = torch.tensor([0, 1, 0], dtype=torch.float)\n",
    "\n",
    "nn.functional.binary_cross_entropy_with_logits(a, b, pos_weight=torch.full((3,), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
