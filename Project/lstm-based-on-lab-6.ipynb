{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# others\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import time\n",
    "\n",
    "# dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import Flowers102\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# read file \n",
    "import pandas as pd\n",
    "\n",
    "# label\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use TweetEval emotion recognition dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '../../Data/tweeteval/datasets/emotion/'\n",
    "# mapping_file = os.path.join(root, 'mapping.txt')\n",
    "# test_labels_file = os.path.join(root, 'test_labels.txt')\n",
    "# test_text_file = os.path.join(root, 'test_text.txt')\n",
    "# train_labels_file = os.path.join(root, 'train_labels.txt')\n",
    "# train_text_file = os.path.join(root, 'train_text.txt')\n",
    "# val_labels_file = os.path.join(root, 'val_labels.txt')\n",
    "# val_text_file = os.path.join(root, 'val_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_pd = pd.read_csv(mapping_file, sep='\\t', header=None)\n",
    "# test_label_pd = pd.read_csv(test_labels_file, sep='\\t', header=None)\n",
    "# test_dataset = open(test_text_file).read().split('\\n')[:-1] # remove last empty line \n",
    "# train_label_pd = pd.read_csv(train_labels_file, sep='\\t', header=None)\n",
    "# train_dataset = open(train_text_file).read().split('\\n')[:-1] # remove last empty line\n",
    "# val_label_pd = pd.read_csv(val_labels_file, sep='\\t', header=None)\n",
    "# val_dataset = open(val_text_file).read().split('\\n')[:-1] # remove last empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training data\n",
    "- Given: Notes with ranges and labels\n",
    "- Transform into label + lists of tokens with [does token describe label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data/'\n",
    "features_path = os.path.join(root, 'features.csv')\n",
    "patient_notes_path = os.path.join(root, 'patient_notes.csv')\n",
    "sample_submission_path = os.path.join(root, 'sample_submission.csv')\n",
    "test_path = os.path.join(root, 'test.csv')\n",
    "train_path = os.path.join(root, 'train.csv')\n",
    "features = pd.read_csv(features_path, sep=',', header=0)\n",
    "patient_notes = pd.read_csv(patient_notes_path, sep=',', header=0)\n",
    "train_raw = pd.read_csv(train_path, sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>208</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>407</td>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>501</td>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>802</td>\n",
       "      <td>8</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>913</td>\n",
       "      <td>9</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_num  case_num feature_text\n",
       "25           112         1       Female\n",
       "34           208         2       Female\n",
       "66           407         4       Female\n",
       "70           501         5       Female\n",
       "99           700         7       Female\n",
       "110          802         8       Female\n",
       "139          913         9       Female"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unusual_numbers = features[\"feature_num\"].value_counts()[features[\"feature_num\"].value_counts() != 1]\n",
    "# unusual_numbers\n",
    "features[features[\"feature_text\"] == \"Female\"]\n",
    "# features[\"feature_num\"] == "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intro \n",
    "- `case_num`: 0~9, each num belongs their groups ... ? \n",
    "- `pn_num`: the id in patient_notes.csv which is 'pn_history', present the note of each case \n",
    "- `feature_num`: the id in features.csv which is 'feature_num', present the feature of each case \n",
    "- `location`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def df_string2list_of_ints(df_string: str):\n",
    "    df_string = df_string.strip(\"[]\")\n",
    "    if df_string == \"\":\n",
    "        return []\n",
    "    entries = re.split(\",|;\", df_string)\n",
    "    entries = [entry.strip(\" '\") for entry in entries]\n",
    "    ranges = [tuple(int(num_as_str) for num_as_str in entry.split(\" \")) for entry in entries]\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>['203 217']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>['70 91', '176 183']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>['222 258']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14295</th>\n",
       "      <td>95333_912</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>912</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14296</th>\n",
       "      <td>95333_913</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>913</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14297</th>\n",
       "      <td>95333_914</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>914</td>\n",
       "      <td>['photobia']</td>\n",
       "      <td>['274 282']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14298</th>\n",
       "      <td>95333_915</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>915</td>\n",
       "      <td>['no sick contacts']</td>\n",
       "      <td>['421 437']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14299</th>\n",
       "      <td>95333_916</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>916</td>\n",
       "      <td>['Subjective fever']</td>\n",
       "      <td>['314 330']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  case_num  pn_num  feature_num  \\\n",
       "0      00016_000         0      16            0   \n",
       "1      00016_001         0      16            1   \n",
       "2      00016_002         0      16            2   \n",
       "3      00016_003         0      16            3   \n",
       "4      00016_004         0      16            4   \n",
       "...          ...       ...     ...          ...   \n",
       "14295  95333_912         9   95333          912   \n",
       "14296  95333_913         9   95333          913   \n",
       "14297  95333_914         9   95333          914   \n",
       "14298  95333_915         9   95333          915   \n",
       "14299  95333_916         9   95333          916   \n",
       "\n",
       "                                     annotation              location  \n",
       "0              ['dad with recent heart attcak']           ['696 724']  \n",
       "1                 ['mom with \"thyroid disease']           ['668 693']  \n",
       "2                            ['chest pressure']           ['203 217']  \n",
       "3          ['intermittent episodes', 'episode']  ['70 91', '176 183']  \n",
       "4      ['felt as if he were going to pass out']           ['222 258']  \n",
       "...                                         ...                   ...  \n",
       "14295                                        []                    []  \n",
       "14296                                        []                    []  \n",
       "14297                              ['photobia']           ['274 282']  \n",
       "14298                      ['no sick contacts']           ['421 437']  \n",
       "14299                      ['Subjective fever']           ['314 330']  \n",
       "\n",
       "[14300 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                                 annotation                location  \\\n",
       "0          ['dad with recent heart attcak']            [(696, 724)]   \n",
       "1             ['mom with \"thyroid disease']            [(668, 693)]   \n",
       "2                        ['chest pressure']            [(203, 217)]   \n",
       "3      ['intermittent episodes', 'episode']  [(70, 91), (176, 183)]   \n",
       "4  ['felt as if he were going to pass out']            [(222, 258)]   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = train_raw.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "data_merged = data_merged.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "data_merged[\"location\"] = data_merged[\"location\"].apply(df_string2list_of_ints)\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history                location  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...            [(696, 724)]  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...            [(668, 693)]  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...            [(203, 217)]  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  [(70, 91), (176, 183)]  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...            [(222, 258)]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_merged[[\"feature_text\", \"pn_history\", \"location\", ]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter training data with no location\n",
    "train = train[train[\"location\"].apply(lambda row: len(row) != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset= 9901\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of dataset= {len(train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "- Use spaCy to split the notes into words.\n",
    "\n",
    "Before start using spaCy\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "# from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leonard\\.conda\\envs\\UniAILab\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "# tokenizer.encode_plus(\"hello i am Drunk\", return_offsets_mapping=True, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pathjoin\n",
    "import pickle\n",
    "cache_dir = \"cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1045, 2572, 7144, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (6, 7), (8, 10), (11, 16), (0, 0)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"hello i am Drunk\", return_offsets_mapping=True, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "BERT_FP = ('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embed_matrix():\n",
    "    bert = BertModel.from_pretrained(BERT_FP)\n",
    "    bert_embeddings = list(bert.children())[0]\n",
    "    bert_word_embeddings = list(bert_embeddings.children())[0]\n",
    "    mat = bert_word_embeddings.weight.data.numpy()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_bert_embed_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache, lru_cache\n",
    "\n",
    "def embed_seq(s: Iterable[int]):\n",
    "    return np.array([onehot_word(word_id) @ embedding_matrix for word_id in s])\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def embed_word(word_id: int):\n",
    "    return onehot_word(word_id) @ embedding_matrix\n",
    "\n",
    "def onehot_word(a: int):\n",
    "    oh = np.zeros(30522, dtype=int)\n",
    "    oh[a] = 1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized patient histories loaded from cache.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_pn_histories.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_pn_histories = pickle.load(f)\n",
    "    print(\"Tokenized patient histories loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized patient histories. Tokenizing...\")\n",
    "    tokenized_pn_histories: Dict[str, List[Dict]] = {}\n",
    "    for pn_history in tqdm(train[\"pn_history\"]):\n",
    "        indexed_words = []\n",
    "        if pn_history in tokenized_pn_histories:\n",
    "            continue\n",
    "\n",
    "        tokenized = tokenizer.encode_plus(pn_history, return_offsets_mapping=True, add_special_tokens=True)\n",
    "\n",
    "        for word, offset_mapping in zip(tokenized[\"input_ids\"], tokenized[\"offset_mapping\"]):\n",
    "            embedded_token = embed_word(word)\n",
    "\n",
    "            indexed_words.append({\n",
    "                \"word_id\": word,\n",
    "                \"embedded\": embedded_token,\n",
    "                \"range\": offset_mapping\n",
    "            })\n",
    "                    \n",
    "        tokenized_pn_histories[pn_history] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_pn_histories, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure:\n",
    "tokenized_pn_histories\n",
    "hist_id -> [tokens]\n",
    "token -> ['word_id', 'embedded', 'range']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_id': 6522,\n",
       " 'embedded': array([-0.01908113, -0.04173963, -0.06673449, -0.01592105,  0.04393673,\n",
       "        -0.0045615 , -0.07181855, -0.03923002, -0.06660692, -0.05208729,\n",
       "        -0.05683259, -0.07923082, -0.07103121, -0.03811198,  0.04058741,\n",
       "        -0.07675945, -0.03249264,  0.01277649,  0.00591493, -0.02265794,\n",
       "        -0.07537558, -0.0377069 , -0.02679249, -0.02630543, -0.10752729,\n",
       "        -0.07387543, -0.03104966, -0.05502456, -0.01574389,  0.03902053,\n",
       "        -0.03393734, -0.04892192, -0.01713544, -0.01373354, -0.03406208,\n",
       "        -0.03928473, -0.01480401, -0.0745343 , -0.04668886,  0.04388529,\n",
       "         0.04298296, -0.02789518, -0.06555539, -0.01023453, -0.05270426,\n",
       "        -0.01351104, -0.01696531, -0.02604903, -0.06805495, -0.0114427 ,\n",
       "        -0.07451096, -0.08470455, -0.06320265, -0.07705162,  0.00924673,\n",
       "        -0.06076133, -0.03972978, -0.00976295,  0.04022013,  0.00853729,\n",
       "        -0.03172083, -0.07831176, -0.04030865, -0.01483213, -0.02548516,\n",
       "        -0.02037099,  0.02583888, -0.00601115,  0.0005211 , -0.0503305 ,\n",
       "         0.01535861, -0.06918605,  0.06422725, -0.04100475, -0.01852632,\n",
       "        -0.00795093, -0.01691452,  0.12692392,  0.0025903 , -0.08476964,\n",
       "        -0.05988051,  0.0401165 , -0.01006817,  0.01409894,  0.02960487,\n",
       "        -0.00199787, -0.0436109 , -0.01663729,  0.01638095,  0.00484425,\n",
       "         0.03679116, -0.02578565, -0.06917788, -0.05262199, -0.01385328,\n",
       "        -0.07637186,  0.03965394, -0.04200633, -0.12093006,  0.01642366,\n",
       "         0.05532187,  0.00137352, -0.02469023,  0.01579396, -0.03945262,\n",
       "        -0.04573373, -0.03275239, -0.05293219,  0.01127648, -0.03078214,\n",
       "        -0.05783765,  0.01816563,  0.00907583, -0.07086688, -0.01004548,\n",
       "        -0.01934193, -0.00457785, -0.05536553, -0.0459183 , -0.0672041 ,\n",
       "         0.03481179,  0.0206055 , -0.01818438, -0.00539505, -0.07162139,\n",
       "        -0.07882681, -0.02764613, -0.09358473, -0.06332196, -0.06640781,\n",
       "        -0.08489089, -0.00042765,  0.12575683, -0.05091361, -0.05330716,\n",
       "         0.01216682, -0.0151412 , -0.00678979, -0.01782138,  0.1514968 ,\n",
       "        -0.05412668, -0.08116943,  0.02427371, -0.03710703, -0.01445157,\n",
       "        -0.02355842,  0.01155926, -0.03040035, -0.01847903, -0.04440971,\n",
       "        -0.00966275,  0.00402587, -0.05210217,  0.04715672, -0.0018505 ,\n",
       "        -0.03760519, -0.0181003 , -0.06754413, -0.06897832,  0.01820193,\n",
       "        -0.04256851,  0.01031271, -0.10166597, -0.06834054, -0.08312954,\n",
       "        -0.09930322,  0.01107772, -0.01796713,  0.03765639, -0.03941826,\n",
       "        -0.01618453,  0.00628192, -0.0043477 , -0.03597885, -0.00878117,\n",
       "         0.08767164, -0.01497303, -0.02665254, -0.01302524, -0.02829309,\n",
       "        -0.08718653, -0.00174683, -0.0600834 , -0.05688876, -0.02207683,\n",
       "        -0.00959872, -0.01209632, -0.05893016, -0.0134788 , -0.05507741,\n",
       "        -0.0325698 , -0.05177484, -0.08775931, -0.0305383 , -0.00073222,\n",
       "        -0.07700045, -0.04340889, -0.00715729, -0.04786425,  0.0315089 ,\n",
       "        -0.07755212, -0.02716055,  0.04729325, -0.08755802, -0.02731228,\n",
       "         0.00718454, -0.03118062, -0.07835725, -0.10915846,  0.00159168,\n",
       "        -0.00731354, -0.01225757, -0.01914476,  0.03351985,  0.01089427,\n",
       "        -0.05832738, -0.14224371, -0.03576584,  0.04456902, -0.000757  ,\n",
       "         0.03093938, -0.02885435, -0.00280817,  0.01134229, -0.04378517,\n",
       "         0.0615575 ,  0.02901632, -0.0430181 ,  0.01795693, -0.00810694,\n",
       "        -0.06790127, -0.00443572,  0.01635651,  0.00989045, -0.04803739,\n",
       "         0.0139738 , -0.06465024, -0.07707804, -0.02952593, -0.09800228,\n",
       "        -0.03218525,  0.00706195, -0.01531379, -0.03954516, -0.01203465,\n",
       "         0.043837  ,  0.00101034, -0.01836536,  0.00685722, -0.07278425,\n",
       "        -0.04377524,  0.01495099, -0.07974492,  0.01325171, -0.02169541,\n",
       "         0.01997804,  0.01421765, -0.03940663, -0.06355347,  0.03975793,\n",
       "        -0.02328743,  0.0563975 ,  0.04225071, -0.03109835, -0.01666352,\n",
       "        -0.03430907, -0.00860342,  0.01001961,  0.00359407,  0.01288842,\n",
       "         0.00498054, -0.00509374, -0.04782482, -0.04231593, -0.01432745,\n",
       "        -0.09286752, -0.04726689, -0.02346229, -0.05374473,  0.01145206,\n",
       "         0.03026417, -0.06029093, -0.00253558,  0.06202162, -0.05841234,\n",
       "        -0.03516613, -0.00256508, -0.08468466, -0.02153719, -0.0368582 ,\n",
       "        -0.03436837,  0.05374306,  0.04055554, -0.07779582, -0.00427109,\n",
       "        -0.05840474, -0.0285328 , -0.02278991, -0.15128602,  0.00052356,\n",
       "        -0.08083797, -0.0130174 , -0.03916908, -0.01433287, -0.14204586,\n",
       "         0.04276562,  0.03892587, -0.02843608,  0.01542204, -0.1145288 ,\n",
       "        -0.02506554,  0.03367257, -0.00213193,  0.01320693, -0.04444836,\n",
       "         0.01883331, -0.05896155, -0.06115995, -0.04090078, -0.01691943,\n",
       "         0.00405402,  0.00108538, -0.09178237, -0.05631986, -0.06067519,\n",
       "        -0.02987276, -0.04702154, -0.02576351, -0.0612037 , -0.05243647,\n",
       "        -0.01006082,  0.01848766,  0.03871314,  0.02257213, -0.07605862,\n",
       "         0.00673083, -0.0318241 ,  0.01370405, -0.02269986, -0.00097142,\n",
       "        -0.02942256,  0.0227625 ,  0.00409969,  0.01073872,  0.08637159,\n",
       "         0.04685065,  0.02199768, -0.03398025, -0.03657096, -0.05069414,\n",
       "        -0.07068926, -0.03799183, -0.07610273,  0.07176458, -0.03762812,\n",
       "         0.02382901, -0.02638155, -0.00514337, -0.02068941, -0.06532674,\n",
       "        -0.02516313, -0.08015107, -0.05505192, -0.06204863,  0.05124347,\n",
       "         0.01688904,  0.01903924, -0.07061321, -0.01526833,  0.01070135,\n",
       "         0.01444305, -0.03720876, -0.04877125, -0.0189193 , -0.0124364 ,\n",
       "        -0.02761597, -0.06310479, -0.06761326, -0.01406777, -0.05796858,\n",
       "        -0.0277304 , -0.03851173,  0.00873912, -0.1048217 , -0.04117056,\n",
       "        -0.04438694, -0.01083885, -0.05739474,  0.00505072, -0.00536615,\n",
       "        -0.01123734, -0.04380842,  0.06836498, -0.04949834,  0.033135  ,\n",
       "        -0.07762786, -0.05575214, -0.02798773, -0.04192498, -0.04895433,\n",
       "        -0.06437089, -0.11018419, -0.00538384,  0.01429083, -0.02152648,\n",
       "        -0.0496478 , -0.07264582, -0.04929105, -0.01951207, -0.0619363 ,\n",
       "        -0.01403196, -0.13095725, -0.07425182, -0.02866054,  0.00218311,\n",
       "        -0.02678812, -0.02157573, -0.04306534, -0.00745715, -0.03779166,\n",
       "        -0.05122556,  0.02723394,  0.00819311, -0.0768657 ,  0.0272549 ,\n",
       "        -0.01246105,  0.01943599, -0.06547591,  0.01579106, -0.06771848,\n",
       "        -0.07174174, -0.00147516, -0.04715826, -0.04099685, -0.06936637,\n",
       "         0.01602412, -0.06892688, -0.05970216,  0.01043891, -0.01967197,\n",
       "        -0.0097134 , -0.0019278 , -0.07333077,  0.08527321,  0.01241145,\n",
       "        -0.08843168, -0.02522356,  0.01829377, -0.02715925, -0.04659085,\n",
       "         0.04570746, -0.03056694, -0.00275488, -0.12433532, -0.0351824 ,\n",
       "        -0.06592382, -0.03078032,  0.00180635, -0.02113222,  0.09001572,\n",
       "        -0.01052239, -0.02229475, -0.03569242,  0.00036761, -0.01738564,\n",
       "        -0.03722301, -0.00113262,  0.05362143, -0.03955411,  0.002896  ,\n",
       "        -0.05952486, -0.05586326, -0.08437575,  0.01152003, -0.03250872,\n",
       "        -0.03138869,  0.01692799, -0.02602969, -0.11407799, -0.09128868,\n",
       "        -0.07265399, -0.00359682,  0.12208072, -0.03638796,  0.01464378,\n",
       "        -0.0503557 ,  0.0402507 , -0.06297771,  0.01648491, -0.00340791,\n",
       "        -0.04787293, -0.08782237, -0.06150886, -0.06769664, -0.00179957,\n",
       "        -0.03849053,  0.09978071,  0.00915475, -0.07090563, -0.10180055,\n",
       "        -0.07990018,  0.00577071,  0.00488389, -0.07092109, -0.01440103,\n",
       "        -0.05694594,  0.00914358, -0.04349818, -0.01060246, -0.01127336,\n",
       "         0.01695217, -0.0403586 ,  0.01587945, -0.08662798, -0.01474116,\n",
       "        -0.06371089, -0.04907921, -0.0448628 ,  0.05979871, -0.02591555,\n",
       "         0.03503687, -0.02138414, -0.0339639 ,  0.00137897,  0.01739392,\n",
       "        -0.01392002, -0.04831953, -0.06016075, -0.06491395,  0.01704529,\n",
       "        -0.04203914, -0.04868194,  0.03360818, -0.02246774, -0.02588319,\n",
       "        -0.02498071, -0.06587233, -0.02461761, -0.08513182,  0.08144202,\n",
       "        -0.04022377,  0.01476349,  0.0111707 , -0.03091095,  0.03112956,\n",
       "        -0.00713935,  0.01671844, -0.10406578, -0.04446519, -0.03654759,\n",
       "        -0.00090015, -0.0822074 , -0.04481355,  0.01534062,  0.03918026,\n",
       "        -0.01107791,  0.01082302, -0.00779742, -0.03331521, -0.0068609 ,\n",
       "         0.02225946, -0.03062034, -0.00548296, -0.04015785,  0.00021749,\n",
       "        -0.03255514, -0.02342268,  0.00763948, -0.08413568,  0.00275202,\n",
       "         0.02455448, -0.02556607,  0.00588004, -0.04904316, -0.00150067,\n",
       "        -0.02676761, -0.11770628,  0.01419074, -0.01508235, -0.03976258,\n",
       "        -0.02756159, -0.08268955, -0.0486187 , -0.06423166,  0.01544593,\n",
       "        -0.02693414,  0.01646638, -0.0501403 , -0.03779647,  0.01287141,\n",
       "        -0.00923243, -0.00107006,  0.00207343, -0.04434131, -0.00433736,\n",
       "        -0.0507568 , -0.05762206, -0.04867698, -0.00996203, -0.00558443,\n",
       "        -0.06657334,  0.01561775, -0.05053767, -0.03807011, -0.01189713,\n",
       "        -0.06210907, -0.0495418 , -0.07157552, -0.09053706,  0.00181156,\n",
       "        -0.04218731, -0.04662285, -0.12897606,  0.03579992,  0.0100256 ,\n",
       "         0.02326034, -0.06064779, -0.04980562, -0.06982776, -0.01257232,\n",
       "        -0.05897349, -0.04919791, -0.04425377,  0.02722542, -0.03964664,\n",
       "        -0.00589339, -0.02847223, -0.05516527,  0.0109106 , -0.03825077,\n",
       "        -0.01748873, -0.04376196, -0.0285569 , -0.02454232, -0.0121718 ,\n",
       "        -0.05331823, -0.03772478,  0.00095005, -0.02478909, -0.02408331,\n",
       "         0.03763863, -0.00808368, -0.00888659,  0.02268378, -0.01658686,\n",
       "        -0.0513445 , -0.01339481, -0.00470968,  0.00539002, -0.0560668 ,\n",
       "         0.0124986 , -0.05672762, -0.0138257 , -0.05866195,  0.0203    ,\n",
       "         0.03280495, -0.05672722, -0.05720117, -0.01101085, -0.00529359,\n",
       "         0.00630592, -0.05287591, -0.09404698,  0.00321512, -0.00732199,\n",
       "        -0.1058289 ,  0.04019904, -0.05588099,  0.01275758, -0.02172038,\n",
       "         0.08089678, -0.08789966, -0.04367155, -0.09806436,  0.02455979,\n",
       "         0.025256  , -0.07345985, -0.04881034, -0.08482597,  0.01101647,\n",
       "         0.00815555, -0.08573112, -0.02448465, -0.07433309, -0.00483828,\n",
       "        -0.0585055 , -0.00990027, -0.08444568, -0.00440183, -0.05598024,\n",
       "        -0.07475943, -0.02485162,  0.00843887, -0.08426636, -0.02037607,\n",
       "        -0.04357914, -0.03103529, -0.03505394, -0.03477909, -0.01211137,\n",
       "        -0.05613294,  0.00179894, -0.05551952, -0.07097323, -0.02187284,\n",
       "        -0.08530426,  0.02231892,  0.00967894, -0.02867814, -0.06385659,\n",
       "         0.00504741,  0.0078729 , -0.04916536, -0.04518002,  0.0471171 ,\n",
       "         0.03352923,  0.03048048, -0.01245142, -0.01099646, -0.00735218,\n",
       "         0.0011658 , -0.03855402, -0.03913379, -0.00843964, -0.03549154,\n",
       "         0.03258345, -0.07661324, -0.03411501, -0.05275038, -0.02779922,\n",
       "        -0.00328957, -0.04803751,  0.03381702, -0.03121832, -0.10256522,\n",
       "        -0.09645248, -0.03838626, -0.04535397, -0.06665562, -0.0674329 ,\n",
       "         0.01928262, -0.04901787, -0.0272354 , -0.07807704,  0.01493715,\n",
       "        -0.0357038 ,  0.02745784, -0.04218743,  0.00024408,  0.02782501,\n",
       "         0.01505823, -0.01638191, -0.02046041,  0.01921135, -0.02198615,\n",
       "        -0.04741193, -0.04992861, -0.0194096 , -0.06105652, -0.00595623,\n",
       "        -0.01816186,  0.0177033 ,  0.00843524,  0.00802204, -0.04932498,\n",
       "        -0.02861249, -0.09120153, -0.03054466]),\n",
       " 'range': (0, 2)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(tokenized_pn_histories.values())[0][0].keys()\n",
    "list(tokenized_pn_histories.values())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized features loaded from cache.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_features.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_features = pickle.load(f)\n",
    "    print(\"Tokenized features loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized features. Tokenizing...\")\n",
    "    tokenized_features: Dict[str, List[str]] = {}\n",
    "    for feature_text in tqdm(train[\"feature_text\"]):\n",
    "        indexed_words = []\n",
    "        if feature_text in tokenized_features:\n",
    "            continue\n",
    "\n",
    "        tokenized = tokenizer.encode_plus(feature_text, add_special_tokens=True)\n",
    "\n",
    "        for word in tokenized[\"input_ids\"]:\n",
    "            embedded_token = embed_word(word)\n",
    "\n",
    "            indexed_words.append({\n",
    "                \"word_id\": word,\n",
    "                \"embedded\": embedded_token,\n",
    "            })\n",
    "                \n",
    "        tokenized_features[feature_text] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_features, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_id': 2155,\n",
       " 'embedded': array([ 1.52311306e-02,  9.42442659e-03, -3.38619053e-02, -2.42799595e-02,\n",
       "         2.34256983e-02, -8.17641430e-03, -2.93032657e-02, -5.23912981e-02,\n",
       "         3.92839797e-02,  2.82708416e-03, -1.66527238e-02, -2.51747109e-02,\n",
       "        -3.52113359e-02,  3.62591706e-02, -7.53024034e-03,  2.13038772e-02,\n",
       "        -2.84320372e-03, -5.02108894e-02, -3.15106139e-02, -1.20872613e-02,\n",
       "        -3.53940949e-02, -3.89684625e-02, -9.54788644e-03,  1.48558039e-02,\n",
       "        -1.46783004e-02,  4.23340984e-02, -1.99949723e-02,  8.76626652e-03,\n",
       "        -2.45130006e-02, -5.38146645e-02,  2.60462761e-02, -1.56000787e-02,\n",
       "        -3.20293419e-02, -1.68701317e-02, -5.71791418e-02,  6.56096172e-03,\n",
       "        -2.73210574e-02, -9.99859255e-03, -2.09460687e-02,  1.66101735e-02,\n",
       "         2.68947575e-02,  1.13374833e-03, -1.66690629e-02,  1.36123579e-02,\n",
       "         2.36188024e-02, -7.47381104e-03,  1.41381836e-02,  1.25802001e-02,\n",
       "         2.57711634e-02,  1.62442252e-02, -3.19506996e-03, -6.07429678e-03,\n",
       "        -3.58607359e-02, -6.43704087e-02, -3.92574770e-03, -1.00825774e-02,\n",
       "         2.75388900e-02, -3.10948975e-02,  2.38278732e-02,  7.44539779e-03,\n",
       "        -4.51974235e-02, -2.97444407e-02, -4.98110019e-02,  2.10578293e-02,\n",
       "        -8.30610245e-02, -9.97002819e-04, -8.61699730e-02,  2.23975535e-02,\n",
       "        -3.63851711e-02,  2.59253103e-02,  1.60876010e-02, -9.07502025e-02,\n",
       "        -5.43588540e-04, -9.99948084e-02, -2.06221528e-02, -3.46545305e-04,\n",
       "        -3.78179066e-02, -7.17385709e-02,  5.57350414e-03, -4.78198193e-02,\n",
       "        -6.21052459e-02, -2.02608649e-02, -8.12239870e-02,  8.56962427e-03,\n",
       "        -5.33923600e-03, -3.35443653e-02,  2.26709917e-02,  1.13433441e-02,\n",
       "        -8.49064216e-02, -1.88676231e-02, -8.39527976e-03, -3.40117491e-03,\n",
       "        -7.06991032e-02,  6.59702858e-03, -1.87366959e-02, -6.42724708e-02,\n",
       "        -4.58222963e-02, -3.97029705e-02, -3.07402085e-03, -2.72165854e-02,\n",
       "        -2.03661732e-02,  1.61965732e-02, -2.54113898e-02, -8.35221335e-02,\n",
       "        -1.46965766e-02, -7.56659508e-02,  2.90891603e-02, -7.51430122e-03,\n",
       "         1.30209271e-02,  5.33524826e-02, -2.45514722e-03, -1.76951755e-02,\n",
       "         1.70296934e-02,  7.50798220e-03, -5.08431718e-03, -1.42686833e-02,\n",
       "        -4.51855659e-02,  3.96616245e-03,  4.22960185e-02, -4.65673115e-03,\n",
       "         1.94535088e-02, -4.98280525e-02,  1.47982426e-02,  3.21139093e-03,\n",
       "        -3.62973884e-02, -3.59461233e-02,  1.67836889e-03, -6.97912276e-02,\n",
       "        -1.32286549e-02, -3.76444422e-02,  4.71290462e-02,  1.99587867e-02,\n",
       "        -4.86811101e-02, -4.27232571e-02,  1.33466849e-03,  2.61137411e-02,\n",
       "         2.37338878e-02,  3.08593665e-03,  2.86678188e-02, -5.49637191e-02,\n",
       "         8.24517012e-03, -5.23012038e-03,  3.77820097e-02, -1.67999149e-03,\n",
       "        -4.83294018e-02, -4.34270166e-02, -3.65219153e-02,  9.22037847e-03,\n",
       "         8.11870769e-03,  7.81739503e-03, -6.72895610e-02, -1.86866131e-02,\n",
       "        -2.00832319e-02, -4.90455143e-02,  1.05020113e-03, -4.55542430e-02,\n",
       "        -4.85595502e-02, -3.35119255e-02, -2.23896205e-02, -6.92902580e-02,\n",
       "         3.83430012e-02,  1.91386443e-05, -5.41733727e-02, -4.62411642e-02,\n",
       "        -2.11368464e-02, -2.94983741e-02,  1.82185769e-02,  3.38594019e-02,\n",
       "        -3.82412262e-02, -2.97777075e-03,  6.72160927e-03, -7.17436150e-03,\n",
       "        -4.06495575e-03, -3.94228101e-02, -5.38021140e-02, -6.17605262e-02,\n",
       "        -5.65859154e-02,  8.64125788e-03, -4.81633469e-02, -9.36725512e-02,\n",
       "        -3.73072028e-02,  4.95297788e-03,  4.02747374e-03, -4.07767780e-02,\n",
       "        -7.65677774e-03, -2.06898395e-02, -1.17031718e-03,  2.28958782e-02,\n",
       "        -1.12383971e-02,  6.36798739e-02, -9.20545086e-02,  5.81580065e-02,\n",
       "        -1.82220433e-02, -4.25849259e-02, -1.29954927e-02,  1.97418220e-02,\n",
       "        -4.16004136e-02,  2.24671829e-02, -3.77326868e-02, -3.30510885e-02,\n",
       "        -1.21155763e-02, -2.03517266e-02, -4.36056405e-02, -7.61120394e-02,\n",
       "        -8.93824548e-02,  1.31214634e-02,  2.85673253e-02,  2.52485983e-02,\n",
       "        -5.57261147e-03,  4.16146964e-02,  1.91640407e-02, -3.57294232e-02,\n",
       "         1.64403860e-02, -4.65510525e-02, -2.96277218e-02, -4.86823817e-04,\n",
       "        -7.15841204e-02, -6.31128326e-02, -4.75690514e-02, -5.93900159e-02,\n",
       "         2.08735801e-02,  2.44462863e-02, -1.45831341e-02, -1.28136994e-02,\n",
       "        -3.97576131e-02, -7.13715702e-02,  1.87176038e-02, -5.16580753e-02,\n",
       "        -5.43301925e-02, -1.17413262e-02, -7.23965243e-02, -8.83746892e-02,\n",
       "        -9.87459160e-03,  1.01658544e-02,  1.61665827e-02, -5.68849482e-02,\n",
       "        -1.27311926e-02,  2.18595620e-02, -5.29118069e-02, -9.06441733e-02,\n",
       "         1.15458379e-02, -3.99737284e-02, -1.26529252e-02, -1.56024471e-02,\n",
       "        -5.21961926e-03, -2.42009237e-02, -2.17853766e-03,  2.80902442e-02,\n",
       "         5.73536905e-04, -1.55107141e-01,  8.53505824e-03,  4.97472510e-02,\n",
       "         4.20570038e-02,  3.38615887e-02, -4.43216274e-03,  3.12609039e-02,\n",
       "         6.20675413e-03, -2.02233028e-02, -5.60681894e-03,  1.64948292e-02,\n",
       "         3.04654222e-02, -4.50957753e-02, -8.02531186e-03, -3.92436385e-02,\n",
       "        -6.30453750e-02,  3.55113596e-02, -6.67691007e-02, -3.63236144e-02,\n",
       "        -4.18387204e-02, -4.26423550e-02, -1.70283001e-02,  2.36203521e-02,\n",
       "        -4.08612043e-02, -7.70878568e-02,  5.72917722e-02, -6.99767992e-02,\n",
       "        -4.64395657e-02,  1.89192742e-02, -7.71297365e-02, -4.31481525e-02,\n",
       "        -2.68922225e-02, -5.83421886e-02,  3.16341892e-02, -6.81618825e-02,\n",
       "        -1.01459846e-02, -1.33461431e-02,  1.34614259e-02,  3.69603820e-02,\n",
       "         7.83465523e-03, -4.28813845e-02,  5.92431845e-03, -5.19340970e-02,\n",
       "         2.69206036e-02, -5.37696294e-02, -4.07555327e-03,  3.93235050e-02,\n",
       "        -7.77556598e-02, -6.68021821e-05, -5.37035391e-02,  1.57563388e-02,\n",
       "        -6.58299774e-02, -4.68439497e-02,  1.98894329e-02, -7.45963529e-02,\n",
       "         2.90387124e-03, -1.93812866e-02, -2.45766323e-02,  3.95529531e-02,\n",
       "        -3.68425213e-02, -1.70048475e-02, -9.45194140e-02, -3.51017457e-03,\n",
       "        -4.69196737e-02, -4.34695184e-03,  2.08559576e-02, -2.31875926e-02,\n",
       "        -2.35556606e-02, -9.85213276e-03,  1.67959761e-02, -5.32903746e-02,\n",
       "        -5.05688861e-02, -3.64725338e-03, -4.53368388e-02, -2.04105787e-02,\n",
       "         1.63314883e-02, -5.16547672e-02,  6.41106814e-02,  1.10853575e-02,\n",
       "        -3.57060246e-02,  4.42247316e-02,  2.65915319e-03, -4.81954701e-02,\n",
       "         2.80114841e-02, -4.38882597e-02,  2.98616104e-02,  8.07774533e-03,\n",
       "        -7.38246813e-02, -2.56575122e-02, -1.20891235e-03, -5.35254218e-02,\n",
       "        -3.03421132e-02, -3.46376821e-02, -1.99767146e-02, -8.55871439e-02,\n",
       "         2.25002579e-02, -8.78942460e-02, -2.18789186e-02,  2.54477244e-02,\n",
       "         1.60067272e-03,  1.91864464e-02, -5.95970266e-03,  1.39965536e-02,\n",
       "        -1.05711684e-01, -7.85354525e-02, -4.32069413e-02, -3.77054606e-03,\n",
       "        -2.20834091e-02, -6.89397007e-02, -6.04365431e-02, -2.98009124e-02,\n",
       "        -5.05183153e-02,  2.33694855e-02, -4.99159750e-03,  1.63934361e-02,\n",
       "         1.02080768e-02,  3.52836959e-03,  6.18576771e-03,  5.88758476e-02,\n",
       "        -1.12545071e-02, -4.26817685e-03,  4.06682380e-02,  2.28572860e-02,\n",
       "        -6.76611438e-02, -2.92034447e-02, -7.90591761e-02,  5.47752250e-03,\n",
       "        -6.78537320e-03, -2.99374294e-02, -3.11338827e-02, -4.36666347e-02,\n",
       "         3.24213393e-02, -2.93021034e-02,  1.93920881e-02,  6.73353719e-03,\n",
       "        -2.53700726e-02,  3.42251137e-02, -7.28035867e-02,  9.03593563e-03,\n",
       "         4.22166623e-02,  5.74758910e-02, -4.18832451e-02,  1.00277308e-02,\n",
       "        -1.27890510e-02, -6.17674440e-02, -6.05846308e-02, -7.12366924e-02,\n",
       "        -7.56696565e-03, -1.11925546e-02,  2.78304443e-02, -7.62914354e-03,\n",
       "         3.03969625e-02, -6.32165149e-02, -2.98878085e-02, -1.06464615e-02,\n",
       "        -1.34275574e-02,  5.33745997e-03,  1.58859137e-02, -3.51887234e-02,\n",
       "        -7.19104987e-03, -1.49495322e-02,  3.07643297e-03,  2.02511586e-02,\n",
       "         6.58996054e-04, -4.89874445e-02,  1.69149507e-02,  4.35995124e-02,\n",
       "        -4.81884778e-02, -8.31077155e-03, -7.14417845e-02,  7.55870044e-02,\n",
       "         4.10913453e-02, -6.23122752e-02,  3.53792831e-02, -7.20357075e-02,\n",
       "        -2.18317490e-02,  4.16731164e-02, -7.10124969e-02, -4.97945212e-02,\n",
       "        -5.90372048e-02, -2.46399026e-02, -9.29307714e-02,  1.45874368e-02,\n",
       "        -1.20130526e-02, -3.22679393e-02, -1.55276097e-02,  1.94596555e-02,\n",
       "         4.00568731e-02, -6.19752593e-02, -2.73674540e-02, -3.16938274e-02,\n",
       "         5.63009568e-02,  2.54132338e-02, -1.75560557e-03, -6.70711473e-02,\n",
       "        -2.62224693e-02, -5.79263605e-02,  3.85424905e-02, -6.09059446e-03,\n",
       "        -3.65220360e-03, -6.24917075e-02, -4.70685139e-02, -4.15379219e-02,\n",
       "         9.23450571e-03, -3.37729529e-02, -4.45652194e-02,  2.26356592e-02,\n",
       "        -5.98876774e-02,  1.05139697e-02, -1.96131822e-02, -2.15638778e-03,\n",
       "         1.75591093e-02, -1.05781227e-01, -1.51436462e-03, -6.12972789e-02,\n",
       "        -1.92073025e-02, -2.87937354e-02, -1.04767969e-02, -2.19317749e-02,\n",
       "         1.54626770e-02, -5.31485565e-02,  1.73501242e-02, -3.59393656e-02,\n",
       "        -3.91451716e-02, -2.48707961e-02, -1.15518114e-02, -1.33844232e-02,\n",
       "         7.91026279e-03, -4.40833196e-02, -3.13281491e-02,  5.17701777e-03,\n",
       "         2.41726916e-02,  2.15686187e-02, -3.01312525e-02,  1.59254251e-03,\n",
       "         3.13285142e-02,  2.56159510e-02,  7.95306917e-03, -1.81422122e-02,\n",
       "         3.82506289e-03,  2.92187426e-02,  9.22858436e-03, -4.38426621e-02,\n",
       "         5.14102355e-03, -9.82083473e-03,  2.44829375e-02, -6.46073595e-02,\n",
       "        -1.09620303e-01, -8.12270388e-04,  1.21808061e-02, -1.81697942e-02,\n",
       "        -2.80488543e-02, -4.14994322e-02, -3.73201706e-02, -3.00048701e-02,\n",
       "        -3.60519662e-02, -1.62596609e-02, -6.43436164e-02,  7.89933372e-03,\n",
       "        -1.22455703e-02, -1.78920366e-02, -7.34208375e-02, -5.51692732e-02,\n",
       "         4.76007164e-02, -8.00941065e-02, -9.89862625e-03,  2.44176947e-02,\n",
       "         1.24313831e-02, -6.44868314e-02, -5.55185378e-02,  8.74918327e-03,\n",
       "        -3.83896679e-02,  1.24899922e-02,  3.05021331e-02, -9.15587135e-03,\n",
       "         4.76321355e-02,  5.24125993e-02, -1.22695873e-02,  2.09699739e-02,\n",
       "         1.96429305e-02, -3.37100476e-02, -5.85683808e-02,  1.11338934e-02,\n",
       "        -2.87697408e-02, -3.24240476e-02, -1.05963545e-02, -3.99678275e-02,\n",
       "         7.84924533e-03,  6.65762648e-02,  3.21839377e-02,  4.31494005e-02,\n",
       "         1.05961384e-02, -4.09080945e-02, -1.06012495e-02, -4.37951734e-04,\n",
       "        -7.11712660e-03, -4.40713204e-02,  8.71266704e-03,  2.52674017e-02,\n",
       "        -2.56595351e-02,  5.41124074e-03,  6.63584098e-03, -2.13722922e-02,\n",
       "         4.16363962e-02, -9.27571114e-03, -3.08887772e-02, -2.52176039e-02,\n",
       "        -3.21201188e-03, -3.74785042e-03, -5.54295555e-02, -5.98303303e-02,\n",
       "        -4.50135842e-02,  7.26621412e-03,  8.26125685e-03, -4.43532392e-02,\n",
       "        -5.36668599e-02, -5.59826866e-02,  5.83275296e-02, -2.26354469e-02,\n",
       "        -2.24626046e-02, -3.48942401e-03, -1.97326597e-02,  9.84343607e-03,\n",
       "         6.60307892e-03,  2.85363346e-02, -3.41117755e-02, -6.81525543e-02,\n",
       "        -9.20211300e-02, -6.34001791e-02, -3.61268036e-02, -1.66131256e-04,\n",
       "        -8.19247663e-02,  2.08414402e-02, -1.68942530e-02,  7.92026147e-03,\n",
       "         9.15625319e-03,  1.62156578e-02,  8.38278234e-03,  4.91934642e-03,\n",
       "        -7.17306137e-02,  3.89475003e-02, -9.35496092e-02, -7.20771700e-02,\n",
       "        -6.46768957e-02, -2.28228681e-02, -2.91856695e-02,  6.64982805e-03,\n",
       "         1.91643536e-02,  3.34108099e-02, -4.16697450e-02, -2.07496732e-02,\n",
       "        -6.27234951e-02, -2.01628916e-03, -4.06005140e-03, -2.08371785e-02,\n",
       "        -3.25768478e-02,  1.99883450e-02, -3.55855599e-02, -5.78509131e-03,\n",
       "         4.68035862e-02, -5.50547428e-02, -2.74876710e-02,  3.32259922e-03,\n",
       "        -4.06967737e-02,  3.76991555e-02, -7.08693266e-02,  3.71663552e-03,\n",
       "         5.03133703e-03, -1.43978521e-02, -2.82731354e-02,  1.85701102e-02,\n",
       "        -3.86685915e-02,  2.22261343e-02,  3.91379185e-03,  7.54645979e-03,\n",
       "        -3.72246169e-02, -5.37433587e-02,  7.55747547e-03, -6.95776492e-02,\n",
       "        -2.79285535e-02, -5.85377216e-02, -4.50596446e-03, -2.69717770e-04,\n",
       "         3.78758498e-02, -3.47519815e-02, -3.93273011e-02, -5.14439456e-02,\n",
       "        -4.74330969e-02, -1.09439073e-02, -3.70996445e-02, -1.93865709e-02,\n",
       "        -1.96613688e-02,  9.40618396e-04, -1.56773217e-02,  3.64401611e-03,\n",
       "        -3.43837254e-02, -3.89860645e-02, -2.49423403e-02,  8.36504716e-03,\n",
       "        -2.78186500e-02,  1.71869493e-03,  5.78296464e-03, -2.53306678e-03,\n",
       "        -3.93334217e-02, -3.97457965e-02, -5.49873188e-02, -4.62207459e-02,\n",
       "        -1.58211980e-02, -2.50746030e-02, -3.13455053e-02, -5.77586843e-03,\n",
       "        -2.93670651e-02, -1.24803996e-02,  4.18163743e-03,  1.41403750e-02,\n",
       "         1.68143697e-02, -5.35635836e-02,  2.87874937e-02, -1.67598464e-02,\n",
       "        -1.32627971e-02,  9.33323335e-03, -8.31953064e-02, -9.45595501e-04,\n",
       "        -4.44996450e-03,  4.18075733e-02, -4.24354225e-02, -3.35405394e-02,\n",
       "         1.97008271e-02, -5.04458733e-02, -2.89398041e-02,  5.09284139e-02,\n",
       "         4.76725884e-02, -1.43112019e-02, -3.01140681e-04, -6.54487610e-02,\n",
       "         7.51618156e-03,  2.27209907e-02, -3.55842635e-02, -2.56626513e-02,\n",
       "        -6.88615302e-03,  1.77982356e-02, -4.24528345e-02, -1.65193086e-03,\n",
       "        -4.28550541e-02, -2.94640753e-02,  8.80818442e-03, -1.37469852e-02,\n",
       "        -9.18758214e-02, -1.80169865e-02, -8.91804397e-02, -6.12931810e-02,\n",
       "        -1.28519749e-02, -6.08912157e-03,  4.85325381e-02, -6.62526116e-02,\n",
       "        -4.07447889e-02, -2.91593820e-02, -5.36921842e-04,  3.26046161e-03,\n",
       "         1.61990337e-02, -2.81461086e-02, -2.81694718e-02,  8.23842920e-03,\n",
       "        -6.10045390e-03,  1.00754609e-03, -2.93709431e-02, -5.30086383e-02,\n",
       "        -5.88793345e-02, -3.39528397e-02, -1.27446912e-02, -2.56063547e-02,\n",
       "         3.94139905e-03, -5.62214255e-02,  9.49775148e-03,  2.24863831e-03,\n",
       "        -5.57465330e-02, -2.01091785e-02,  1.34388525e-02, -1.14753349e-02,\n",
       "         1.08914003e-02, -5.01763411e-02, -3.63819189e-02,  3.38192880e-02,\n",
       "         4.13231961e-02, -4.98285294e-02, -3.86047736e-02,  4.57839528e-03,\n",
       "         4.91425917e-02, -3.65576930e-02,  6.80767139e-03, -1.43696666e-02,\n",
       "        -2.49997601e-02,  6.44037826e-03,  8.70285928e-03, -7.69861937e-02,\n",
       "        -1.04244985e-02, -2.14265324e-02, -3.94667238e-02,  3.64254937e-02,\n",
       "        -3.65097187e-02,  5.31419925e-02,  4.42693047e-02, -2.81242188e-03,\n",
       "        -3.75478789e-02, -5.29529899e-02,  1.93552990e-02,  3.82753951e-03,\n",
       "        -2.99465526e-02, -1.02200909e-02,  1.28179248e-02,  1.41479671e-02,\n",
       "         2.69795326e-03,  4.07936750e-03, -3.60189155e-02, -1.88907720e-02,\n",
       "        -6.15102472e-03, -1.76733863e-02,  3.19157355e-03,  5.39453290e-02])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenized_features.values())[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follow the example described here. Use the same architecture, but:\n",
    "  - only use the last output of the LSTM in the loss function\n",
    "  - use an embedding dim of 128\n",
    "  - use a hidden dim of 256.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature-relevancy of tokens via char ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9901it [00:08, 1231.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_preprocessed = dict()\n",
    "for i, (feature_text, pn_history, location) in tqdm(train.iterrows()):\n",
    "    tokenized_history = tokenized_pn_histories[pn_history]\n",
    "    tokens_with_scores = []\n",
    "    for token in tokenized_history:\n",
    "        percentages = []\n",
    "        for feature_relevant_range in location:\n",
    "            token_start, token_end = token[\"range\"]\n",
    "            range_start, range_end = feature_relevant_range[0], feature_relevant_range[1]\n",
    "            \n",
    "            percentage_of_token_in_range = max(min(token_end, range_end)+1 - max(token_start, range_start), 0) / (token_end+1 - token_start)\n",
    "            percentages.append(percentage_of_token_in_range)\n",
    "            # if percentage_of_token_in_range > 0:\n",
    "            #     print(percentage_of_token_in_range, token, feature_relevant_range)\n",
    "        \n",
    "\n",
    "        tokens_with_scores.append({\"token\": token,\n",
    "                                   \"score\": int(max(percentages) > 0.9)})\n",
    "\n",
    "    train_data_preprocessed[i] = {\n",
    "                                    \"pn_history_tokens\": [ts[\"token\"] for ts in tokens_with_scores],\n",
    "                                    \"scores\": torch.tensor([ts[\"score\"] for ts in tokens_with_scores]),\n",
    "                                    \"feature_tokens\": tokenized_features[feature_text],\n",
    "                                    \"locations\": location\n",
    "                                   }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering 2 out of 9901 datapoints because they don't contain any positive scores.\n"
     ]
    }
   ],
   "source": [
    "num_no_positives = sum([1 for dp in train_data_preprocessed.values() if sum(dp[\"scores\"]) == 0])\n",
    "print(f\"filtering {num_no_positives} out of {len(train_data_preprocessed)} datapoints because they don't contain any positive scores.\")\n",
    "train_data_preprocessed = {key: dp for key, dp in train_data_preprocessed.items() if sum(dp[\"scores\"]) != 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of the Model\n",
    "Layers in LSTM Model:\n",
    "1. embed feature tokens\n",
    "2. lstm feature -> constant size vector\n",
    "\n",
    "3. pass to 2nd lstm\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_matrix.shape[1]\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTokenScorer(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout=0.0):\n",
    "        super(LSTMTokenScorer, self).__init__()\n",
    "\n",
    "        self.pn_history_hidden_dim = hidden_dim\n",
    "\n",
    "        # self.bert_embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix))\n",
    "\n",
    "        self.feature_lstm = nn.LSTM(embedding_dim, embedding_dim, bidirectional=False, dropout=dropout) # the feature is now one tensor of size [embedding_dim].\n",
    "\n",
    "        self.total_lstm = nn.LSTM(embedding_dim * 2, self.pn_history_hidden_dim, bidirectional=False, dropout=dropout)\n",
    "        \n",
    "        self.hidden2score = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, pn_history, feature):\n",
    "        # feature_embeds = self.bert_embedding(feature)\n",
    "        feature_lstm_out, _ = self.feature_lstm(feature.view(len(feature), 1, -1)) # the feature is now one tensor of size [embedding_dim].\n",
    "        feature_reduced = torch.squeeze(feature_lstm_out[-1]) #.view(1, -1)\n",
    "        feature_multiplied = feature_reduced.repeat((len(pn_history), 1)) # duplicate feature vector to be same size as embedded pn_history vector.\n",
    "\n",
    "        # pn_history_embeds = self.bert_embedding(pn_history)\n",
    "        pn_history_and_features = torch.concat((feature_multiplied, pn_history), dim=1)\n",
    "\n",
    "        pn_history_reduced, _ = self.total_lstm(pn_history_and_features)\n",
    "        pred_score_raw = torch.squeeze(self.hidden2score(pn_history_reduced))\n",
    "        pred_score = self.sigmoid(pred_score_raw)\n",
    "        return pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_scores = [d[\"scores\"].numpy() for d in train_data_preprocessed.values()]\n",
    "# avg_neg_div_pos = np.mean([(scores.shape[0] - np.sum(scores)) / np.sum(scores) for scores in all_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pn_history_tokens', 'scores', 'feature_tokens', 'locations'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_preprocessed[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_tokens = train_data_preprocessed[0][\"feature_tokens\"]\n",
    "# pn_history_tokens = train_data_preprocessed[0][\"pn_history_tokens\"]\n",
    "\n",
    "# feature_tensor = torch.tensor(np.array([t[\"embedded\"] for t in feature_tokens]), dtype=torch.float)\n",
    "# pn_history_tensor = torch.tensor(np.array([t[\"embedded\"] for t in pn_history_tokens]), dtype=torch.float)\n",
    "\n",
    "# model(pn_history_tensor, feature_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds: List[List[Tuple[int, int]]], truths: List[List[Tuple[int, int]]]):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_micro_f1([[(1,3)]], [[(1,3)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scored_ranges2spans(rnges: Tuple[int, int], scores: int) -> List[Tuple[int, int]]:\n",
    "    thresh = 0.9\n",
    "    spans: List[Tuple[int, int]] = []\n",
    "    active_span_start = None\n",
    "    active_span_end = None\n",
    "    for rng, score in zip(rnges, scores):\n",
    "        if active_span_start is None:\n",
    "            if score > thresh:\n",
    "                active_span_start = rng[0]\n",
    "                active_span_end = rng[1]\n",
    "        else: # prev. words are already part of span\n",
    "            if score > thresh:\n",
    "                active_span_end = rng[1]\n",
    "            else:\n",
    "                spans.append((active_span_start, active_span_end))\n",
    "                active_span_start = None\n",
    "                active_span_end = None\n",
    "    if active_span_start is not None:\n",
    "        spans.append((active_span_start, active_span_end))\n",
    "    return spans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3172 in train_data_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "logfile_name = \"training_log.txt\"\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def log(logtext: str = \"\") -> None:\n",
    "    print(logtext)\n",
    "    with open(logfile_name, \"a\", encoding=\"utf8\") as f:\n",
    "        f.write(str(logtext) + \"\\n\")\n",
    "    \n",
    "\n",
    "def train_model(model: LSTMTokenScorer, criterion, optimizer, scheduler, num_epochs=1, prev_losses=[], prev_f1s=[]):\n",
    "    since = time.time()\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        best_loss = 9999999999999\n",
    "\n",
    "        losses = prev_losses\n",
    "        f1s = prev_f1s\n",
    "        for epoch in range(num_epochs):\n",
    "            log(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            log('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train']: #, 'test'\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else: \n",
    "                    model.eval()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_loss_average = 0.0\n",
    "                running_f1_total = 0.0\n",
    "                running_f1_average = 0.0\n",
    "                num_non_zero_outputs_in_epoch = 0.0\n",
    "\n",
    "                # batch = random.choices(list(train_data_preprocessed.values()), k=64)\n",
    "                data_ids = list(train_data_preprocessed.keys())\n",
    "                random.shuffle(data_ids)\n",
    "\n",
    "                # Iterate over data.\n",
    "                for i, data_id in enumerate(data_ids):\n",
    "                    datum_preprocessed = train_data_preprocessed[data_id]\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    pn_history_tokens = datum_preprocessed[\"pn_history_tokens\"]\n",
    "                    scores = datum_preprocessed[\"scores\"]\n",
    "                    feature_tokens = datum_preprocessed[\"feature_tokens\"]\n",
    "\n",
    "                    feature_tensor = torch.tensor(np.array([t[\"embedded\"] for t in feature_tokens]), dtype=torch.float)\n",
    "                    pn_history_tensor = torch.tensor(np.array([t[\"embedded\"] for t in pn_history_tokens]), dtype=torch.float)\n",
    "\n",
    "                    # track history only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(pn_history_tensor, feature_tensor)\n",
    "                        \n",
    "                        num_non_zero_outputs = np.count_nonzero(outputs.detach().numpy().round().astype(int))\n",
    "                        num_non_zero_outputs_in_epoch += num_non_zero_outputs\n",
    "                        \n",
    "                        loss = criterion(outputs.float(), scores.float())\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    try:\n",
    "                        f1 = f1_score(scores.int(), outputs.detach().round().int())\n",
    "                    except Exception as e:\n",
    "                        log(\"F1 score calc failed:\")\n",
    "                        log(\"Scores:\")\n",
    "                        log(scores.int())\n",
    "                        log(\"\\nOutputs\")\n",
    "                        log(outputs.detach().round().int())\n",
    "                        log(\"\\n\")\n",
    "                        raise Exception(e)\n",
    "                    # statistics\n",
    "                    running_loss += loss.item()\n",
    "                    running_loss_average = running_loss / (i + 1)\n",
    "                    losses.append(loss.item())\n",
    "                    running_f1_total += f1\n",
    "                    running_f1_average = running_f1_total / (i + 1)\n",
    "                    f1s.append(f1)\n",
    "                    \n",
    "                    if i % 1000 == 0:\n",
    "                        log(f\"Epoch {epoch}, i={i}, avg. loss={running_loss_average}, avg. F1={running_f1_average}, nonzero outputs={num_non_zero_outputs_in_epoch}\")\n",
    "                        # log(\"LSTM output:\")\n",
    "                        # log(outputs)\n",
    "                        # log(\"Truth:\")\n",
    "                        # log(scores)\n",
    "                        log(\"Number of nonzero outputs (in sample prediction):\")\n",
    "                        log(num_non_zero_outputs)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss # / dataset_sizes[phase]\n",
    "                epoch_f1 = running_f1_average # / dataset_sizes[phase]\n",
    "                log(f'{phase} Loss: {epoch_loss:.4f} F1: {epoch_f1:.4f} Time elapsed: {round((time.time() - since))} sec.')\n",
    "                \n",
    "                # deep copy the model\n",
    "                if phase == 'test' and epoch_loss < best_loss: #epoch_acc > best_acc:\n",
    "                    best_acc = epoch_f1\n",
    "                    best_loss = epoch_loss\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            log()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        log(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        log(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model, losses, f1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9899it [00:00, 54091.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43.99553335680338"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = list(train_data_preprocessed.keys())\n",
    "neg_values = 0.0\n",
    "pos_values = 0.0\n",
    "for i, data_id in tqdm(enumerate(data_ids)):\n",
    "    datum_preprocessed = train_data_preprocessed[data_id]\n",
    "\n",
    "    pn_history_tokens = datum_preprocessed[\"pn_history_tokens\"]\n",
    "    scores = datum_preprocessed[\"scores\"]\n",
    "    pos_values += np.count_nonzero(scores.numpy().round().astype(int))\n",
    "    neg_values += (scores.shape[0] - np.count_nonzero(scores.numpy().round().astype(int)))\n",
    "\n",
    "neg_pos_ratio = neg_values / pos_values\n",
    "neg_pos_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training for lr=0.001...\n",
      "Epoch 0/9\n",
      "----------\n",
      "Epoch 0, i=0, avg. loss=1.3483296632766724, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=1000, avg. loss=1.3920301597911517, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=2000, avg. loss=1.3929614389854215, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=3000, avg. loss=1.3873411853962523, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=4000, avg. loss=1.3767362828255891, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=5000, avg. loss=1.3813388242504163, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=6000, avg. loss=1.3777394090924853, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=7000, avg. loss=1.3728073656839943, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=8000, avg. loss=1.3720091809378372, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=9000, avg. loss=1.370022808369127, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13584.6316 F1: 0.0000 Time elapsed: 904 sec.\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "Epoch 1, i=0, avg. loss=1.0382559299468994, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=1000, avg. loss=1.3646449277570079, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=2000, avg. loss=1.3650210183123122, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=3000, avg. loss=1.3739941558493094, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=4000, avg. loss=1.3671308931217465, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=5000, avg. loss=1.3666864219629105, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=6000, avg. loss=1.364402847287258, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=7000, avg. loss=1.3631919036335884, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=8000, avg. loss=1.3619814357106768, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=9000, avg. loss=1.3667897709925008, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13517.8258 F1: 0.0000 Time elapsed: 1822 sec.\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "Epoch 2, i=0, avg. loss=2.204024314880371, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=1000, avg. loss=1.3769972397611812, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=2000, avg. loss=1.3756974644508437, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=3000, avg. loss=1.3693916723554191, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=4000, avg. loss=1.366817225429065, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=5000, avg. loss=1.3644767449012258, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=6000, avg. loss=1.3643094164652538, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=7000, avg. loss=1.362501353710792, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=8000, avg. loss=1.3633353667637063, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=9000, avg. loss=1.3629915057559925, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13513.3004 F1: 0.0000 Time elapsed: 2589 sec.\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "Epoch 3, i=0, avg. loss=1.2474864721298218, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=1000, avg. loss=1.3489126192701684, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=2000, avg. loss=1.3540337713940747, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=3000, avg. loss=1.348429519627262, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=4000, avg. loss=1.3572226369181326, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=5000, avg. loss=1.3588551355347445, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=6000, avg. loss=1.3641039506730746, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=7000, avg. loss=1.3630826236298894, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=8000, avg. loss=1.3630265146698777, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=9000, avg. loss=1.3630023140627043, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13507.4914 F1: 0.0000 Time elapsed: 3358 sec.\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "Epoch 4, i=0, avg. loss=0.9728705286979675, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=1000, avg. loss=1.3821759533572506, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=2000, avg. loss=1.3554913535647128, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=3000, avg. loss=1.360230591566473, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=4000, avg. loss=1.3595059754192398, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=5000, avg. loss=1.3611255208627389, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=6000, avg. loss=1.363449413961936, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=7000, avg. loss=1.3635026702641113, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=8000, avg. loss=1.3624512746533428, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=9000, avg. loss=1.3617372273564432, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13491.7131 F1: 0.0000 Time elapsed: 4266 sec.\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "Epoch 5, i=0, avg. loss=0.843648374080658, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=1000, avg. loss=1.3560750153395797, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=2000, avg. loss=1.3615626480983294, avg. F1=0.0012147257468641932, nonzero outputs=4444.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=3000, avg. loss=1.3561335252110698, avg. F1=0.005115533767668565, nonzero outputs=21169.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=4000, avg. loss=1.3458980335887747, avg. F1=0.006114907117504934, nonzero outputs=31358.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=5000, avg. loss=1.3430382293764294, avg. F1=0.008976070531251207, nonzero outputs=56051.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=6000, avg. loss=1.3442635186074914, avg. F1=0.01166771575285623, nonzero outputs=78335.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 5, i=7000, avg. loss=1.3450538400513259, avg. F1=0.016900665058725832, nonzero outputs=105942.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "2\n",
      "Epoch 5, i=8000, avg. loss=1.3372684550753178, avg. F1=0.022878013783406387, nonzero outputs=129661.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "8\n",
      "Epoch 5, i=9000, avg. loss=1.338023872729633, avg. F1=0.029684984372737182, nonzero outputs=163674.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "17\n",
      "train Loss: 13235.5969 F1: 0.0352 Time elapsed: 5152 sec.\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "Epoch 6, i=0, avg. loss=1.603230595588684, avg. F1=0.0, nonzero outputs=19.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "19\n",
      "Epoch 6, i=1000, avg. loss=1.3343526888917852, avg. F1=0.08482470640422107, nonzero outputs=40378.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "54\n",
      "Epoch 6, i=2000, avg. loss=1.333094732365806, avg. F1=0.08563641919162002, nonzero outputs=85568.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "157\n",
      "Epoch 6, i=3000, avg. loss=1.3243164033422625, avg. F1=0.08818947024144491, nonzero outputs=123939.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "121\n",
      "Epoch 6, i=4000, avg. loss=1.3214535932635045, avg. F1=0.08841322697974503, nonzero outputs=167152.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "106\n",
      "Epoch 6, i=5000, avg. loss=1.3186986487022854, avg. F1=0.09017029638447471, nonzero outputs=204386.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "20\n",
      "Epoch 6, i=6000, avg. loss=1.3336948189829174, avg. F1=0.08844963471857367, nonzero outputs=299622.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "18\n",
      "Epoch 6, i=7000, avg. loss=1.3299056231813795, avg. F1=0.0898758463080783, nonzero outputs=334805.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "67\n",
      "Epoch 6, i=8000, avg. loss=1.3305832799964064, avg. F1=0.08969751810707041, nonzero outputs=386736.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "24\n",
      "Epoch 6, i=9000, avg. loss=1.3305924366002824, avg. F1=0.0889197600369538, nonzero outputs=441869.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "32\n",
      "train Loss: 13151.8735 F1: 0.0890 Time elapsed: 6016 sec.\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "Epoch 7, i=0, avg. loss=1.1332124471664429, avg. F1=0.0, nonzero outputs=10.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "Epoch 7, i=1000, avg. loss=1.3308900397617023, avg. F1=0.09821884653494609, nonzero outputs=36194.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "4\n",
      "Epoch 7, i=2000, avg. loss=1.316680606903284, avg. F1=0.09797895045173499, nonzero outputs=83708.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "Epoch 7, i=3000, avg. loss=1.3058732014344319, avg. F1=0.10231424876787373, nonzero outputs=129123.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "97\n",
      "Epoch 7, i=4000, avg. loss=1.302854002237141, avg. F1=0.10449662105748928, nonzero outputs=166489.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "26\n",
      "Epoch 7, i=5000, avg. loss=1.2990939692482188, avg. F1=0.10582410669901966, nonzero outputs=205578.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "42\n",
      "Epoch 7, i=6000, avg. loss=1.2977307149915374, avg. F1=0.10535076696707257, nonzero outputs=249385.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "Epoch 7, i=7000, avg. loss=1.2972379022199008, avg. F1=0.10530751055111066, nonzero outputs=294072.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "118\n",
      "Epoch 7, i=8000, avg. loss=1.2970370229088266, avg. F1=0.10505755132255733, nonzero outputs=346588.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "51\n",
      "Epoch 7, i=9000, avg. loss=1.298675338356591, avg. F1=0.10526366876311695, nonzero outputs=393396.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "train Loss: 12854.9861 F1: 0.1046 Time elapsed: 6942 sec.\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "Epoch 8, i=0, avg. loss=1.108947515487671, avg. F1=0.11764705882352941, nonzero outputs=64.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "64\n",
      "Epoch 8, i=1000, avg. loss=1.2907445232470434, avg. F1=0.1117900771319291, nonzero outputs=49807.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "150\n",
      "Epoch 8, i=2000, avg. loss=1.2910203139285097, avg. F1=0.10769675387319633, nonzero outputs=90371.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "20\n",
      "Epoch 8, i=3000, avg. loss=1.2920344321579187, avg. F1=0.10566839105989125, nonzero outputs=137618.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "5\n",
      "Epoch 8, i=4000, avg. loss=1.2957129424376894, avg. F1=0.10306510880097382, nonzero outputs=192249.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "43\n",
      "Epoch 8, i=5000, avg. loss=1.3003635386470984, avg. F1=0.10257387321399614, nonzero outputs=237672.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "21\n",
      "Epoch 8, i=6000, avg. loss=1.3031243879464125, avg. F1=0.10313279657690712, nonzero outputs=290487.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "8\n",
      "Epoch 8, i=7000, avg. loss=1.3024576541407247, avg. F1=0.10310481861247439, nonzero outputs=335729.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 8, i=8000, avg. loss=1.2989482589176842, avg. F1=0.10339755236128526, nonzero outputs=385422.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "54\n",
      "Epoch 8, i=9000, avg. loss=1.2954255385327877, avg. F1=0.10401856043565991, nonzero outputs=429117.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "24\n",
      "train Loss: 12823.5380 F1: 0.1045 Time elapsed: 7789 sec.\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "Epoch 9, i=0, avg. loss=1.1934040784835815, avg. F1=0.19607843137254902, nonzero outputs=46.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "46\n",
      "Epoch 9, i=1000, avg. loss=1.2951923160405308, avg. F1=0.10337649550778077, nonzero outputs=48046.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "75\n",
      "Epoch 9, i=2000, avg. loss=1.2962760656014614, avg. F1=0.10431380385052384, nonzero outputs=89564.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "42\n",
      "Epoch 9, i=3000, avg. loss=1.2928457743960275, avg. F1=0.10433399330578653, nonzero outputs=134269.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "49\n",
      "Epoch 9, i=4000, avg. loss=1.2932635086442614, avg. F1=0.10362893183398662, nonzero outputs=180440.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "28\n",
      "Epoch 9, i=5000, avg. loss=1.2925082851805416, avg. F1=0.10298229194045531, nonzero outputs=226190.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "12\n",
      "Epoch 9, i=6000, avg. loss=1.2930763573512258, avg. F1=0.10307462019909332, nonzero outputs=278468.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "26\n",
      "Epoch 9, i=7000, avg. loss=1.293349910337505, avg. F1=0.1021465988304174, nonzero outputs=335863.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "105\n",
      "Epoch 9, i=8000, avg. loss=1.2943908254394918, avg. F1=0.10265139816671295, nonzero outputs=387353.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "Epoch 9, i=9000, avg. loss=1.295108442736419, avg. F1=0.10284331523360218, nonzero outputs=428988.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "13\n",
      "train Loss: 12808.9333 F1: 0.1038 Time elapsed: 8700 sec.\n",
      "\n",
      "Training complete in 145m 0s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "open(logfile_name, \"w\", encoding=\"utf8\") # clear logs\n",
    "\n",
    "for lr in [0.001]:\n",
    "    # make model with vocab sizes, including placeholder indices\n",
    "    model = LSTMTokenScorer(EMBEDDING_DIM, HIDDEN_DIM)\n",
    "    # loss_function = nn.BCELoss()\n",
    "    pos_weight = torch.full((1,), neg_pos_ratio)\n",
    "    loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)#make positive class more valuable\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    log(f\"Starting model training for lr={lr}...\")\n",
    "    model, losses, f1s = train_model(model, loss_function, optimizer, exp_lr_scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Epoch 0, i=0, avg. loss=1.279767632484436, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=1000, avg. loss=1.4072314542490285, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=2000, avg. loss=1.4122031802597312, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=3000, avg. loss=1.4011241416063598, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=4000, avg. loss=1.3987611889451839, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, losses, f1s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_lr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mprev_losses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mprev_f1s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf1s\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[61], line 67\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs, prev_losses, prev_f1s)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# backward + optimize only if in training phase\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Leonard\\.conda\\envs\\UniAILab\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leonard\\.conda\\envs\\UniAILab\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, losses, f1s = train_model(model, loss_function, optimizer, exp_lr_scheduler, num_epochs=10,\n",
    "                                 prev_losses=losses,\n",
    "                                 prev_f1s=f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(pathjoin(cache_dir, \"losses.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(losses, f)\n",
    "# with open(pathjoin(cache_dir, \"f1s.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(f1s, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3483296632766724"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22cfda92080>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth(x: Iterable, N: int = 5000):\n",
    "    return np.convolve(x, np.ones(N)/N, mode='valid')\n",
    "    \n",
    "plt.plot(smooth(losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
