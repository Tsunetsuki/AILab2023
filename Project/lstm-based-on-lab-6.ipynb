{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# others\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import time\n",
    "\n",
    "# dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import Flowers102\n",
    "\n",
    "# read file \n",
    "import pandas as pd\n",
    "\n",
    "# label\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use TweetEval emotion recognition dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '../../Data/tweeteval/datasets/emotion/'\n",
    "# mapping_file = os.path.join(root, 'mapping.txt')\n",
    "# test_labels_file = os.path.join(root, 'test_labels.txt')\n",
    "# test_text_file = os.path.join(root, 'test_text.txt')\n",
    "# train_labels_file = os.path.join(root, 'train_labels.txt')\n",
    "# train_text_file = os.path.join(root, 'train_text.txt')\n",
    "# val_labels_file = os.path.join(root, 'val_labels.txt')\n",
    "# val_text_file = os.path.join(root, 'val_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_pd = pd.read_csv(mapping_file, sep='\\t', header=None)\n",
    "# test_label_pd = pd.read_csv(test_labels_file, sep='\\t', header=None)\n",
    "# test_dataset = open(test_text_file).read().split('\\n')[:-1] # remove last empty line \n",
    "# train_label_pd = pd.read_csv(train_labels_file, sep='\\t', header=None)\n",
    "# train_dataset = open(train_text_file).read().split('\\n')[:-1] # remove last empty line\n",
    "# val_label_pd = pd.read_csv(val_labels_file, sep='\\t', header=None)\n",
    "# val_dataset = open(val_text_file).read().split('\\n')[:-1] # remove last empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training data\n",
    "- Given: Notes with ranges and labels\n",
    "- Transform into label + lists of tokens with [does token describe label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data/'\n",
    "features_path = os.path.join(root, 'features.csv')\n",
    "patient_notes_path = os.path.join(root, 'patient_notes.csv')\n",
    "sample_submission_path = os.path.join(root, 'sample_submission.csv')\n",
    "test_path = os.path.join(root, 'test.csv')\n",
    "train_path = os.path.join(root, 'train.csv')\n",
    "features = pd.read_csv(features_path, sep=',', header=0)\n",
    "patient_notes = pd.read_csv(patient_notes_path, sep=',', header=0)\n",
    "train_raw = pd.read_csv(train_path, sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>208</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>407</td>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>501</td>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>802</td>\n",
       "      <td>8</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>913</td>\n",
       "      <td>9</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_num  case_num feature_text\n",
       "25           112         1       Female\n",
       "34           208         2       Female\n",
       "66           407         4       Female\n",
       "70           501         5       Female\n",
       "99           700         7       Female\n",
       "110          802         8       Female\n",
       "139          913         9       Female"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unusual_numbers = features[\"feature_num\"].value_counts()[features[\"feature_num\"].value_counts() != 1]\n",
    "# unusual_numbers\n",
    "features[features[\"feature_text\"] == \"Female\"]\n",
    "# features[\"feature_num\"] == "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intro \n",
    "- `case_num`: 0~9, each num belongs their groups ... ? \n",
    "- `pn_num`: the id in patient_notes.csv which is 'pn_history', present the note of each case \n",
    "- `feature_num`: the id in features.csv which is 'feature_num', present the feature of each case \n",
    "- `location`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def df_string2list_of_ints(df_string: str):\n",
    "    df_string = df_string.strip(\"[]\")\n",
    "    if df_string == \"\":\n",
    "        return []\n",
    "    entries = re.split(\",|;\", df_string)\n",
    "    entries = [entry.strip(\" '\") for entry in entries]\n",
    "    ranges = [tuple(int(num_as_str) for num_as_str in entry.split(\" \")) for entry in entries]\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>['203 217']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>['70 91', '176 183']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>['222 258']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14295</th>\n",
       "      <td>95333_912</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>912</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14296</th>\n",
       "      <td>95333_913</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>913</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14297</th>\n",
       "      <td>95333_914</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>914</td>\n",
       "      <td>['photobia']</td>\n",
       "      <td>['274 282']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14298</th>\n",
       "      <td>95333_915</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>915</td>\n",
       "      <td>['no sick contacts']</td>\n",
       "      <td>['421 437']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14299</th>\n",
       "      <td>95333_916</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>916</td>\n",
       "      <td>['Subjective fever']</td>\n",
       "      <td>['314 330']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  case_num  pn_num  feature_num  \\\n",
       "0      00016_000         0      16            0   \n",
       "1      00016_001         0      16            1   \n",
       "2      00016_002         0      16            2   \n",
       "3      00016_003         0      16            3   \n",
       "4      00016_004         0      16            4   \n",
       "...          ...       ...     ...          ...   \n",
       "14295  95333_912         9   95333          912   \n",
       "14296  95333_913         9   95333          913   \n",
       "14297  95333_914         9   95333          914   \n",
       "14298  95333_915         9   95333          915   \n",
       "14299  95333_916         9   95333          916   \n",
       "\n",
       "                                     annotation              location  \n",
       "0              ['dad with recent heart attcak']           ['696 724']  \n",
       "1                 ['mom with \"thyroid disease']           ['668 693']  \n",
       "2                            ['chest pressure']           ['203 217']  \n",
       "3          ['intermittent episodes', 'episode']  ['70 91', '176 183']  \n",
       "4      ['felt as if he were going to pass out']           ['222 258']  \n",
       "...                                         ...                   ...  \n",
       "14295                                        []                    []  \n",
       "14296                                        []                    []  \n",
       "14297                              ['photobia']           ['274 282']  \n",
       "14298                      ['no sick contacts']           ['421 437']  \n",
       "14299                      ['Subjective fever']           ['314 330']  \n",
       "\n",
       "[14300 rows x 6 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                                 annotation                location  \\\n",
       "0          ['dad with recent heart attcak']            [(696, 724)]   \n",
       "1             ['mom with \"thyroid disease']            [(668, 693)]   \n",
       "2                        ['chest pressure']            [(203, 217)]   \n",
       "3      ['intermittent episodes', 'episode']  [(70, 91), (176, 183)]   \n",
       "4  ['felt as if he were going to pass out']            [(222, 258)]   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = train_raw.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "data_merged = data_merged.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "data_merged[\"location\"] = data_merged[\"location\"].apply(df_string2list_of_ints)\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history                location  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...            [(696, 724)]  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...            [(668, 693)]  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...            [(203, 217)]  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  [(70, 91), (176, 183)]  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...            [(222, 258)]  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_merged[[\"feature_text\", \"pn_history\", \"location\", ]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter training data with no location\n",
    "train = train[train[\"location\"].apply(lambda row: len(row) != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset= 9901\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of dataset= {len(train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "- Use spaCy to split the notes into words.\n",
    "\n",
    "Before start using spaCy\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from collections import Counter\n",
    "\n",
    "# use spacy to tokenize the sentence with english model \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Iterable\n",
    "\n",
    "def build_vocab_from_lines(lines: Iterable[str]):\n",
    "    text_to_count_tokens = ' '.join(lines)\n",
    "    doc = nlp(text_to_count_tokens)\n",
    "    # Get the most frequent words, filtering out stop words and punctuation.\n",
    "    word_freq = Counter(token.text.lower() for token in doc if \\\n",
    "                        not token.is_punct and \\\n",
    "                            not token.is_stop and \\\n",
    "                                not token.is_space)\n",
    "    return word_freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded from cache.\n",
      "Top 10 words:  pain, 2, denies, ago, 3, pmh, months, changes, 4, use\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary by getting the most common words across (unique) patient histories\n",
    "import pickle\n",
    "import os\n",
    "from os.path import join as pathjoin\n",
    "\n",
    "cache_dir = \"cache\"\n",
    "cache_file = pathjoin(cache_dir, \"vocab.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    print(\"Vocabulary loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached vocabulary. Creating...\")\n",
    "    \n",
    "    most_common_words = build_vocab_from_lines(train[\"pn_history\"].drop_duplicates())[:5000]\n",
    "    \n",
    "    vocab = {word[0]: idx for idx, word in enumerate(most_common_words)}\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "\n",
    "print(\"Top 10 words: \", \", \".join(list(vocab)[:10]))\n",
    "# [(k, v) for k, v in vocab.items() if v == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vocabulary loaded from cache.\n",
      "Top 10 words:  symptoms, ago, year, history, use, months, pain, family, recent, irregular\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary by getting the most common words across (unique) patient histories\n",
    "import pickle\n",
    "import os\n",
    "from os.path import join as pathjoin\n",
    "\n",
    "cache_dir = \"cache\"\n",
    "cache_file = pathjoin(cache_dir, \"feature_vocab.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        feature_vocab = pickle.load(f)\n",
    "    print(\"Feature vocabulary loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached feature vocabulary. Creating...\")\n",
    "    \n",
    "    most_common_words = build_vocab_from_lines(train[\"feature_text\"].drop_duplicates())[:5000]\n",
    "    \n",
    "    feature_vocab = {word[0]: idx for idx, word in enumerate(most_common_words)}\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(feature_vocab, f)\n",
    "\n",
    "print(\"Top 10 words: \", \", \".join(list(feature_vocab)[:10]))\n",
    "# [(k, v) for k, v in vocab.items() if v == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized patient histories loaded from cache.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "placeholder_index = 5000\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_pn_histories.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_pn_histories = pickle.load(f)\n",
    "    print(\"Tokenized patient histories loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized patient histories. Tokenizing...\")\n",
    "    tokenized_pn_histories: Dict[str, List[str]] = {}\n",
    "    for pn_history in tqdm(train[\"pn_history\"]):\n",
    "        indexed_words = []\n",
    "        if pn_history in tokenized_pn_histories:\n",
    "            continue\n",
    "        for token in nlp(pn_history):\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "                word = token.text.lower()\n",
    "                start_idx = token.idx\n",
    "                end_idx = token.idx + len(token.text)\n",
    "\n",
    "                word_as_number = vocab[word] if word in vocab else placeholder_index\n",
    "                \n",
    "                indexed_words.append({\n",
    "                    \"word_idx\": word_as_number,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": end_idx\n",
    "                })\n",
    "                    \n",
    "        tokenized_pn_histories[pn_history] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_pn_histories, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no cached tokenized features. Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9901/9901 [00:01<00:00, 6662.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "placeholder_index = len(feature_vocab)\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_features.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_features = pickle.load(f)\n",
    "    print(\"Tokenized features loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized features. Tokenizing...\")\n",
    "    tokenized_features: Dict[str, List[str]] = {}\n",
    "    for feature_text in tqdm(train[\"feature_text\"]):\n",
    "        indexed_words = []\n",
    "        if feature_text in tokenized_features:\n",
    "            continue\n",
    "        for token in nlp(feature_text):\n",
    "            if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "                word = token.text.lower()\n",
    "                word_as_number = vocab[word] if word in vocab else placeholder_index\n",
    "                \n",
    "                indexed_words.append(word_as_number)\n",
    "                    \n",
    "        tokenized_features[feature_text] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_features, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Family-history-of-MI-OR-Family-history-of-myocardial-infarction': [106,\n",
       "  29,\n",
       "  127,\n",
       "  106,\n",
       "  29,\n",
       "  1760,\n",
       "  1538],\n",
       " 'Family-history-of-thyroid-disorder': [106, 29, 210, 1067],\n",
       " 'Chest-pressure': [17, 265],\n",
       " 'Intermittent-symptoms': [311, 101],\n",
       " 'Lightheaded': [1066],\n",
       " 'Adderall-use': [303, 9],\n",
       " 'heart-pounding-OR-heart-racing': [54, 166, 54, 349],\n",
       " 'Few-months-duration': [6, 478],\n",
       " '17-year': [115, 34],\n",
       " 'Male': [124],\n",
       " 'Shortness-of-breath': [223, 149],\n",
       " 'No-hair-changes-OR-no-nail-changes-OR-no-temperature-intolerance': [160,\n",
       "  7,\n",
       "  1755,\n",
       "  7,\n",
       "  735,\n",
       "  157],\n",
       " 'Caffeine-use': [533, 9],\n",
       " 'No-vaginal-discharge': [89, 155],\n",
       " 'Not-sexually-active': [20, 16],\n",
       " '20-year': [82, 34],\n",
       " 'Recurrent-bouts-over-past-6-months': [2209, 3352, 253, 6],\n",
       " 'Right-sided-LQ-abdominal-pain-OR-Right-lower-quadrant-abdominal-pain': [418,\n",
       "  606,\n",
       "  253,\n",
       "  86,\n",
       "  0,\n",
       "  418,\n",
       "  377,\n",
       "  505,\n",
       "  86,\n",
       "  0],\n",
       " 'No-urinary-symptoms': [138, 101],\n",
       " 'Normal-LMP-2-weeks-ago-OR-Normal-last-menstrual-period-2-weeks-ago': [85,\n",
       "  253,\n",
       "  11,\n",
       "  3,\n",
       "  85,\n",
       "  182,\n",
       "  253,\n",
       "  11,\n",
       "  3],\n",
       " '8-to-10-hours-of-acute-pain': [162, 253, 98, 818, 0],\n",
       " 'Female': [65],\n",
       " 'Prior-episodes-of-diarrhea': [201, 33, 61],\n",
       " 'No-bloody-bowel-movements': [725, 100, 397],\n",
       " 'Diminished-appetite': [1955, 58],\n",
       " 'Weight-loss': [38, 60],\n",
       " 'Vaginal-dryness': [89, 282],\n",
       " 'Irregular-menses': [93, 298],\n",
       " 'Recent-nausea-vomiting-OR-Recent-flulike-symptoms': [79,\n",
       "  36,\n",
       "  51,\n",
       "  79,\n",
       "  253,\n",
       "  101],\n",
       " 'LMP-2-months-ago-or-Last-menstrual-period-2-months-ago': [253,\n",
       "  6,\n",
       "  3,\n",
       "  182,\n",
       "  253,\n",
       "  6,\n",
       "  3],\n",
       " 'Hot-flashes': [125, 222],\n",
       " 'Irregular-flow-OR-Irregular-frequency-OR-Irregular-intervals': [93,\n",
       "  165,\n",
       "  93,\n",
       "  337,\n",
       "  93,\n",
       "  1409],\n",
       " 'Onset-3-years-ago': [253, 31, 3],\n",
       " 'Heavy-sweating': [117, 183],\n",
       " '44-year': [259, 34],\n",
       " 'Sexually-active': [20, 16],\n",
       " 'Last-Pap-smear-I-year-ago': [179, 294, 34, 3],\n",
       " 'Prior-normal-periods': [201, 85, 23],\n",
       " 'IUD': [368],\n",
       " 'Sleep-disturbance-OR-Early-awakenings': [76, 1517, 333, 731],\n",
       " 'No-premenstrual-symptoms': [1431, 101],\n",
       " 'Stress': [211],\n",
       " 'FHx-of-PUD-OR-Family-history-of-peptic-ulcer-disease': [175,\n",
       "  909,\n",
       "  106,\n",
       "  29,\n",
       "  960,\n",
       "  280,\n",
       "  471],\n",
       " 'Epigastric-discomfort': [263, 632],\n",
       " 'Darker-bowel-movements': [549, 100, 397],\n",
       " 'NSAID-use-OR-Nonsteroidal-anti-inflammatory-drug-use': [1448,\n",
       "  9,\n",
       "  253,\n",
       "  253,\n",
       "  3147,\n",
       "  48,\n",
       "  9],\n",
       " 'burning-OR-gnawing-OR-burning-and-gnawing': [279, 571, 279, 571],\n",
       " 'getting-worse-OR-progressive-OR-symptoms-now-daily': [218,\n",
       "  49,\n",
       "  565,\n",
       "  101,\n",
       "  313],\n",
       " '2-to-3-beers-a-week': [1, 253, 147, 24],\n",
       " 'Intermittent': [311],\n",
       " 'Minimal-to-no-change-with-Tums': [830, 68, 190],\n",
       " 'Nausea': [36],\n",
       " '35-year': [102, 34],\n",
       " 'Post-prandial-bloating-OR-fullness-with-meals': [1309, 253, 389, 956, 621],\n",
       " 'duration-2-months': [253, 6],\n",
       " 'Awakens-at-night': [1910, 77],\n",
       " 'No-blood-in-stool': [171, 262],\n",
       " 'Lack-of-other-thyroid-symptoms': [867, 210, 101],\n",
       " 'anxious-OR-nervous': [794, 572],\n",
       " 'No-depressed-mood': [673, 217],\n",
       " 'Weight-stable': [38, 1521],\n",
       " 'Insomnia': [671],\n",
       " '45-year': [215, 34],\n",
       " 'Stress-due-to-caring-for-elderly-parents': [211, 1331, 777, 395],\n",
       " 'Decreased-appetite': [131, 58],\n",
       " 'Heavy-caffeine-use': [117, 533, 9],\n",
       " 'Associated-SOB-OR-Associated-shortness-of-breath': [73, 64, 73, 223, 149],\n",
       " 'Episodes-of-heart-racing': [33, 54, 349],\n",
       " 'Recent-visit-to-emergency-department-with-negative-workup': [79,\n",
       "  550,\n",
       "  861,\n",
       "  1032,\n",
       "  43,\n",
       "  874],\n",
       " 'No-chest-pain': [17, 0],\n",
       " 'No-illicit-drug-use': [109, 48, 9],\n",
       " 'Increased-frequency-recently': [119, 337, 107],\n",
       " 'Episodes-last-15-to-30-minutes': [33, 253, 253, 232],\n",
       " 'Feels-hot-OR-Feels-clammy': [87, 125, 87, 412],\n",
       " 'Episode-of-hand-numbness-OR-Episode-of-finger-numbness': [99,\n",
       "  940,\n",
       "  195,\n",
       "  99,\n",
       "  812,\n",
       "  195],\n",
       " 'Increased-stress': [119, 211],\n",
       " '26-year': [236, 34],\n",
       " 'Onset-5-years-ago': [253, 31, 3],\n",
       " 'Associated-feeling-of-impending-doom': [73, 91, 821, 641],\n",
       " 'Fatigue-OR-Difficulty-concentrating': [196, 114, 765],\n",
       " 'No-caffeine-use': [533, 9],\n",
       " 'Associated-nausea': [73, 36],\n",
       " 'Associated-throat-tightness': [73, 176, 464],\n",
       " 'Subjective-fevers': [366, 230],\n",
       " 'Recent-upper-respiratory-symptoms': [79, 624, 2008, 101],\n",
       " 'Worse-with-deep-breath-OR-pleuritic': [49, 216, 149, 1476],\n",
       " 'Exercise-induced-asthma': [141, 205, 103],\n",
       " 'Chest-pain': [17, 0],\n",
       " 'Duration-x-1-day': [478, 253, 22],\n",
       " 'No-shortness-of-breath': [223, 149],\n",
       " 'No-relief-with-asthma-inhaler': [307, 103, 251],\n",
       " 'Sharp-OR-stabbing-OR-7-to-8-out-of-10-on-pain-scale': [229,\n",
       "  810,\n",
       "  253,\n",
       "  253,\n",
       "  253,\n",
       "  0,\n",
       "  1095],\n",
       " 'Recent-heavy-lifting-at-work-OR-recent-rock-climbing': [79,\n",
       "  117,\n",
       "  1246,\n",
       "  142,\n",
       "  79,\n",
       "  497,\n",
       "  750],\n",
       " 'heavy-periods-OR-irregular-periods': [117, 23, 93, 23],\n",
       " 'Last-menstrual-period-2-months-ago': [182, 253, 6, 3],\n",
       " 'Unprotected-Sex': [1241, 261],\n",
       " 'symptoms-for-6-months': [101, 253, 6],\n",
       " 'Weight-Gain': [38, 240],\n",
       " 'Fatigue': [196],\n",
       " 'Infertility-HX-OR-Infertility-history': [692, 40, 692, 29],\n",
       " 'Increased-appetite': [119, 58],\n",
       " 'Son-died-3-weeks-ago': [104, 253, 11, 3],\n",
       " 'tossing-and-turning': [841, 877],\n",
       " '67-year': [248, 34],\n",
       " 'Difficulty-falling-asleep': [114, 156, 111],\n",
       " 'duration-3-weeks': [253, 11],\n",
       " 'Sleeping-medication-ineffective': [113, 444, 253],\n",
       " 'Diminished-energy-OR-feeling-drained': [1955, 213, 91, 1058],\n",
       " 'loss-of-interest': [60, 267],\n",
       " 'Visual-hallucination-once': [361, 1054],\n",
       " 'Early-wakening': [333, 253],\n",
       " 'No-suicidal-ideations': [341, 1335],\n",
       " 'Difficulty-with-sleep': [114, 76],\n",
       " 'FHx-of-depression-OR-Family-history-of-depression': [175, 260, 106, 29, 260],\n",
       " 'Auditory-hallucination-once': [815, 1054],\n",
       " 'Unsuccessful-napping': [2652, 253],\n",
       " 'Hallucinations-after-taking-Ambien': [487, 150, 219],\n",
       " 'No-relief-with-Motrin-OR-no-relief-with-tylenol': [307, 214, 307, 62],\n",
       " '1-day-duration-OR-2-days-duration': [13, 22, 478, 253, 10, 478],\n",
       " 'Vomiting': [51],\n",
       " 'Shares-an-apartment': [2988, 1122],\n",
       " 'Photophobia': [402],\n",
       " 'No-known-illness-contacts': [585, 545, 257],\n",
       " 'Subjective-fever': [366, 59],\n",
       " 'viral-symptoms-OR-rhinorrhea-OR-scratchy-throat': [4963, 101, 785, 967, 176],\n",
       " 'Family-history-of-migraines': [106, 29, 369],\n",
       " 'Myalgias': [1046],\n",
       " 'Global-headache-OR-diffuse-headache': [2052, 55, 999, 55],\n",
       " 'No-rash': [616],\n",
       " 'Neck-pain': [228, 0],\n",
       " 'Meningococcal-vaccine-status-unknown': [253, 1681, 4770, 1271]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follow the example described here. Use the same architecture, but:\n",
    "  - only use the last output of the LSTM in the loss function\n",
    "  - use an embedding dim of 128\n",
    "  - use a hidden dim of 256.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature-relevancy of tokens via char ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9901it [00:05, 1822.62it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokens_with_scores = dict()\n",
    "for i, (feature_text, pn_history, location) in tqdm(train.iterrows()):\n",
    "    tokenized_history = tokenized_pn_histories[pn_history]\n",
    "    tokens_with_scores = []\n",
    "    for token in tokenized_history:\n",
    "        for feature_relevant_range in location:\n",
    "            token_start, token_end = token[\"start\"], token[\"end\"]\n",
    "            range_start, range_end = feature_relevant_range[0], feature_relevant_range[1]\n",
    "            \n",
    "            percentage_of_token_in_range = max(min(token_end, range_end)+1 - max(token_start, range_start), 0) / (token_end+1 - token_start)\n",
    "            # if percentage_of_token_in_range > 0:\n",
    "            #     print(percentage_of_token_in_range, token, feature_relevant_range)\n",
    "            tokens_with_scores.append({\"word\": token[\"word_idx\"], \"score\": int(percentage_of_token_in_range > 0.9)})\n",
    "    \n",
    "    train_tokens_with_scores[i] = {\n",
    "                                    \"scored_tokens\": tokens_with_scores,\n",
    "                                    \"feature_tokens\": [tokenized_features[feature_text]]\n",
    "                                   }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"data format:\")\n",
    "# train_tokens_with_scores[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring feature label into training data for LSTM!\n",
    "- must encode the feature text into the LSTM input data to train. How to do it?\n",
    "- 2 vocabs\n",
    "\n",
    "Layers in LSTM Model:\n",
    "1. embed feature tokens\n",
    "2. lstm feature -> constant size vector\n",
    "\n",
    "3. pass to 2nd lstm\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128 # 將word轉換成維度為128的向量\n",
    "HIDDEN_DIM = 256 # 在RNN或LSTM中模型中隱藏曾神經元的數量大小\n",
    "word_to_ix = vocab # 詞彙表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, dropout=0.0):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=dropout)\n",
    "\n",
    "        self.hidden2score = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        # Take only the last output of the LSTM\n",
    "        # last_output = lstm_out[-1].view(1, -1)  # Selecting the last output\n",
    "        output = lstm_out.view(len(sentence), -1)\n",
    "        tag_space = self.hidden2score(output) # 將LSTM模型的最後輸出轉換成 詞標籤 空間\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_to_ix)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size 要添加 1 因為如果 sentence 中有出現沒在 vocab 中的單字，使用 5000 來代替，所以要加 1\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix)+1, len(tag_to_ix))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence to sequence \n",
    "def prepare_sentence_sequence(seq, to_ix):\n",
    "    idx = []\n",
    "    # use spacy to tokenize the sentence \n",
    "    for token in nlp(seq):\n",
    "        # filter out the punctuation and stop words and space \n",
    "        if not token.is_punct and not token.is_stop and not token.is_space:\n",
    "            word = token.text\n",
    "            # if the token is in the top 5000 words in the vocab, add its index to the list\n",
    "            if word in to_ix:\n",
    "                idx.append(to_ix[word])\n",
    "            else:\n",
    "                # else add the index of the placeholder token\n",
    "                idx.append(placeholder_index)\n",
    "    return torch.tensor(idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(val, to_ix):\n",
    "    result = []\n",
    "    for k, v in to_ix.items(): \n",
    "        if val == k:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return torch.tensor(result, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping)\n",
    "print(one_hot_encode(2, tag_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i. ; j有3個、i有5個字，i, j 表示第i個字的第j個tag的分數\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "sentence_idx = 1\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sentence_sequence(train_dataset[sentence_idx], word_to_ix)\n",
    "    labels = one_hot_encode(train_label_pd[0][sentence_idx], tag_to_ix)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    result_idx = torch.argmax(outputs).item()\n",
    "    loss = loss_function(outputs[0], labels)\n",
    "\n",
    "    print(f'First Sentense = {train_dataset[sentence_idx]}')\n",
    "    print(f'Sentense to tensor = {inputs}')\n",
    "    print(f'Sentense of result to tensor = {labels}')\n",
    "    print(f'tag_scores = {outputs}')\n",
    "    print(f'loss = {loss}')\n",
    "    print(f'preds = {preds}')\n",
    "    print(f'result = {result_idx}, ans = {train_label_pd[0][sentence_idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train': train_dataset, 'test': test_dataset}\n",
    "resultloaders = {'train': train_label_pd[0].tolist(), 'test': test_label_pd[0].tolist()}\n",
    "dataset_sizes = {x: len(dataloaders[x]) for x in ['train', 'test']}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'train'\n",
    "\n",
    "len(resultloaders[phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=1):\n",
    "    since = time.time()\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'test']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else: \n",
    "                    model.eval()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for input, label in zip(dataloaders[phase], resultloaders[phase]):\n",
    "                    inputs_vector = prepare_sentence_sequence(input, word_to_ix)\n",
    "                    labels_vector = one_hot_encode(label, tag_to_ix)\n",
    "                    \n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs_vector) # 取得針對每個emotion的預測結果tensor (e.g. tensor([[-1.3948, -1.4476, -1.3804, -1.3261]]))\n",
    "                        pred = torch.argmax(outputs).item() # 取得最大值的index (e.g. 2)\n",
    "                        loss = criterion(outputs[0], labels_vector) # 外面還有一層，只需取得內層 [-1.3948, -1.4476, -1.3804, -1.3261] 與 [0, 0, 1, 0] 的計算loss\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item()\n",
    "                    if pred == label:\n",
    "                        running_corrects += 1\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Time elapsed: {round((time.time() - since))} sec.')\n",
    "                \n",
    "                # deep copy the model\n",
    "                if phase == 'test' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, loss_function, optimizer, exp_lr_scheduler, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size 要添加 2 因為如果 sentence 中有出現沒在 vocab 中的單字，使用 5001 來代替，所以要加 1\n",
    "model_LSTM = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix)+1, len(tag_to_ix), dropout=0.5)\n",
    "loss_function_LSTM = nn.CrossEntropyLoss()\n",
    "optimizer_LSTM = optim.SGD(model_LSTM.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler_LSTM = lr_scheduler.StepLR(optimizer_LSTM, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM = train_model(model_LSTM, loss_function_LSTM, optimizer_LSTM, exp_lr_scheduler_LSTM, num_epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
