{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# others\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "import time\n",
    "\n",
    "# dataset\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets import Flowers102\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# read file \n",
    "import pandas as pd\n",
    "\n",
    "# label\n",
    "from scipy.io import loadmat\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use TweetEval emotion recognition dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '../../Data/tweeteval/datasets/emotion/'\n",
    "# mapping_file = os.path.join(root, 'mapping.txt')\n",
    "# test_labels_file = os.path.join(root, 'test_labels.txt')\n",
    "# test_text_file = os.path.join(root, 'test_text.txt')\n",
    "# train_labels_file = os.path.join(root, 'train_labels.txt')\n",
    "# train_text_file = os.path.join(root, 'train_text.txt')\n",
    "# val_labels_file = os.path.join(root, 'val_labels.txt')\n",
    "# val_text_file = os.path.join(root, 'val_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_pd = pd.read_csv(mapping_file, sep='\\t', header=None)\n",
    "# test_label_pd = pd.read_csv(test_labels_file, sep='\\t', header=None)\n",
    "# test_dataset = open(test_text_file).read().split('\\n')[:-1] # remove last empty line \n",
    "# train_label_pd = pd.read_csv(train_labels_file, sep='\\t', header=None)\n",
    "# train_dataset = open(train_text_file).read().split('\\n')[:-1] # remove last empty line\n",
    "# val_label_pd = pd.read_csv(val_labels_file, sep='\\t', header=None)\n",
    "# val_dataset = open(val_text_file).read().split('\\n')[:-1] # remove last empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training data\n",
    "- Given: Notes with ranges and labels\n",
    "- Transform into label + lists of tokens with [does token describe label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data/'\n",
    "features_path = os.path.join(root, 'features.csv')\n",
    "patient_notes_path = os.path.join(root, 'patient_notes.csv')\n",
    "sample_submission_path = os.path.join(root, 'sample_submission.csv')\n",
    "test_path = os.path.join(root, 'test.csv')\n",
    "train_path = os.path.join(root, 'train.csv')\n",
    "features = pd.read_csv(features_path, sep=',', header=0)\n",
    "patient_notes = pd.read_csv(patient_notes_path, sep=',', header=0)\n",
    "train_raw = pd.read_csv(train_path, sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>208</td>\n",
       "      <td>2</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>407</td>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>501</td>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>802</td>\n",
       "      <td>8</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>913</td>\n",
       "      <td>9</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_num  case_num feature_text\n",
       "25           112         1       Female\n",
       "34           208         2       Female\n",
       "66           407         4       Female\n",
       "70           501         5       Female\n",
       "99           700         7       Female\n",
       "110          802         8       Female\n",
       "139          913         9       Female"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unusual_numbers = features[\"feature_num\"].value_counts()[features[\"feature_num\"].value_counts() != 1]\n",
    "# unusual_numbers\n",
    "features[features[\"feature_text\"] == \"Female\"]\n",
    "# features[\"feature_num\"] == "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intro \n",
    "- `case_num`: 0~9, each num belongs their groups ... ? \n",
    "- `pn_num`: the id in patient_notes.csv which is 'pn_history', present the note of each case \n",
    "- `feature_num`: the id in features.csv which is 'feature_num', present the feature of each case \n",
    "- `location`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def df_string2list_of_ints(df_string: str):\n",
    "    df_string = df_string.strip(\"[]\")\n",
    "    if df_string == \"\":\n",
    "        return []\n",
    "    entries = re.split(\",|;\", df_string)\n",
    "    entries = [entry.strip(\" '\") for entry in entries]\n",
    "    ranges = [tuple(int(num_as_str) for num_as_str in entry.split(\" \")) for entry in entries]\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>['203 217']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>['70 91', '176 183']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>['222 258']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14295</th>\n",
       "      <td>95333_912</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>912</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14296</th>\n",
       "      <td>95333_913</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>913</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14297</th>\n",
       "      <td>95333_914</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>914</td>\n",
       "      <td>['photobia']</td>\n",
       "      <td>['274 282']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14298</th>\n",
       "      <td>95333_915</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>915</td>\n",
       "      <td>['no sick contacts']</td>\n",
       "      <td>['421 437']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14299</th>\n",
       "      <td>95333_916</td>\n",
       "      <td>9</td>\n",
       "      <td>95333</td>\n",
       "      <td>916</td>\n",
       "      <td>['Subjective fever']</td>\n",
       "      <td>['314 330']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14300 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  case_num  pn_num  feature_num  \\\n",
       "0      00016_000         0      16            0   \n",
       "1      00016_001         0      16            1   \n",
       "2      00016_002         0      16            2   \n",
       "3      00016_003         0      16            3   \n",
       "4      00016_004         0      16            4   \n",
       "...          ...       ...     ...          ...   \n",
       "14295  95333_912         9   95333          912   \n",
       "14296  95333_913         9   95333          913   \n",
       "14297  95333_914         9   95333          914   \n",
       "14298  95333_915         9   95333          915   \n",
       "14299  95333_916         9   95333          916   \n",
       "\n",
       "                                     annotation              location  \n",
       "0              ['dad with recent heart attcak']           ['696 724']  \n",
       "1                 ['mom with \"thyroid disease']           ['668 693']  \n",
       "2                            ['chest pressure']           ['203 217']  \n",
       "3          ['intermittent episodes', 'episode']  ['70 91', '176 183']  \n",
       "4      ['felt as if he were going to pass out']           ['222 258']  \n",
       "...                                         ...                   ...  \n",
       "14295                                        []                    []  \n",
       "14296                                        []                    []  \n",
       "14297                              ['photobia']           ['274 282']  \n",
       "14298                      ['no sick contacts']           ['421 437']  \n",
       "14299                      ['Subjective fever']           ['314 330']  \n",
       "\n",
       "[14300 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                                 annotation                location  \\\n",
       "0          ['dad with recent heart attcak']            [(696, 724)]   \n",
       "1             ['mom with \"thyroid disease']            [(668, 693)]   \n",
       "2                        ['chest pressure']            [(203, 217)]   \n",
       "3      ['intermittent episodes', 'episode']  [(70, 91), (176, 183)]   \n",
       "4  ['felt as if he were going to pass out']            [(222, 258)]   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_merged = train_raw.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "data_merged = data_merged.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "data_merged[\"location\"] = data_merged[\"location\"].apply(df_string2list_of_ints)\n",
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(696, 724)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(668, 693)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(203, 217)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(70, 91), (176, 183)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>[(222, 258)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history                location  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...            [(696, 724)]  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...            [(668, 693)]  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...            [(203, 217)]  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  [(70, 91), (176, 183)]  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...            [(222, 258)]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data_merged[[\"feature_text\", \"pn_history\", \"location\", ]]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter training data with no location\n",
    "train = train[train[\"location\"].apply(lambda row: len(row) != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset= 9901\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of dataset= {len(train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "- Use spaCy to split the notes into words.\n",
    "\n",
    "Before start using spaCy\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "# from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Leonard\\.conda\\envs\\UniAILab\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "# tokenizer.encode_plus(\"hello i am Drunk\", return_offsets_mapping=True, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pathjoin\n",
    "import pickle\n",
    "cache_dir = \"cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1045, 2572, 7144, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (6, 7), (8, 10), (11, 16), (0, 0)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"hello i am Drunk\", return_offsets_mapping=True, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "BERT_FP = ('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embed_matrix():\n",
    "    bert = BertModel.from_pretrained(BERT_FP)\n",
    "    bert_embeddings = list(bert.children())[0]\n",
    "    bert_word_embeddings = list(bert_embeddings.children())[0]\n",
    "    mat = bert_word_embeddings.weight.data.numpy()\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_bert_embed_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache, lru_cache\n",
    "\n",
    "def embed_seq(s: Iterable[int]):\n",
    "    return np.array([onehot_word(word_id) @ embedding_matrix for word_id in s])\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def embed_word(word_id: int):\n",
    "    return onehot_word(word_id) @ embedding_matrix\n",
    "\n",
    "def onehot_word(a: int):\n",
    "    oh = np.zeros(30522, dtype=int)\n",
    "    oh[a] = 1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized patient histories loaded from cache.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_pn_histories.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_pn_histories = pickle.load(f)\n",
    "    print(\"Tokenized patient histories loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized patient histories. Tokenizing...\")\n",
    "    tokenized_pn_histories: Dict[str, List[Dict]] = {}\n",
    "    for pn_history in tqdm(train[\"pn_history\"]):\n",
    "        indexed_words = []\n",
    "        if pn_history in tokenized_pn_histories:\n",
    "            continue\n",
    "\n",
    "        tokenized = tokenizer.encode_plus(pn_history, return_offsets_mapping=True, add_special_tokens=True)\n",
    "\n",
    "        for word, offset_mapping in zip(tokenized[\"input_ids\"], tokenized[\"offset_mapping\"]):\n",
    "            embedded_token = embed_word(word)\n",
    "\n",
    "            indexed_words.append({\n",
    "                \"word_id\": word,\n",
    "                \"embedded\": embedded_token,\n",
    "                \"range\": offset_mapping\n",
    "            })\n",
    "                    \n",
    "        tokenized_pn_histories[pn_history] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_pn_histories, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure:\n",
    "tokenized_pn_histories\n",
    "hist_id -> [tokens]\n",
    "token -> ['word_id', 'embedded', 'range']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_id': 6522,\n",
       " 'embedded': array([-0.01908113, -0.04173963, -0.06673449, -0.01592105,  0.04393673,\n",
       "        -0.0045615 , -0.07181855, -0.03923002, -0.06660692, -0.05208729,\n",
       "        -0.05683259, -0.07923082, -0.07103121, -0.03811198,  0.04058741,\n",
       "        -0.07675945, -0.03249264,  0.01277649,  0.00591493, -0.02265794,\n",
       "        -0.07537558, -0.0377069 , -0.02679249, -0.02630543, -0.10752729,\n",
       "        -0.07387543, -0.03104966, -0.05502456, -0.01574389,  0.03902053,\n",
       "        -0.03393734, -0.04892192, -0.01713544, -0.01373354, -0.03406208,\n",
       "        -0.03928473, -0.01480401, -0.0745343 , -0.04668886,  0.04388529,\n",
       "         0.04298296, -0.02789518, -0.06555539, -0.01023453, -0.05270426,\n",
       "        -0.01351104, -0.01696531, -0.02604903, -0.06805495, -0.0114427 ,\n",
       "        -0.07451096, -0.08470455, -0.06320265, -0.07705162,  0.00924673,\n",
       "        -0.06076133, -0.03972978, -0.00976295,  0.04022013,  0.00853729,\n",
       "        -0.03172083, -0.07831176, -0.04030865, -0.01483213, -0.02548516,\n",
       "        -0.02037099,  0.02583888, -0.00601115,  0.0005211 , -0.0503305 ,\n",
       "         0.01535861, -0.06918605,  0.06422725, -0.04100475, -0.01852632,\n",
       "        -0.00795093, -0.01691452,  0.12692392,  0.0025903 , -0.08476964,\n",
       "        -0.05988051,  0.0401165 , -0.01006817,  0.01409894,  0.02960487,\n",
       "        -0.00199787, -0.0436109 , -0.01663729,  0.01638095,  0.00484425,\n",
       "         0.03679116, -0.02578565, -0.06917788, -0.05262199, -0.01385328,\n",
       "        -0.07637186,  0.03965394, -0.04200633, -0.12093006,  0.01642366,\n",
       "         0.05532187,  0.00137352, -0.02469023,  0.01579396, -0.03945262,\n",
       "        -0.04573373, -0.03275239, -0.05293219,  0.01127648, -0.03078214,\n",
       "        -0.05783765,  0.01816563,  0.00907583, -0.07086688, -0.01004548,\n",
       "        -0.01934193, -0.00457785, -0.05536553, -0.0459183 , -0.0672041 ,\n",
       "         0.03481179,  0.0206055 , -0.01818438, -0.00539505, -0.07162139,\n",
       "        -0.07882681, -0.02764613, -0.09358473, -0.06332196, -0.06640781,\n",
       "        -0.08489089, -0.00042765,  0.12575683, -0.05091361, -0.05330716,\n",
       "         0.01216682, -0.0151412 , -0.00678979, -0.01782138,  0.1514968 ,\n",
       "        -0.05412668, -0.08116943,  0.02427371, -0.03710703, -0.01445157,\n",
       "        -0.02355842,  0.01155926, -0.03040035, -0.01847903, -0.04440971,\n",
       "        -0.00966275,  0.00402587, -0.05210217,  0.04715672, -0.0018505 ,\n",
       "        -0.03760519, -0.0181003 , -0.06754413, -0.06897832,  0.01820193,\n",
       "        -0.04256851,  0.01031271, -0.10166597, -0.06834054, -0.08312954,\n",
       "        -0.09930322,  0.01107772, -0.01796713,  0.03765639, -0.03941826,\n",
       "        -0.01618453,  0.00628192, -0.0043477 , -0.03597885, -0.00878117,\n",
       "         0.08767164, -0.01497303, -0.02665254, -0.01302524, -0.02829309,\n",
       "        -0.08718653, -0.00174683, -0.0600834 , -0.05688876, -0.02207683,\n",
       "        -0.00959872, -0.01209632, -0.05893016, -0.0134788 , -0.05507741,\n",
       "        -0.0325698 , -0.05177484, -0.08775931, -0.0305383 , -0.00073222,\n",
       "        -0.07700045, -0.04340889, -0.00715729, -0.04786425,  0.0315089 ,\n",
       "        -0.07755212, -0.02716055,  0.04729325, -0.08755802, -0.02731228,\n",
       "         0.00718454, -0.03118062, -0.07835725, -0.10915846,  0.00159168,\n",
       "        -0.00731354, -0.01225757, -0.01914476,  0.03351985,  0.01089427,\n",
       "        -0.05832738, -0.14224371, -0.03576584,  0.04456902, -0.000757  ,\n",
       "         0.03093938, -0.02885435, -0.00280817,  0.01134229, -0.04378517,\n",
       "         0.0615575 ,  0.02901632, -0.0430181 ,  0.01795693, -0.00810694,\n",
       "        -0.06790127, -0.00443572,  0.01635651,  0.00989045, -0.04803739,\n",
       "         0.0139738 , -0.06465024, -0.07707804, -0.02952593, -0.09800228,\n",
       "        -0.03218525,  0.00706195, -0.01531379, -0.03954516, -0.01203465,\n",
       "         0.043837  ,  0.00101034, -0.01836536,  0.00685722, -0.07278425,\n",
       "        -0.04377524,  0.01495099, -0.07974492,  0.01325171, -0.02169541,\n",
       "         0.01997804,  0.01421765, -0.03940663, -0.06355347,  0.03975793,\n",
       "        -0.02328743,  0.0563975 ,  0.04225071, -0.03109835, -0.01666352,\n",
       "        -0.03430907, -0.00860342,  0.01001961,  0.00359407,  0.01288842,\n",
       "         0.00498054, -0.00509374, -0.04782482, -0.04231593, -0.01432745,\n",
       "        -0.09286752, -0.04726689, -0.02346229, -0.05374473,  0.01145206,\n",
       "         0.03026417, -0.06029093, -0.00253558,  0.06202162, -0.05841234,\n",
       "        -0.03516613, -0.00256508, -0.08468466, -0.02153719, -0.0368582 ,\n",
       "        -0.03436837,  0.05374306,  0.04055554, -0.07779582, -0.00427109,\n",
       "        -0.05840474, -0.0285328 , -0.02278991, -0.15128602,  0.00052356,\n",
       "        -0.08083797, -0.0130174 , -0.03916908, -0.01433287, -0.14204586,\n",
       "         0.04276562,  0.03892587, -0.02843608,  0.01542204, -0.1145288 ,\n",
       "        -0.02506554,  0.03367257, -0.00213193,  0.01320693, -0.04444836,\n",
       "         0.01883331, -0.05896155, -0.06115995, -0.04090078, -0.01691943,\n",
       "         0.00405402,  0.00108538, -0.09178237, -0.05631986, -0.06067519,\n",
       "        -0.02987276, -0.04702154, -0.02576351, -0.0612037 , -0.05243647,\n",
       "        -0.01006082,  0.01848766,  0.03871314,  0.02257213, -0.07605862,\n",
       "         0.00673083, -0.0318241 ,  0.01370405, -0.02269986, -0.00097142,\n",
       "        -0.02942256,  0.0227625 ,  0.00409969,  0.01073872,  0.08637159,\n",
       "         0.04685065,  0.02199768, -0.03398025, -0.03657096, -0.05069414,\n",
       "        -0.07068926, -0.03799183, -0.07610273,  0.07176458, -0.03762812,\n",
       "         0.02382901, -0.02638155, -0.00514337, -0.02068941, -0.06532674,\n",
       "        -0.02516313, -0.08015107, -0.05505192, -0.06204863,  0.05124347,\n",
       "         0.01688904,  0.01903924, -0.07061321, -0.01526833,  0.01070135,\n",
       "         0.01444305, -0.03720876, -0.04877125, -0.0189193 , -0.0124364 ,\n",
       "        -0.02761597, -0.06310479, -0.06761326, -0.01406777, -0.05796858,\n",
       "        -0.0277304 , -0.03851173,  0.00873912, -0.1048217 , -0.04117056,\n",
       "        -0.04438694, -0.01083885, -0.05739474,  0.00505072, -0.00536615,\n",
       "        -0.01123734, -0.04380842,  0.06836498, -0.04949834,  0.033135  ,\n",
       "        -0.07762786, -0.05575214, -0.02798773, -0.04192498, -0.04895433,\n",
       "        -0.06437089, -0.11018419, -0.00538384,  0.01429083, -0.02152648,\n",
       "        -0.0496478 , -0.07264582, -0.04929105, -0.01951207, -0.0619363 ,\n",
       "        -0.01403196, -0.13095725, -0.07425182, -0.02866054,  0.00218311,\n",
       "        -0.02678812, -0.02157573, -0.04306534, -0.00745715, -0.03779166,\n",
       "        -0.05122556,  0.02723394,  0.00819311, -0.0768657 ,  0.0272549 ,\n",
       "        -0.01246105,  0.01943599, -0.06547591,  0.01579106, -0.06771848,\n",
       "        -0.07174174, -0.00147516, -0.04715826, -0.04099685, -0.06936637,\n",
       "         0.01602412, -0.06892688, -0.05970216,  0.01043891, -0.01967197,\n",
       "        -0.0097134 , -0.0019278 , -0.07333077,  0.08527321,  0.01241145,\n",
       "        -0.08843168, -0.02522356,  0.01829377, -0.02715925, -0.04659085,\n",
       "         0.04570746, -0.03056694, -0.00275488, -0.12433532, -0.0351824 ,\n",
       "        -0.06592382, -0.03078032,  0.00180635, -0.02113222,  0.09001572,\n",
       "        -0.01052239, -0.02229475, -0.03569242,  0.00036761, -0.01738564,\n",
       "        -0.03722301, -0.00113262,  0.05362143, -0.03955411,  0.002896  ,\n",
       "        -0.05952486, -0.05586326, -0.08437575,  0.01152003, -0.03250872,\n",
       "        -0.03138869,  0.01692799, -0.02602969, -0.11407799, -0.09128868,\n",
       "        -0.07265399, -0.00359682,  0.12208072, -0.03638796,  0.01464378,\n",
       "        -0.0503557 ,  0.0402507 , -0.06297771,  0.01648491, -0.00340791,\n",
       "        -0.04787293, -0.08782237, -0.06150886, -0.06769664, -0.00179957,\n",
       "        -0.03849053,  0.09978071,  0.00915475, -0.07090563, -0.10180055,\n",
       "        -0.07990018,  0.00577071,  0.00488389, -0.07092109, -0.01440103,\n",
       "        -0.05694594,  0.00914358, -0.04349818, -0.01060246, -0.01127336,\n",
       "         0.01695217, -0.0403586 ,  0.01587945, -0.08662798, -0.01474116,\n",
       "        -0.06371089, -0.04907921, -0.0448628 ,  0.05979871, -0.02591555,\n",
       "         0.03503687, -0.02138414, -0.0339639 ,  0.00137897,  0.01739392,\n",
       "        -0.01392002, -0.04831953, -0.06016075, -0.06491395,  0.01704529,\n",
       "        -0.04203914, -0.04868194,  0.03360818, -0.02246774, -0.02588319,\n",
       "        -0.02498071, -0.06587233, -0.02461761, -0.08513182,  0.08144202,\n",
       "        -0.04022377,  0.01476349,  0.0111707 , -0.03091095,  0.03112956,\n",
       "        -0.00713935,  0.01671844, -0.10406578, -0.04446519, -0.03654759,\n",
       "        -0.00090015, -0.0822074 , -0.04481355,  0.01534062,  0.03918026,\n",
       "        -0.01107791,  0.01082302, -0.00779742, -0.03331521, -0.0068609 ,\n",
       "         0.02225946, -0.03062034, -0.00548296, -0.04015785,  0.00021749,\n",
       "        -0.03255514, -0.02342268,  0.00763948, -0.08413568,  0.00275202,\n",
       "         0.02455448, -0.02556607,  0.00588004, -0.04904316, -0.00150067,\n",
       "        -0.02676761, -0.11770628,  0.01419074, -0.01508235, -0.03976258,\n",
       "        -0.02756159, -0.08268955, -0.0486187 , -0.06423166,  0.01544593,\n",
       "        -0.02693414,  0.01646638, -0.0501403 , -0.03779647,  0.01287141,\n",
       "        -0.00923243, -0.00107006,  0.00207343, -0.04434131, -0.00433736,\n",
       "        -0.0507568 , -0.05762206, -0.04867698, -0.00996203, -0.00558443,\n",
       "        -0.06657334,  0.01561775, -0.05053767, -0.03807011, -0.01189713,\n",
       "        -0.06210907, -0.0495418 , -0.07157552, -0.09053706,  0.00181156,\n",
       "        -0.04218731, -0.04662285, -0.12897606,  0.03579992,  0.0100256 ,\n",
       "         0.02326034, -0.06064779, -0.04980562, -0.06982776, -0.01257232,\n",
       "        -0.05897349, -0.04919791, -0.04425377,  0.02722542, -0.03964664,\n",
       "        -0.00589339, -0.02847223, -0.05516527,  0.0109106 , -0.03825077,\n",
       "        -0.01748873, -0.04376196, -0.0285569 , -0.02454232, -0.0121718 ,\n",
       "        -0.05331823, -0.03772478,  0.00095005, -0.02478909, -0.02408331,\n",
       "         0.03763863, -0.00808368, -0.00888659,  0.02268378, -0.01658686,\n",
       "        -0.0513445 , -0.01339481, -0.00470968,  0.00539002, -0.0560668 ,\n",
       "         0.0124986 , -0.05672762, -0.0138257 , -0.05866195,  0.0203    ,\n",
       "         0.03280495, -0.05672722, -0.05720117, -0.01101085, -0.00529359,\n",
       "         0.00630592, -0.05287591, -0.09404698,  0.00321512, -0.00732199,\n",
       "        -0.1058289 ,  0.04019904, -0.05588099,  0.01275758, -0.02172038,\n",
       "         0.08089678, -0.08789966, -0.04367155, -0.09806436,  0.02455979,\n",
       "         0.025256  , -0.07345985, -0.04881034, -0.08482597,  0.01101647,\n",
       "         0.00815555, -0.08573112, -0.02448465, -0.07433309, -0.00483828,\n",
       "        -0.0585055 , -0.00990027, -0.08444568, -0.00440183, -0.05598024,\n",
       "        -0.07475943, -0.02485162,  0.00843887, -0.08426636, -0.02037607,\n",
       "        -0.04357914, -0.03103529, -0.03505394, -0.03477909, -0.01211137,\n",
       "        -0.05613294,  0.00179894, -0.05551952, -0.07097323, -0.02187284,\n",
       "        -0.08530426,  0.02231892,  0.00967894, -0.02867814, -0.06385659,\n",
       "         0.00504741,  0.0078729 , -0.04916536, -0.04518002,  0.0471171 ,\n",
       "         0.03352923,  0.03048048, -0.01245142, -0.01099646, -0.00735218,\n",
       "         0.0011658 , -0.03855402, -0.03913379, -0.00843964, -0.03549154,\n",
       "         0.03258345, -0.07661324, -0.03411501, -0.05275038, -0.02779922,\n",
       "        -0.00328957, -0.04803751,  0.03381702, -0.03121832, -0.10256522,\n",
       "        -0.09645248, -0.03838626, -0.04535397, -0.06665562, -0.0674329 ,\n",
       "         0.01928262, -0.04901787, -0.0272354 , -0.07807704,  0.01493715,\n",
       "        -0.0357038 ,  0.02745784, -0.04218743,  0.00024408,  0.02782501,\n",
       "         0.01505823, -0.01638191, -0.02046041,  0.01921135, -0.02198615,\n",
       "        -0.04741193, -0.04992861, -0.0194096 , -0.06105652, -0.00595623,\n",
       "        -0.01816186,  0.0177033 ,  0.00843524,  0.00802204, -0.04932498,\n",
       "        -0.02861249, -0.09120153, -0.03054466]),\n",
       " 'range': (0, 2)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(tokenized_pn_histories.values())[0][0].keys()\n",
    "list(tokenized_pn_histories.values())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized features loaded from cache.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "cache_file = pathjoin(cache_dir, \"tokenized_features.pkl\")\n",
    "if os.path.isfile(cache_file):\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        tokenized_features = pickle.load(f)\n",
    "    print(\"Tokenized features loaded from cache.\")\n",
    "else:\n",
    "    print(\"Found no cached tokenized features. Tokenizing...\")\n",
    "    tokenized_features: Dict[str, List[str]] = {}\n",
    "    for feature_text in tqdm(train[\"feature_text\"]):\n",
    "        indexed_words = []\n",
    "        if feature_text in tokenized_features:\n",
    "            continue\n",
    "\n",
    "        tokenized = tokenizer.encode_plus(feature_text, add_special_tokens=True)\n",
    "\n",
    "        for word in tokenized[\"input_ids\"]:\n",
    "            embedded_token = embed_word(word)\n",
    "\n",
    "            indexed_words.append({\n",
    "                \"word_id\": word,\n",
    "                \"embedded\": embedded_token,\n",
    "            })\n",
    "                \n",
    "        tokenized_features[feature_text] = indexed_words\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_features, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_id': 2155,\n",
       " 'embedded': array([ 1.52311306e-02,  9.42442659e-03, -3.38619053e-02, -2.42799595e-02,\n",
       "         2.34256983e-02, -8.17641430e-03, -2.93032657e-02, -5.23912981e-02,\n",
       "         3.92839797e-02,  2.82708416e-03, -1.66527238e-02, -2.51747109e-02,\n",
       "        -3.52113359e-02,  3.62591706e-02, -7.53024034e-03,  2.13038772e-02,\n",
       "        -2.84320372e-03, -5.02108894e-02, -3.15106139e-02, -1.20872613e-02,\n",
       "        -3.53940949e-02, -3.89684625e-02, -9.54788644e-03,  1.48558039e-02,\n",
       "        -1.46783004e-02,  4.23340984e-02, -1.99949723e-02,  8.76626652e-03,\n",
       "        -2.45130006e-02, -5.38146645e-02,  2.60462761e-02, -1.56000787e-02,\n",
       "        -3.20293419e-02, -1.68701317e-02, -5.71791418e-02,  6.56096172e-03,\n",
       "        -2.73210574e-02, -9.99859255e-03, -2.09460687e-02,  1.66101735e-02,\n",
       "         2.68947575e-02,  1.13374833e-03, -1.66690629e-02,  1.36123579e-02,\n",
       "         2.36188024e-02, -7.47381104e-03,  1.41381836e-02,  1.25802001e-02,\n",
       "         2.57711634e-02,  1.62442252e-02, -3.19506996e-03, -6.07429678e-03,\n",
       "        -3.58607359e-02, -6.43704087e-02, -3.92574770e-03, -1.00825774e-02,\n",
       "         2.75388900e-02, -3.10948975e-02,  2.38278732e-02,  7.44539779e-03,\n",
       "        -4.51974235e-02, -2.97444407e-02, -4.98110019e-02,  2.10578293e-02,\n",
       "        -8.30610245e-02, -9.97002819e-04, -8.61699730e-02,  2.23975535e-02,\n",
       "        -3.63851711e-02,  2.59253103e-02,  1.60876010e-02, -9.07502025e-02,\n",
       "        -5.43588540e-04, -9.99948084e-02, -2.06221528e-02, -3.46545305e-04,\n",
       "        -3.78179066e-02, -7.17385709e-02,  5.57350414e-03, -4.78198193e-02,\n",
       "        -6.21052459e-02, -2.02608649e-02, -8.12239870e-02,  8.56962427e-03,\n",
       "        -5.33923600e-03, -3.35443653e-02,  2.26709917e-02,  1.13433441e-02,\n",
       "        -8.49064216e-02, -1.88676231e-02, -8.39527976e-03, -3.40117491e-03,\n",
       "        -7.06991032e-02,  6.59702858e-03, -1.87366959e-02, -6.42724708e-02,\n",
       "        -4.58222963e-02, -3.97029705e-02, -3.07402085e-03, -2.72165854e-02,\n",
       "        -2.03661732e-02,  1.61965732e-02, -2.54113898e-02, -8.35221335e-02,\n",
       "        -1.46965766e-02, -7.56659508e-02,  2.90891603e-02, -7.51430122e-03,\n",
       "         1.30209271e-02,  5.33524826e-02, -2.45514722e-03, -1.76951755e-02,\n",
       "         1.70296934e-02,  7.50798220e-03, -5.08431718e-03, -1.42686833e-02,\n",
       "        -4.51855659e-02,  3.96616245e-03,  4.22960185e-02, -4.65673115e-03,\n",
       "         1.94535088e-02, -4.98280525e-02,  1.47982426e-02,  3.21139093e-03,\n",
       "        -3.62973884e-02, -3.59461233e-02,  1.67836889e-03, -6.97912276e-02,\n",
       "        -1.32286549e-02, -3.76444422e-02,  4.71290462e-02,  1.99587867e-02,\n",
       "        -4.86811101e-02, -4.27232571e-02,  1.33466849e-03,  2.61137411e-02,\n",
       "         2.37338878e-02,  3.08593665e-03,  2.86678188e-02, -5.49637191e-02,\n",
       "         8.24517012e-03, -5.23012038e-03,  3.77820097e-02, -1.67999149e-03,\n",
       "        -4.83294018e-02, -4.34270166e-02, -3.65219153e-02,  9.22037847e-03,\n",
       "         8.11870769e-03,  7.81739503e-03, -6.72895610e-02, -1.86866131e-02,\n",
       "        -2.00832319e-02, -4.90455143e-02,  1.05020113e-03, -4.55542430e-02,\n",
       "        -4.85595502e-02, -3.35119255e-02, -2.23896205e-02, -6.92902580e-02,\n",
       "         3.83430012e-02,  1.91386443e-05, -5.41733727e-02, -4.62411642e-02,\n",
       "        -2.11368464e-02, -2.94983741e-02,  1.82185769e-02,  3.38594019e-02,\n",
       "        -3.82412262e-02, -2.97777075e-03,  6.72160927e-03, -7.17436150e-03,\n",
       "        -4.06495575e-03, -3.94228101e-02, -5.38021140e-02, -6.17605262e-02,\n",
       "        -5.65859154e-02,  8.64125788e-03, -4.81633469e-02, -9.36725512e-02,\n",
       "        -3.73072028e-02,  4.95297788e-03,  4.02747374e-03, -4.07767780e-02,\n",
       "        -7.65677774e-03, -2.06898395e-02, -1.17031718e-03,  2.28958782e-02,\n",
       "        -1.12383971e-02,  6.36798739e-02, -9.20545086e-02,  5.81580065e-02,\n",
       "        -1.82220433e-02, -4.25849259e-02, -1.29954927e-02,  1.97418220e-02,\n",
       "        -4.16004136e-02,  2.24671829e-02, -3.77326868e-02, -3.30510885e-02,\n",
       "        -1.21155763e-02, -2.03517266e-02, -4.36056405e-02, -7.61120394e-02,\n",
       "        -8.93824548e-02,  1.31214634e-02,  2.85673253e-02,  2.52485983e-02,\n",
       "        -5.57261147e-03,  4.16146964e-02,  1.91640407e-02, -3.57294232e-02,\n",
       "         1.64403860e-02, -4.65510525e-02, -2.96277218e-02, -4.86823817e-04,\n",
       "        -7.15841204e-02, -6.31128326e-02, -4.75690514e-02, -5.93900159e-02,\n",
       "         2.08735801e-02,  2.44462863e-02, -1.45831341e-02, -1.28136994e-02,\n",
       "        -3.97576131e-02, -7.13715702e-02,  1.87176038e-02, -5.16580753e-02,\n",
       "        -5.43301925e-02, -1.17413262e-02, -7.23965243e-02, -8.83746892e-02,\n",
       "        -9.87459160e-03,  1.01658544e-02,  1.61665827e-02, -5.68849482e-02,\n",
       "        -1.27311926e-02,  2.18595620e-02, -5.29118069e-02, -9.06441733e-02,\n",
       "         1.15458379e-02, -3.99737284e-02, -1.26529252e-02, -1.56024471e-02,\n",
       "        -5.21961926e-03, -2.42009237e-02, -2.17853766e-03,  2.80902442e-02,\n",
       "         5.73536905e-04, -1.55107141e-01,  8.53505824e-03,  4.97472510e-02,\n",
       "         4.20570038e-02,  3.38615887e-02, -4.43216274e-03,  3.12609039e-02,\n",
       "         6.20675413e-03, -2.02233028e-02, -5.60681894e-03,  1.64948292e-02,\n",
       "         3.04654222e-02, -4.50957753e-02, -8.02531186e-03, -3.92436385e-02,\n",
       "        -6.30453750e-02,  3.55113596e-02, -6.67691007e-02, -3.63236144e-02,\n",
       "        -4.18387204e-02, -4.26423550e-02, -1.70283001e-02,  2.36203521e-02,\n",
       "        -4.08612043e-02, -7.70878568e-02,  5.72917722e-02, -6.99767992e-02,\n",
       "        -4.64395657e-02,  1.89192742e-02, -7.71297365e-02, -4.31481525e-02,\n",
       "        -2.68922225e-02, -5.83421886e-02,  3.16341892e-02, -6.81618825e-02,\n",
       "        -1.01459846e-02, -1.33461431e-02,  1.34614259e-02,  3.69603820e-02,\n",
       "         7.83465523e-03, -4.28813845e-02,  5.92431845e-03, -5.19340970e-02,\n",
       "         2.69206036e-02, -5.37696294e-02, -4.07555327e-03,  3.93235050e-02,\n",
       "        -7.77556598e-02, -6.68021821e-05, -5.37035391e-02,  1.57563388e-02,\n",
       "        -6.58299774e-02, -4.68439497e-02,  1.98894329e-02, -7.45963529e-02,\n",
       "         2.90387124e-03, -1.93812866e-02, -2.45766323e-02,  3.95529531e-02,\n",
       "        -3.68425213e-02, -1.70048475e-02, -9.45194140e-02, -3.51017457e-03,\n",
       "        -4.69196737e-02, -4.34695184e-03,  2.08559576e-02, -2.31875926e-02,\n",
       "        -2.35556606e-02, -9.85213276e-03,  1.67959761e-02, -5.32903746e-02,\n",
       "        -5.05688861e-02, -3.64725338e-03, -4.53368388e-02, -2.04105787e-02,\n",
       "         1.63314883e-02, -5.16547672e-02,  6.41106814e-02,  1.10853575e-02,\n",
       "        -3.57060246e-02,  4.42247316e-02,  2.65915319e-03, -4.81954701e-02,\n",
       "         2.80114841e-02, -4.38882597e-02,  2.98616104e-02,  8.07774533e-03,\n",
       "        -7.38246813e-02, -2.56575122e-02, -1.20891235e-03, -5.35254218e-02,\n",
       "        -3.03421132e-02, -3.46376821e-02, -1.99767146e-02, -8.55871439e-02,\n",
       "         2.25002579e-02, -8.78942460e-02, -2.18789186e-02,  2.54477244e-02,\n",
       "         1.60067272e-03,  1.91864464e-02, -5.95970266e-03,  1.39965536e-02,\n",
       "        -1.05711684e-01, -7.85354525e-02, -4.32069413e-02, -3.77054606e-03,\n",
       "        -2.20834091e-02, -6.89397007e-02, -6.04365431e-02, -2.98009124e-02,\n",
       "        -5.05183153e-02,  2.33694855e-02, -4.99159750e-03,  1.63934361e-02,\n",
       "         1.02080768e-02,  3.52836959e-03,  6.18576771e-03,  5.88758476e-02,\n",
       "        -1.12545071e-02, -4.26817685e-03,  4.06682380e-02,  2.28572860e-02,\n",
       "        -6.76611438e-02, -2.92034447e-02, -7.90591761e-02,  5.47752250e-03,\n",
       "        -6.78537320e-03, -2.99374294e-02, -3.11338827e-02, -4.36666347e-02,\n",
       "         3.24213393e-02, -2.93021034e-02,  1.93920881e-02,  6.73353719e-03,\n",
       "        -2.53700726e-02,  3.42251137e-02, -7.28035867e-02,  9.03593563e-03,\n",
       "         4.22166623e-02,  5.74758910e-02, -4.18832451e-02,  1.00277308e-02,\n",
       "        -1.27890510e-02, -6.17674440e-02, -6.05846308e-02, -7.12366924e-02,\n",
       "        -7.56696565e-03, -1.11925546e-02,  2.78304443e-02, -7.62914354e-03,\n",
       "         3.03969625e-02, -6.32165149e-02, -2.98878085e-02, -1.06464615e-02,\n",
       "        -1.34275574e-02,  5.33745997e-03,  1.58859137e-02, -3.51887234e-02,\n",
       "        -7.19104987e-03, -1.49495322e-02,  3.07643297e-03,  2.02511586e-02,\n",
       "         6.58996054e-04, -4.89874445e-02,  1.69149507e-02,  4.35995124e-02,\n",
       "        -4.81884778e-02, -8.31077155e-03, -7.14417845e-02,  7.55870044e-02,\n",
       "         4.10913453e-02, -6.23122752e-02,  3.53792831e-02, -7.20357075e-02,\n",
       "        -2.18317490e-02,  4.16731164e-02, -7.10124969e-02, -4.97945212e-02,\n",
       "        -5.90372048e-02, -2.46399026e-02, -9.29307714e-02,  1.45874368e-02,\n",
       "        -1.20130526e-02, -3.22679393e-02, -1.55276097e-02,  1.94596555e-02,\n",
       "         4.00568731e-02, -6.19752593e-02, -2.73674540e-02, -3.16938274e-02,\n",
       "         5.63009568e-02,  2.54132338e-02, -1.75560557e-03, -6.70711473e-02,\n",
       "        -2.62224693e-02, -5.79263605e-02,  3.85424905e-02, -6.09059446e-03,\n",
       "        -3.65220360e-03, -6.24917075e-02, -4.70685139e-02, -4.15379219e-02,\n",
       "         9.23450571e-03, -3.37729529e-02, -4.45652194e-02,  2.26356592e-02,\n",
       "        -5.98876774e-02,  1.05139697e-02, -1.96131822e-02, -2.15638778e-03,\n",
       "         1.75591093e-02, -1.05781227e-01, -1.51436462e-03, -6.12972789e-02,\n",
       "        -1.92073025e-02, -2.87937354e-02, -1.04767969e-02, -2.19317749e-02,\n",
       "         1.54626770e-02, -5.31485565e-02,  1.73501242e-02, -3.59393656e-02,\n",
       "        -3.91451716e-02, -2.48707961e-02, -1.15518114e-02, -1.33844232e-02,\n",
       "         7.91026279e-03, -4.40833196e-02, -3.13281491e-02,  5.17701777e-03,\n",
       "         2.41726916e-02,  2.15686187e-02, -3.01312525e-02,  1.59254251e-03,\n",
       "         3.13285142e-02,  2.56159510e-02,  7.95306917e-03, -1.81422122e-02,\n",
       "         3.82506289e-03,  2.92187426e-02,  9.22858436e-03, -4.38426621e-02,\n",
       "         5.14102355e-03, -9.82083473e-03,  2.44829375e-02, -6.46073595e-02,\n",
       "        -1.09620303e-01, -8.12270388e-04,  1.21808061e-02, -1.81697942e-02,\n",
       "        -2.80488543e-02, -4.14994322e-02, -3.73201706e-02, -3.00048701e-02,\n",
       "        -3.60519662e-02, -1.62596609e-02, -6.43436164e-02,  7.89933372e-03,\n",
       "        -1.22455703e-02, -1.78920366e-02, -7.34208375e-02, -5.51692732e-02,\n",
       "         4.76007164e-02, -8.00941065e-02, -9.89862625e-03,  2.44176947e-02,\n",
       "         1.24313831e-02, -6.44868314e-02, -5.55185378e-02,  8.74918327e-03,\n",
       "        -3.83896679e-02,  1.24899922e-02,  3.05021331e-02, -9.15587135e-03,\n",
       "         4.76321355e-02,  5.24125993e-02, -1.22695873e-02,  2.09699739e-02,\n",
       "         1.96429305e-02, -3.37100476e-02, -5.85683808e-02,  1.11338934e-02,\n",
       "        -2.87697408e-02, -3.24240476e-02, -1.05963545e-02, -3.99678275e-02,\n",
       "         7.84924533e-03,  6.65762648e-02,  3.21839377e-02,  4.31494005e-02,\n",
       "         1.05961384e-02, -4.09080945e-02, -1.06012495e-02, -4.37951734e-04,\n",
       "        -7.11712660e-03, -4.40713204e-02,  8.71266704e-03,  2.52674017e-02,\n",
       "        -2.56595351e-02,  5.41124074e-03,  6.63584098e-03, -2.13722922e-02,\n",
       "         4.16363962e-02, -9.27571114e-03, -3.08887772e-02, -2.52176039e-02,\n",
       "        -3.21201188e-03, -3.74785042e-03, -5.54295555e-02, -5.98303303e-02,\n",
       "        -4.50135842e-02,  7.26621412e-03,  8.26125685e-03, -4.43532392e-02,\n",
       "        -5.36668599e-02, -5.59826866e-02,  5.83275296e-02, -2.26354469e-02,\n",
       "        -2.24626046e-02, -3.48942401e-03, -1.97326597e-02,  9.84343607e-03,\n",
       "         6.60307892e-03,  2.85363346e-02, -3.41117755e-02, -6.81525543e-02,\n",
       "        -9.20211300e-02, -6.34001791e-02, -3.61268036e-02, -1.66131256e-04,\n",
       "        -8.19247663e-02,  2.08414402e-02, -1.68942530e-02,  7.92026147e-03,\n",
       "         9.15625319e-03,  1.62156578e-02,  8.38278234e-03,  4.91934642e-03,\n",
       "        -7.17306137e-02,  3.89475003e-02, -9.35496092e-02, -7.20771700e-02,\n",
       "        -6.46768957e-02, -2.28228681e-02, -2.91856695e-02,  6.64982805e-03,\n",
       "         1.91643536e-02,  3.34108099e-02, -4.16697450e-02, -2.07496732e-02,\n",
       "        -6.27234951e-02, -2.01628916e-03, -4.06005140e-03, -2.08371785e-02,\n",
       "        -3.25768478e-02,  1.99883450e-02, -3.55855599e-02, -5.78509131e-03,\n",
       "         4.68035862e-02, -5.50547428e-02, -2.74876710e-02,  3.32259922e-03,\n",
       "        -4.06967737e-02,  3.76991555e-02, -7.08693266e-02,  3.71663552e-03,\n",
       "         5.03133703e-03, -1.43978521e-02, -2.82731354e-02,  1.85701102e-02,\n",
       "        -3.86685915e-02,  2.22261343e-02,  3.91379185e-03,  7.54645979e-03,\n",
       "        -3.72246169e-02, -5.37433587e-02,  7.55747547e-03, -6.95776492e-02,\n",
       "        -2.79285535e-02, -5.85377216e-02, -4.50596446e-03, -2.69717770e-04,\n",
       "         3.78758498e-02, -3.47519815e-02, -3.93273011e-02, -5.14439456e-02,\n",
       "        -4.74330969e-02, -1.09439073e-02, -3.70996445e-02, -1.93865709e-02,\n",
       "        -1.96613688e-02,  9.40618396e-04, -1.56773217e-02,  3.64401611e-03,\n",
       "        -3.43837254e-02, -3.89860645e-02, -2.49423403e-02,  8.36504716e-03,\n",
       "        -2.78186500e-02,  1.71869493e-03,  5.78296464e-03, -2.53306678e-03,\n",
       "        -3.93334217e-02, -3.97457965e-02, -5.49873188e-02, -4.62207459e-02,\n",
       "        -1.58211980e-02, -2.50746030e-02, -3.13455053e-02, -5.77586843e-03,\n",
       "        -2.93670651e-02, -1.24803996e-02,  4.18163743e-03,  1.41403750e-02,\n",
       "         1.68143697e-02, -5.35635836e-02,  2.87874937e-02, -1.67598464e-02,\n",
       "        -1.32627971e-02,  9.33323335e-03, -8.31953064e-02, -9.45595501e-04,\n",
       "        -4.44996450e-03,  4.18075733e-02, -4.24354225e-02, -3.35405394e-02,\n",
       "         1.97008271e-02, -5.04458733e-02, -2.89398041e-02,  5.09284139e-02,\n",
       "         4.76725884e-02, -1.43112019e-02, -3.01140681e-04, -6.54487610e-02,\n",
       "         7.51618156e-03,  2.27209907e-02, -3.55842635e-02, -2.56626513e-02,\n",
       "        -6.88615302e-03,  1.77982356e-02, -4.24528345e-02, -1.65193086e-03,\n",
       "        -4.28550541e-02, -2.94640753e-02,  8.80818442e-03, -1.37469852e-02,\n",
       "        -9.18758214e-02, -1.80169865e-02, -8.91804397e-02, -6.12931810e-02,\n",
       "        -1.28519749e-02, -6.08912157e-03,  4.85325381e-02, -6.62526116e-02,\n",
       "        -4.07447889e-02, -2.91593820e-02, -5.36921842e-04,  3.26046161e-03,\n",
       "         1.61990337e-02, -2.81461086e-02, -2.81694718e-02,  8.23842920e-03,\n",
       "        -6.10045390e-03,  1.00754609e-03, -2.93709431e-02, -5.30086383e-02,\n",
       "        -5.88793345e-02, -3.39528397e-02, -1.27446912e-02, -2.56063547e-02,\n",
       "         3.94139905e-03, -5.62214255e-02,  9.49775148e-03,  2.24863831e-03,\n",
       "        -5.57465330e-02, -2.01091785e-02,  1.34388525e-02, -1.14753349e-02,\n",
       "         1.08914003e-02, -5.01763411e-02, -3.63819189e-02,  3.38192880e-02,\n",
       "         4.13231961e-02, -4.98285294e-02, -3.86047736e-02,  4.57839528e-03,\n",
       "         4.91425917e-02, -3.65576930e-02,  6.80767139e-03, -1.43696666e-02,\n",
       "        -2.49997601e-02,  6.44037826e-03,  8.70285928e-03, -7.69861937e-02,\n",
       "        -1.04244985e-02, -2.14265324e-02, -3.94667238e-02,  3.64254937e-02,\n",
       "        -3.65097187e-02,  5.31419925e-02,  4.42693047e-02, -2.81242188e-03,\n",
       "        -3.75478789e-02, -5.29529899e-02,  1.93552990e-02,  3.82753951e-03,\n",
       "        -2.99465526e-02, -1.02200909e-02,  1.28179248e-02,  1.41479671e-02,\n",
       "         2.69795326e-03,  4.07936750e-03, -3.60189155e-02, -1.88907720e-02,\n",
       "        -6.15102472e-03, -1.76733863e-02,  3.19157355e-03,  5.39453290e-02])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenized_features.values())[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follow the example described here. Use the same architecture, but:\n",
    "  - only use the last output of the LSTM in the loss function\n",
    "  - use an embedding dim of 128\n",
    "  - use a hidden dim of 256.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature-relevancy of tokens via char ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9901it [00:11, 860.53it/s] \n"
     ]
    }
   ],
   "source": [
    "train_data_preprocessed = dict()\n",
    "for i, (feature_text, pn_history, location) in tqdm(train.iterrows()):\n",
    "    tokenized_history = tokenized_pn_histories[pn_history]\n",
    "    tokens_with_scores = []\n",
    "    for token in tokenized_history:\n",
    "        percentages = []\n",
    "        for feature_relevant_range in location:\n",
    "            token_start, token_end = token[\"range\"]\n",
    "            range_start, range_end = feature_relevant_range[0], feature_relevant_range[1]\n",
    "            \n",
    "            percentage_of_token_in_range = max(min(token_end, range_end)+1 - max(token_start, range_start), 0) / (token_end+1 - token_start)\n",
    "            percentages.append(percentage_of_token_in_range)\n",
    "            # if percentage_of_token_in_range > 0:\n",
    "            #     print(percentage_of_token_in_range, token, feature_relevant_range)\n",
    "        \n",
    "\n",
    "        tokens_with_scores.append({\"token\": token,\n",
    "                                   \"score\": int(max(percentages) > 0.9)})\n",
    "\n",
    "    train_data_preprocessed[i] = {\n",
    "                                    \"pn_history_tokens\": [ts[\"token\"] for ts in tokens_with_scores],\n",
    "                                    \"scores\": torch.tensor([ts[\"score\"] for ts in tokens_with_scores]),\n",
    "                                    \"feature_tokens\": tokenized_features[feature_text],\n",
    "                                    \"locations\": location\n",
    "                                   }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering 2 out of 9901 datapoints because they don't contain any positive scores.\n"
     ]
    }
   ],
   "source": [
    "num_no_positives = sum([1 for dp in train_data_preprocessed.values() if sum(dp[\"scores\"]) == 0])\n",
    "print(f\"filtering {num_no_positives} out of {len(train_data_preprocessed)} datapoints because they don't contain any positive scores.\")\n",
    "train_data_preprocessed = {key: dp for key, dp in train_data_preprocessed.items() if sum(dp[\"scores\"]) != 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of the Model\n",
    "Layers in LSTM Model:\n",
    "1. embed feature tokens\n",
    "2. lstm feature -> constant size vector\n",
    "\n",
    "3. pass to 2nd lstm\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_matrix.shape[1]\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTokenScorer(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout=0.0):\n",
    "        super(LSTMTokenScorer, self).__init__()\n",
    "\n",
    "        self.pn_history_hidden_dim = hidden_dim\n",
    "\n",
    "        # self.bert_embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix))\n",
    "\n",
    "        self.feature_lstm = nn.LSTM(embedding_dim, embedding_dim, bidirectional=False, dropout=dropout) # the feature is now one tensor of size [embedding_dim].\n",
    "\n",
    "        self.total_lstm = nn.LSTM(embedding_dim * 2, self.pn_history_hidden_dim, bidirectional=False, dropout=dropout)\n",
    "        \n",
    "        self.hidden2score = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, pn_history, feature):\n",
    "        # feature_embeds = self.bert_embedding(feature)\n",
    "        feature_lstm_out, _ = self.feature_lstm(feature.view(len(feature), 1, -1)) # the feature is now one tensor of size [embedding_dim].\n",
    "        feature_reduced = torch.squeeze(feature_lstm_out[-1]) #.view(1, -1)\n",
    "        feature_multiplied = feature_reduced.repeat((len(pn_history), 1)) # duplicate feature vector to be same size as embedded pn_history vector.\n",
    "\n",
    "        # pn_history_embeds = self.bert_embedding(pn_history)\n",
    "        pn_history_and_features = torch.concat((feature_multiplied, pn_history), dim=1)\n",
    "\n",
    "        pn_history_reduced, _ = self.total_lstm(pn_history_and_features)\n",
    "        pred_score_raw = torch.squeeze(self.hidden2score(pn_history_reduced))\n",
    "        pred_score = self.sigmoid(pred_score_raw)\n",
    "        return pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_scores = [d[\"scores\"].numpy() for d in train_data_preprocessed.values()]\n",
    "# avg_neg_div_pos = np.mean([(scores.shape[0] - np.sum(scores)) / np.sum(scores) for scores in all_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pn_history_tokens', 'scores', 'feature_tokens', 'locations'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_preprocessed[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_tokens = train_data_preprocessed[0][\"feature_tokens\"]\n",
    "# pn_history_tokens = train_data_preprocessed[0][\"pn_history_tokens\"]\n",
    "\n",
    "# feature_tensor = torch.tensor(np.array([t[\"embedded\"] for t in feature_tokens]), dtype=torch.float)\n",
    "# pn_history_tensor = torch.tensor(np.array([t[\"embedded\"] for t in pn_history_tokens]), dtype=torch.float)\n",
    "\n",
    "# model(pn_history_tensor, feature_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds: List[List[Tuple[int, int]]], truths: List[List[Tuple[int, int]]]):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_micro_f1([[(1,3)]], [[(1,3)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scored_ranges2spans(rnges: Tuple[int, int], scores: int) -> List[Tuple[int, int]]:\n",
    "    thresh = 0.9\n",
    "    spans: List[Tuple[int, int]] = []\n",
    "    active_span_start = None\n",
    "    active_span_end = None\n",
    "    for rng, score in zip(rnges, scores):\n",
    "        if active_span_start is None:\n",
    "            if score > thresh:\n",
    "                active_span_start = rng[0]\n",
    "                active_span_end = rng[1]\n",
    "        else: # prev. words are already part of span\n",
    "            if score > thresh:\n",
    "                active_span_end = rng[1]\n",
    "            else:\n",
    "                spans.append((active_span_start, active_span_end))\n",
    "                active_span_start = None\n",
    "                active_span_end = None\n",
    "    if active_span_start is not None:\n",
    "        spans.append((active_span_start, active_span_end))\n",
    "    return spans\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3172 in train_data_preprocessed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "logfile_name = \"training_log.txt\"\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def log(logtext: str = \"\") -> None:\n",
    "    print(logtext)\n",
    "    with open(logfile_name, \"a\", encoding=\"utf8\") as f:\n",
    "        f.write(str(logtext) + \"\\n\")\n",
    "    \n",
    "\n",
    "def train_model(model: LSTMTokenScorer, criterion, optimizer, scheduler, num_epochs=1, prev_losses=[], prev_f1s=[]):\n",
    "    since = time.time()\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_f1 = 0.0\n",
    "        best_loss = 9999999999999\n",
    "\n",
    "        losses = prev_losses\n",
    "        f1s = prev_f1s\n",
    "        for epoch in range(num_epochs):\n",
    "            log(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            log('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'test']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else: \n",
    "                    model.eval()\n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_loss_average = 0.0\n",
    "                running_f1_total = 0.0\n",
    "                running_f1_average = 0.0\n",
    "                num_non_zero_outputs_in_epoch = 0.0\n",
    "\n",
    "                # batch = random.choices(list(train_data_preprocessed.values()), k=64)\n",
    "                data_ids = list(train_data_preprocessed.keys())\n",
    "                random.shuffle(data_ids)\n",
    "\n",
    "                # Iterate over data.\n",
    "                for i, data_id in enumerate(data_ids):\n",
    "                    datum_preprocessed = train_data_preprocessed[data_id]\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    pn_history_tokens = datum_preprocessed[\"pn_history_tokens\"]\n",
    "                    scores = datum_preprocessed[\"scores\"]\n",
    "                    feature_tokens = datum_preprocessed[\"feature_tokens\"]\n",
    "\n",
    "                    feature_tensor = torch.tensor(np.array([t[\"embedded\"] for t in feature_tokens]), dtype=torch.float)\n",
    "                    pn_history_tensor = torch.tensor(np.array([t[\"embedded\"] for t in pn_history_tokens]), dtype=torch.float)\n",
    "\n",
    "                    # track history only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(pn_history_tensor, feature_tensor)\n",
    "                        \n",
    "                        num_non_zero_outputs = np.count_nonzero(outputs.detach().numpy().round().astype(int))\n",
    "                        num_non_zero_outputs_in_epoch += num_non_zero_outputs\n",
    "                        \n",
    "                        loss = criterion(outputs.float(), scores.float())\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    try:\n",
    "                        f1 = f1_score(scores.int(), outputs.detach().round().int())\n",
    "                    except Exception as e:\n",
    "                        log(\"F1 score calc failed:\")\n",
    "                        log(\"Scores:\")\n",
    "                        log(scores.int())\n",
    "                        log(\"\\nOutputs\")\n",
    "                        log(outputs.detach().round().int())\n",
    "                        log(\"\\n\")\n",
    "                        raise Exception(e)\n",
    "                    # statistics\n",
    "                    running_loss += loss.item()\n",
    "                    running_loss_average = running_loss / (i + 1)\n",
    "                    losses.append(loss.item())\n",
    "                    running_f1_total += f1\n",
    "                    running_f1_average = running_f1_total / (i + 1)\n",
    "                    f1s.append(f1)\n",
    "                    \n",
    "                    if i % 1000 == 0:\n",
    "                        log(f\"Epoch {epoch}, i={i}, avg. loss={running_loss_average}, avg. F1={running_f1_average}, nonzero outputs={num_non_zero_outputs_in_epoch}\")\n",
    "                        # log(\"LSTM output:\")\n",
    "                        # log(outputs)\n",
    "                        # log(\"Truth:\")\n",
    "                        # log(scores)\n",
    "                        log(\"Number of nonzero outputs (in sample prediction):\")\n",
    "                        log(num_non_zero_outputs)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss # / dataset_sizes[phase]\n",
    "                epoch_f1 = running_f1_average # / dataset_sizes[phase]\n",
    "                log(f'{phase} Loss: {epoch_loss:.4f} F1: {epoch_f1:.4f} Time elapsed: {round((time.time() - since))} sec.')\n",
    "                \n",
    "                # deep copy the model\n",
    "                if phase == 'test' and epoch_loss < best_loss: #epoch_acc > best_acc:\n",
    "                    best_f1 = epoch_f1\n",
    "                    best_loss = epoch_loss\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            log()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        log(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        log(f'Best val Acc: {best_f1:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model, losses, f1s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9899it [00:00, 20773.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43.99553335680338"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ids = list(train_data_preprocessed.keys())\n",
    "neg_values = 0.0\n",
    "pos_values = 0.0\n",
    "for i, data_id in tqdm(enumerate(data_ids)):\n",
    "    datum_preprocessed = train_data_preprocessed[data_id]\n",
    "\n",
    "    pn_history_tokens = datum_preprocessed[\"pn_history_tokens\"]\n",
    "    scores = datum_preprocessed[\"scores\"]\n",
    "    pos_values += np.count_nonzero(scores.numpy().round().astype(int))\n",
    "    neg_values += (scores.shape[0] - np.count_nonzero(scores.numpy().round().astype(int)))\n",
    "\n",
    "neg_pos_ratio = neg_values / pos_values\n",
    "neg_pos_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training for lr=0.001...\n",
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, i=0, avg. loss=1.0784095525741577, avg. F1=0.010204081632653062, nonzero outputs=195.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "195\n",
      "Epoch 0, i=1000, avg. loss=1.4001221967624737, avg. F1=0.002136226094267965, nonzero outputs=8222.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=2000, avg. loss=1.385379591862718, avg. F1=0.0010686468367627352, nonzero outputs=8222.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=3000, avg. loss=1.3825646156630726, avg. F1=0.0007125499234795845, nonzero outputs=8222.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=4000, avg. loss=1.374714329268926, avg. F1=0.000534456965849096, nonzero outputs=8222.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=5000, avg. loss=1.3708790938774602, avg. F1=0.00042758694668311, nonzero outputs=8222.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=6000, avg. loss=1.3723319926632978, avg. F1=0.0003563343310052046, nonzero outputs=8222.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=7000, avg. loss=1.3723771677427914, avg. F1=0.00030543669766636665, nonzero outputs=8222.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=8000, avg. loss=1.3726066162311767, avg. F1=0.0002672618823099904, nonzero outputs=8222.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=9000, avg. loss=1.3727275444830593, avg. F1=0.00023756941677171791, nonzero outputs=8222.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13603.5945 F1: 0.0002 Time elapsed: 1281 sec.\n",
      "Epoch 0, i=0, avg. loss=1.2469836473464966, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=1000, avg. loss=1.3543075433263292, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=2000, avg. loss=1.374574201396559, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=3000, avg. loss=1.3731688783392038, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=4000, avg. loss=1.370509752494876, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=5000, avg. loss=1.365344825124197, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=6000, avg. loss=1.3658815396565076, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=7000, avg. loss=1.3642531330799004, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=8000, avg. loss=1.3653010069437437, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 0, i=9000, avg. loss=1.3673218593718197, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "test Loss: 13526.5066 F1: 0.0000 Time elapsed: 1714 sec.\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "Epoch 1, i=0, avg. loss=1.5492662191390991, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=1000, avg. loss=1.3593802197234375, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=2000, avg. loss=1.3728854710134728, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=3000, avg. loss=1.3765127296965745, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=4000, avg. loss=1.366969311082402, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=5000, avg. loss=1.3704286260953833, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=6000, avg. loss=1.3678099444023353, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=7000, avg. loss=1.3646192229946856, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=8000, avg. loss=1.363219066040767, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=9000, avg. loss=1.3647869126940553, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13521.7077 F1: 0.0000 Time elapsed: 3083 sec.\n",
      "Epoch 1, i=0, avg. loss=1.0178146362304688, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=1000, avg. loss=1.331880552844925, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=2000, avg. loss=1.3502484028307216, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=3000, avg. loss=1.3599250460616432, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=4000, avg. loss=1.3643465632052756, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=5000, avg. loss=1.366564780145949, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=6000, avg. loss=1.3617783105585937, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=7000, avg. loss=1.364322925317187, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=8000, avg. loss=1.3673653727083739, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 1, i=9000, avg. loss=1.3664729666238413, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "test Loss: 13518.7657 F1: 0.0000 Time elapsed: 3536 sec.\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "Epoch 2, i=0, avg. loss=1.2862112522125244, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=1000, avg. loss=1.3759308325779902, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=2000, avg. loss=1.3731982696062324, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=3000, avg. loss=1.366437519879708, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=4000, avg. loss=1.365640033828828, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=5000, avg. loss=1.3673877781616453, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=6000, avg. loss=1.365453787931421, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=7000, avg. loss=1.3641000518354751, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=8000, avg. loss=1.3645544874639812, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=9000, avg. loss=1.365394249166095, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13517.1408 F1: 0.0000 Time elapsed: 4726 sec.\n",
      "Epoch 2, i=0, avg. loss=3.422544002532959, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=1000, avg. loss=1.3615982510469533, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=2000, avg. loss=1.3719693548318328, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=3000, avg. loss=1.3681312993700128, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=4000, avg. loss=1.3730981515157643, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=5000, avg. loss=1.3697232754796391, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=6000, avg. loss=1.369061021800042, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=7000, avg. loss=1.367693650383657, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=8000, avg. loss=1.3656080559169839, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 2, i=9000, avg. loss=1.3651419938464864, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "test Loss: 13515.3131 F1: 0.0000 Time elapsed: 5028 sec.\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "Epoch 3, i=0, avg. loss=1.105116367340088, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=1000, avg. loss=1.3578401174578634, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=2000, avg. loss=1.3417626221676817, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=3000, avg. loss=1.3535893681008828, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=4000, avg. loss=1.3665817083939407, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=5000, avg. loss=1.365890218708139, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=6000, avg. loss=1.3694728193849628, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=7000, avg. loss=1.3668150959858092, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=8000, avg. loss=1.3689027303919883, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=9000, avg. loss=1.3670949702222617, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13513.3595 F1: 0.0000 Time elapsed: 5802 sec.\n",
      "Epoch 3, i=0, avg. loss=1.7050526142120361, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=1000, avg. loss=1.3716977863878637, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=2000, avg. loss=1.357597657944309, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=3000, avg. loss=1.3686625009495113, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=4000, avg. loss=1.3697882141807145, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=5000, avg. loss=1.3641767214355742, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=6000, avg. loss=1.3662078447827417, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=7000, avg. loss=1.3628190685094994, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=8000, avg. loss=1.3629616588417433, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 3, i=9000, avg. loss=1.3645691022172581, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "test Loss: 13510.7811 F1: 0.0000 Time elapsed: 6046 sec.\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "Epoch 4, i=0, avg. loss=1.403857707977295, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=1000, avg. loss=1.3867269089886478, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=2000, avg. loss=1.3651537997075642, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=3000, avg. loss=1.3591801403602415, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=4000, avg. loss=1.3574815819454742, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=5000, avg. loss=1.3623753496704758, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=6000, avg. loss=1.3599470837416996, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=7000, avg. loss=1.3614726300121052, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=8000, avg. loss=1.3658778092411514, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=9000, avg. loss=1.3635706042477904, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13507.0524 F1: 0.0000 Time elapsed: 6793 sec.\n",
      "Epoch 4, i=0, avg. loss=1.2388256788253784, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=1000, avg. loss=1.3569679516297835, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=2000, avg. loss=1.3540718140213683, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=3000, avg. loss=1.3598421475204854, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=4000, avg. loss=1.3577668743859348, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=5000, avg. loss=1.361893489798935, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=6000, avg. loss=1.3641240138090605, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=7000, avg. loss=1.366715737426814, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=8000, avg. loss=1.3646553751677666, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 4, i=9000, avg. loss=1.3656597924277512, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "test Loss: 13501.5308 F1: 0.0000 Time elapsed: 7094 sec.\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "Epoch 5, i=0, avg. loss=0.9692952632904053, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=1000, avg. loss=1.3800811391848546, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=2000, avg. loss=1.3792580359879283, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=3000, avg. loss=1.3722494191545043, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=4000, avg. loss=1.3618706178289746, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=5000, avg. loss=1.3641238329506378, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=6000, avg. loss=1.3675860940188374, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=7000, avg. loss=1.3672350487561247, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=8000, avg. loss=1.362884599698244, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=9000, avg. loss=1.3619598716831833, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "train Loss: 13477.1258 F1: 0.0000 Time elapsed: 7929 sec.\n",
      "Epoch 5, i=0, avg. loss=1.2654767036437988, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=1000, avg. loss=1.3522604231115107, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=2000, avg. loss=1.3470234604611033, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=3000, avg. loss=1.3504570189891993, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=4000, avg. loss=1.3474692845159815, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=5000, avg. loss=1.346411454906418, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=6000, avg. loss=1.3447971264236551, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=7000, avg. loss=1.3465493316565935, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=8000, avg. loss=1.3464182453026787, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 5, i=9000, avg. loss=1.3455026116109983, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "test Loss: 13310.2200 F1: 0.0000 Time elapsed: 8220 sec.\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "Epoch 6, i=0, avg. loss=1.1461756229400635, avg. F1=0.0, nonzero outputs=0.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 6, i=1000, avg. loss=1.3382445727433119, avg. F1=0.012696440123369377, nonzero outputs=17191.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "76\n",
      "Epoch 6, i=2000, avg. loss=1.3453828924182414, avg. F1=0.012861480230259672, nonzero outputs=31988.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "12\n",
      "Epoch 6, i=3000, avg. loss=1.3417717758237184, avg. F1=0.014136301314454723, nonzero outputs=51176.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "0\n",
      "Epoch 6, i=4000, avg. loss=1.349192793579049, avg. F1=0.01819332612707003, nonzero outputs=84342.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "117\n",
      "Epoch 6, i=5000, avg. loss=1.341734534226711, avg. F1=0.02097741408693119, nonzero outputs=115542.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "122\n",
      "Epoch 6, i=6000, avg. loss=1.3363603012717937, avg. F1=0.02389524063507415, nonzero outputs=143862.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "20\n",
      "Epoch 6, i=7000, avg. loss=1.3310968915559005, avg. F1=0.02746513820080683, nonzero outputs=172653.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "3\n",
      "Epoch 6, i=8000, avg. loss=1.3328266747518416, avg. F1=0.033349867427815205, nonzero outputs=220751.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "3\n",
      "Epoch 6, i=9000, avg. loss=1.330633787370605, avg. F1=0.0372820141934765, nonzero outputs=245983.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "train Loss: 13187.1501 F1: 0.0417 Time elapsed: 9111 sec.\n",
      "Epoch 6, i=0, avg. loss=1.4356521368026733, avg. F1=0.0, nonzero outputs=75.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "75\n",
      "Epoch 6, i=1000, avg. loss=1.3336195271093767, avg. F1=0.10451630540702912, nonzero outputs=30258.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "Epoch 6, i=2000, avg. loss=1.3161650063215882, avg. F1=0.09593833091394034, nonzero outputs=57944.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "Epoch 6, i=3000, avg. loss=1.3086776178266875, avg. F1=0.09482432209111585, nonzero outputs=85440.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "173\n",
      "Epoch 6, i=4000, avg. loss=1.304989361295221, avg. F1=0.0948331373403032, nonzero outputs=112835.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "2\n",
      "Epoch 6, i=5000, avg. loss=1.3072344014297077, avg. F1=0.0951199491920725, nonzero outputs=141327.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "3\n",
      "Epoch 6, i=6000, avg. loss=1.3058468466241286, avg. F1=0.09491918162730115, nonzero outputs=168855.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "4\n",
      "Epoch 6, i=7000, avg. loss=1.303859038236158, avg. F1=0.09428331197484074, nonzero outputs=197869.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "20\n",
      "Epoch 6, i=8000, avg. loss=1.302991939855954, avg. F1=0.09371877372870914, nonzero outputs=224369.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "5\n",
      "Epoch 6, i=9000, avg. loss=1.3060363588988972, avg. F1=0.09402488595405148, nonzero outputs=252561.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "3\n",
      "test Loss: 12928.5112 F1: 0.0948 Time elapsed: 9379 sec.\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "Epoch 7, i=0, avg. loss=1.0260376930236816, avg. F1=0.2857142857142857, nonzero outputs=18.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "18\n",
      "Epoch 7, i=1000, avg. loss=1.3072523010836972, avg. F1=0.09850193622201332, nonzero outputs=43321.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "19\n",
      "Epoch 7, i=2000, avg. loss=1.3046066255822055, avg. F1=0.10299560445303955, nonzero outputs=86171.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "61\n",
      "Epoch 7, i=3000, avg. loss=1.305152485327735, avg. F1=0.10225820975478099, nonzero outputs=130974.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "66\n",
      "Epoch 7, i=4000, avg. loss=1.3043198147078687, avg. F1=0.10082982155417032, nonzero outputs=175490.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "16\n",
      "Epoch 7, i=5000, avg. loss=1.3050495057767735, avg. F1=0.0996689427117389, nonzero outputs=219705.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "142\n",
      "Epoch 7, i=6000, avg. loss=1.3059235261372975, avg. F1=0.0998129740598311, nonzero outputs=265165.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "21\n",
      "Epoch 7, i=7000, avg. loss=1.3074470130550302, avg. F1=0.1000925312530436, nonzero outputs=309038.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "Epoch 7, i=8000, avg. loss=1.3053283162183158, avg. F1=0.10013770356968768, nonzero outputs=349612.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "12\n",
      "Epoch 7, i=9000, avg. loss=1.3050448555917744, avg. F1=0.10108920358779751, nonzero outputs=395812.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "24\n",
      "train Loss: 12898.3935 F1: 0.1006 Time elapsed: 10335 sec.\n",
      "Epoch 7, i=0, avg. loss=1.0111678838729858, avg. F1=0.4444444444444444, nonzero outputs=6.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "6\n",
      "Epoch 7, i=1000, avg. loss=1.3045936521593031, avg. F1=0.10841061732928176, nonzero outputs=28513.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "5\n",
      "Epoch 7, i=2000, avg. loss=1.3018154845364032, avg. F1=0.10666838572566227, nonzero outputs=58915.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "27\n",
      "Epoch 7, i=3000, avg. loss=1.3040299007829845, avg. F1=0.10479451031071765, nonzero outputs=88578.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "20\n",
      "Epoch 7, i=4000, avg. loss=1.30674172191136, avg. F1=0.10522237254997341, nonzero outputs=119940.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "50\n",
      "Epoch 7, i=5000, avg. loss=1.3034170423238618, avg. F1=0.10522011442642366, nonzero outputs=150105.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "2\n",
      "Epoch 7, i=6000, avg. loss=1.3031114879756585, avg. F1=0.10444508145734074, nonzero outputs=180036.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "22\n",
      "Epoch 7, i=7000, avg. loss=1.3039968736834773, avg. F1=0.10399566780193444, nonzero outputs=210056.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "6\n",
      "Epoch 7, i=8000, avg. loss=1.304818583769942, avg. F1=0.10222697127029927, nonzero outputs=241903.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "79\n",
      "Epoch 7, i=9000, avg. loss=1.3038023760909174, avg. F1=0.10206374611789805, nonzero outputs=271215.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "45\n",
      "test Loss: 12904.4241 F1: 0.1022 Time elapsed: 10620 sec.\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "Epoch 8, i=0, avg. loss=2.3578896522521973, avg. F1=0.0, nonzero outputs=92.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "92\n",
      "Epoch 8, i=1000, avg. loss=1.313682995416544, avg. F1=0.09076310430740048, nonzero outputs=56563.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "12\n",
      "Epoch 8, i=2000, avg. loss=1.3022430218618433, avg. F1=0.09799023815869054, nonzero outputs=101982.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "2\n",
      "Epoch 8, i=3000, avg. loss=1.3053262625404771, avg. F1=0.09863789952835719, nonzero outputs=147352.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "48\n",
      "Epoch 8, i=4000, avg. loss=1.3059928866065582, avg. F1=0.09816362192102358, nonzero outputs=190899.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "52\n",
      "Epoch 8, i=5000, avg. loss=1.3048875882515452, avg. F1=0.09915860897247274, nonzero outputs=239354.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "163\n",
      "Epoch 8, i=6000, avg. loss=1.3037332553621968, avg. F1=0.10052848975656939, nonzero outputs=290226.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "84\n",
      "Epoch 8, i=7000, avg. loss=1.3037412297025848, avg. F1=0.1009593539882235, nonzero outputs=331766.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "6\n",
      "Epoch 8, i=8000, avg. loss=1.302150829183118, avg. F1=0.10090680939281503, nonzero outputs=380524.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "23\n",
      "Epoch 8, i=9000, avg. loss=1.301960394377444, avg. F1=0.1011431895662608, nonzero outputs=427712.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "34\n",
      "train Loss: 12877.0676 F1: 0.1012 Time elapsed: 11525 sec.\n",
      "Epoch 8, i=0, avg. loss=1.438664436340332, avg. F1=0.0, nonzero outputs=73.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "73\n",
      "Epoch 8, i=1000, avg. loss=1.297626718179091, avg. F1=0.10739407528387548, nonzero outputs=41047.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "64\n",
      "Epoch 8, i=2000, avg. loss=1.2995924118576736, avg. F1=0.10473772961936734, nonzero outputs=80929.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "15\n",
      "Epoch 8, i=3000, avg. loss=1.2991931206462621, avg. F1=0.10547690052652403, nonzero outputs=118936.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "30\n",
      "Epoch 8, i=4000, avg. loss=1.3003403791723416, avg. F1=0.10490877808042119, nonzero outputs=158454.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 8, i=5000, avg. loss=1.2997736407503846, avg. F1=0.10531150545868176, nonzero outputs=197651.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "49\n",
      "Epoch 8, i=6000, avg. loss=1.2975248392473318, avg. F1=0.10513799868302602, nonzero outputs=238762.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "186\n",
      "Epoch 8, i=7000, avg. loss=1.2955574207928433, avg. F1=0.10484027044516188, nonzero outputs=278069.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "41\n",
      "Epoch 8, i=8000, avg. loss=1.2958292700233884, avg. F1=0.10414399414562034, nonzero outputs=318315.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "6\n",
      "Epoch 8, i=9000, avg. loss=1.293352225279493, avg. F1=0.10444634580598523, nonzero outputs=357100.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "test Loss: 12820.3560 F1: 0.1033 Time elapsed: 11832 sec.\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "Epoch 9, i=0, avg. loss=0.8809347748756409, avg. F1=0.0, nonzero outputs=3.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "3\n",
      "Epoch 9, i=1000, avg. loss=1.28139432910439, avg. F1=0.10424458241038494, nonzero outputs=43898.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "73\n",
      "Epoch 9, i=2000, avg. loss=1.2805000408955183, avg. F1=0.09911563950609686, nonzero outputs=87134.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "6\n",
      "Epoch 9, i=3000, avg. loss=1.2915877009462968, avg. F1=0.09656591269597634, nonzero outputs=144315.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "45\n",
      "Epoch 9, i=4000, avg. loss=1.293123422414474, avg. F1=0.0985578421664932, nonzero outputs=188453.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "224\n",
      "Epoch 9, i=5000, avg. loss=1.2945083540049918, avg. F1=0.09870754600092951, nonzero outputs=246698.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "22\n",
      "Epoch 9, i=6000, avg. loss=1.3024988697084103, avg. F1=0.09877470189393, nonzero outputs=304072.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "141\n",
      "Epoch 9, i=7000, avg. loss=1.304469588994469, avg. F1=0.0984035902132953, nonzero outputs=356078.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "35\n",
      "Epoch 9, i=8000, avg. loss=1.3047082829201255, avg. F1=0.0983761005784908, nonzero outputs=403382.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "8\n",
      "Epoch 9, i=9000, avg. loss=1.302044676457653, avg. F1=0.10028388393719169, nonzero outputs=446872.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "133\n",
      "train Loss: 12881.0422 F1: 0.1003 Time elapsed: 12723 sec.\n",
      "Epoch 9, i=0, avg. loss=1.509533166885376, avg. F1=0.0, nonzero outputs=62.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "62\n",
      "Epoch 9, i=1000, avg. loss=1.3031869313338182, avg. F1=0.10526386024428962, nonzero outputs=32339.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "43\n",
      "Epoch 9, i=2000, avg. loss=1.2988336600106338, avg. F1=0.104418492177273, nonzero outputs=66304.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "69\n",
      "Epoch 9, i=3000, avg. loss=1.304815541085145, avg. F1=0.1056437953062609, nonzero outputs=99018.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "69\n",
      "Epoch 9, i=4000, avg. loss=1.301446011053923, avg. F1=0.10406934541373383, nonzero outputs=132127.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "4\n",
      "Epoch 9, i=5000, avg. loss=1.302745614426538, avg. F1=0.10368560145472633, nonzero outputs=165252.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "146\n",
      "Epoch 9, i=6000, avg. loss=1.3042986741107299, avg. F1=0.1044808915862287, nonzero outputs=198024.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "6\n",
      "Epoch 9, i=7000, avg. loss=1.3011987408796832, avg. F1=0.10434083233871927, nonzero outputs=229185.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "6\n",
      "Epoch 9, i=8000, avg. loss=1.2979119942123003, avg. F1=0.10344071707731706, nonzero outputs=259852.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "109\n",
      "Epoch 9, i=9000, avg. loss=1.3000269227856438, avg. F1=0.10438128807983954, nonzero outputs=291056.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "36\n",
      "test Loss: 12850.0860 F1: 0.1039 Time elapsed: 12998 sec.\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "Epoch 10, i=0, avg. loss=3.709493398666382, avg. F1=0.12987012987012986, nonzero outputs=49.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "49\n",
      "Epoch 10, i=1000, avg. loss=1.3046253531962841, avg. F1=0.09367718859818176, nonzero outputs=46194.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "14\n",
      "Epoch 10, i=2000, avg. loss=1.2954256253978838, avg. F1=0.09833754170009568, nonzero outputs=89603.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "61\n",
      "Epoch 10, i=3000, avg. loss=1.2881748740254382, avg. F1=0.09831016848303661, nonzero outputs=127402.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "12\n",
      "Epoch 10, i=4000, avg. loss=1.2921651997765253, avg. F1=0.09699190180469594, nonzero outputs=187821.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 10, i=5000, avg. loss=1.2934000064696916, avg. F1=0.09821145421735648, nonzero outputs=242584.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "17\n",
      "Epoch 10, i=6000, avg. loss=1.2908773526730606, avg. F1=0.09848710234495926, nonzero outputs=288361.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "Epoch 10, i=7000, avg. loss=1.2928364688950664, avg. F1=0.09933119703022279, nonzero outputs=335324.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "75\n",
      "Epoch 10, i=8000, avg. loss=1.2942817080931253, avg. F1=0.09989438594778283, nonzero outputs=390273.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "13\n",
      "Epoch 10, i=9000, avg. loss=1.2962068861066707, avg. F1=0.10070604756121926, nonzero outputs=436860.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "14\n",
      "train Loss: 12855.2106 F1: 0.1018 Time elapsed: 13728 sec.\n",
      "Epoch 10, i=0, avg. loss=1.0002014636993408, avg. F1=0.0, nonzero outputs=37.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "37\n",
      "Epoch 10, i=1000, avg. loss=1.2899177761582823, avg. F1=0.1067263300527206, nonzero outputs=38730.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "31\n",
      "Epoch 10, i=2000, avg. loss=1.2866196209999516, avg. F1=0.1047202499006581, nonzero outputs=74100.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "92\n",
      "Epoch 10, i=3000, avg. loss=1.286184556581147, avg. F1=0.10323229369647502, nonzero outputs=112741.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "8\n",
      "Epoch 10, i=4000, avg. loss=1.290140878033918, avg. F1=0.10541135045200839, nonzero outputs=151126.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "24\n",
      "Epoch 10, i=5000, avg. loss=1.2887106617434791, avg. F1=0.10582258620298042, nonzero outputs=189673.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "27\n",
      "Epoch 10, i=6000, avg. loss=1.2881765972175432, avg. F1=0.10721868966610118, nonzero outputs=225627.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "55\n",
      "Epoch 10, i=7000, avg. loss=1.2905967955997273, avg. F1=0.10609416199541812, nonzero outputs=263518.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "15\n",
      "Epoch 10, i=8000, avg. loss=1.2920572074290113, avg. F1=0.10578461675166115, nonzero outputs=302384.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "15\n",
      "Epoch 10, i=9000, avg. loss=1.2913122783831896, avg. F1=0.10601833793114226, nonzero outputs=341235.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "57\n",
      "test Loss: 12785.5428 F1: 0.1064 Time elapsed: 13975 sec.\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "Epoch 11, i=0, avg. loss=1.1523096561431885, avg. F1=0.2222222222222222, nonzero outputs=40.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "40\n",
      "Epoch 11, i=1000, avg. loss=1.2887972952364446, avg. F1=0.1062036752832898, nonzero outputs=47073.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "28\n",
      "Epoch 11, i=2000, avg. loss=1.2958219897621932, avg. F1=0.10401306895339438, nonzero outputs=97610.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "8\n",
      "Epoch 11, i=3000, avg. loss=1.2965988922182698, avg. F1=0.10154052546251212, nonzero outputs=140085.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "28\n",
      "Epoch 11, i=4000, avg. loss=1.2896867416793483, avg. F1=0.10367631979979208, nonzero outputs=180328.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "8\n",
      "Epoch 11, i=5000, avg. loss=1.2928820066608397, avg. F1=0.10424986573213313, nonzero outputs=231823.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "106\n",
      "Epoch 11, i=6000, avg. loss=1.2934293413317177, avg. F1=0.1050936426853984, nonzero outputs=281323.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "Epoch 11, i=7000, avg. loss=1.2915734048060665, avg. F1=0.10516853041026696, nonzero outputs=326808.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "20\n",
      "Epoch 11, i=8000, avg. loss=1.2946114853804833, avg. F1=0.10422385492739039, nonzero outputs=378766.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "39\n",
      "Epoch 11, i=9000, avg. loss=1.293647561155681, avg. F1=0.10357499340735465, nonzero outputs=426174.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "4\n",
      "train Loss: 12803.2123 F1: 0.1036 Time elapsed: 14701 sec.\n",
      "Epoch 11, i=0, avg. loss=1.2601665258407593, avg. F1=0.0, nonzero outputs=22.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "22\n",
      "Epoch 11, i=1000, avg. loss=1.2853832190687007, avg. F1=0.10455798504577397, nonzero outputs=39684.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "36\n",
      "Epoch 11, i=2000, avg. loss=1.3027571117443064, avg. F1=0.10452504621832233, nonzero outputs=79659.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "15\n",
      "Epoch 11, i=3000, avg. loss=1.2951763896138142, avg. F1=0.10288474462210745, nonzero outputs=119214.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "27\n",
      "Epoch 11, i=4000, avg. loss=1.290484571242386, avg. F1=0.10235453513284673, nonzero outputs=158019.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "112\n",
      "Epoch 11, i=5000, avg. loss=1.2918219230599794, avg. F1=0.10547937256271475, nonzero outputs=196924.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "11\n",
      "Epoch 11, i=6000, avg. loss=1.288388324308308, avg. F1=0.1057447579198484, nonzero outputs=238175.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "100\n",
      "Epoch 11, i=7000, avg. loss=1.287875998720001, avg. F1=0.1056848950776253, nonzero outputs=278421.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "55\n",
      "Epoch 11, i=8000, avg. loss=1.290357637540979, avg. F1=0.1060707320572337, nonzero outputs=319481.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 11, i=9000, avg. loss=1.2888984637463865, avg. F1=0.10605952716684944, nonzero outputs=357688.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "48\n",
      "test Loss: 12755.1252 F1: 0.1063 Time elapsed: 14942 sec.\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "Epoch 12, i=0, avg. loss=1.7336934804916382, avg. F1=0.0, nonzero outputs=58.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "58\n",
      "Epoch 12, i=1000, avg. loss=1.2842107637897953, avg. F1=0.10709888853358995, nonzero outputs=48837.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "72\n",
      "Epoch 12, i=2000, avg. loss=1.2840804858603279, avg. F1=0.10464299887206704, nonzero outputs=90079.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "134\n",
      "Epoch 12, i=3000, avg. loss=1.2851206773084547, avg. F1=0.10296891290512832, nonzero outputs=136784.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "136\n",
      "Epoch 12, i=4000, avg. loss=1.2933959045192058, avg. F1=0.10235723796143363, nonzero outputs=189814.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "Epoch 12, i=5000, avg. loss=1.2940759028560804, avg. F1=0.10226281042873134, nonzero outputs=237483.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "61\n",
      "Epoch 12, i=6000, avg. loss=1.2937679942737637, avg. F1=0.1016013477125152, nonzero outputs=285595.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "146\n",
      "Epoch 12, i=7000, avg. loss=1.2926079596796813, avg. F1=0.10163387367660026, nonzero outputs=336410.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "29\n",
      "Epoch 12, i=8000, avg. loss=1.293334655732814, avg. F1=0.10203413771825248, nonzero outputs=376099.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "11\n",
      "Epoch 12, i=9000, avg. loss=1.2926641629213969, avg. F1=0.10275062773207452, nonzero outputs=426562.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "27\n",
      "train Loss: 12787.7774 F1: 0.1035 Time elapsed: 15735 sec.\n",
      "Epoch 12, i=0, avg. loss=1.612549066543579, avg. F1=0.1834862385321101, nonzero outputs=99.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "99\n",
      "Epoch 12, i=1000, avg. loss=1.2788950124463359, avg. F1=0.10450453332406946, nonzero outputs=53410.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "52\n",
      "Epoch 12, i=2000, avg. loss=1.2889461907251425, avg. F1=0.10357565333153669, nonzero outputs=110148.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 12, i=3000, avg. loss=1.2908988314563137, avg. F1=0.10477952013774008, nonzero outputs=168168.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "41\n",
      "Epoch 12, i=4000, avg. loss=1.2914813518434785, avg. F1=0.10379697385208773, nonzero outputs=224877.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "168\n",
      "Epoch 12, i=5000, avg. loss=1.290127233120709, avg. F1=0.10307207698451666, nonzero outputs=279913.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "46\n",
      "Epoch 12, i=6000, avg. loss=1.2846747650402504, avg. F1=0.10310288975271888, nonzero outputs=334874.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "62\n",
      "Epoch 12, i=7000, avg. loss=1.2863664613027537, avg. F1=0.10194945269905377, nonzero outputs=392954.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "54\n",
      "Epoch 12, i=8000, avg. loss=1.2851780426158055, avg. F1=0.10214695075619046, nonzero outputs=448593.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 12, i=9000, avg. loss=1.2859180986901122, avg. F1=0.10155003032860918, nonzero outputs=503727.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "20\n",
      "test Loss: 12749.7348 F1: 0.1018 Time elapsed: 16015 sec.\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "Epoch 13, i=0, avg. loss=0.9905104041099548, avg. F1=0.0, nonzero outputs=17.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "17\n",
      "Epoch 13, i=1000, avg. loss=1.2903991291572998, avg. F1=0.10557399139809007, nonzero outputs=46478.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "25\n",
      "Epoch 13, i=2000, avg. loss=1.2826561116683728, avg. F1=0.10654619504853262, nonzero outputs=94216.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "8\n",
      "Epoch 13, i=3000, avg. loss=1.2915108419386239, avg. F1=0.10843953762301217, nonzero outputs=139338.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "13\n",
      "Epoch 13, i=4000, avg. loss=1.2862178413637337, avg. F1=0.10835444009842866, nonzero outputs=190452.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "224\n",
      "Epoch 13, i=5000, avg. loss=1.2871681349774737, avg. F1=0.10611207219515412, nonzero outputs=240387.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "Epoch 13, i=6000, avg. loss=1.2884705910045413, avg. F1=0.1054915977892993, nonzero outputs=288455.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "70\n",
      "Epoch 13, i=7000, avg. loss=1.2867132154741792, avg. F1=0.10478387551875065, nonzero outputs=336496.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "57\n",
      "Epoch 13, i=8000, avg. loss=1.2840494726869616, avg. F1=0.1039549040588147, nonzero outputs=380805.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 13, i=9000, avg. loss=1.2868146901328807, avg. F1=0.1039633352407456, nonzero outputs=426498.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "21\n",
      "train Loss: 12785.7750 F1: 0.1044 Time elapsed: 16844 sec.\n",
      "Epoch 13, i=0, avg. loss=1.5915590524673462, avg. F1=0.0, nonzero outputs=15.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "15\n",
      "Epoch 13, i=1000, avg. loss=1.3165891055579666, avg. F1=0.10322586239304268, nonzero outputs=23928.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "26\n",
      "Epoch 13, i=2000, avg. loss=1.3079263908990557, avg. F1=0.09823425786037736, nonzero outputs=48463.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "26\n",
      "Epoch 13, i=3000, avg. loss=1.30844257468583, avg. F1=0.1026420003990304, nonzero outputs=72077.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "40\n",
      "Epoch 13, i=4000, avg. loss=1.3066559869389152, avg. F1=0.1022005463864557, nonzero outputs=96759.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "6\n",
      "Epoch 13, i=5000, avg. loss=1.3078514862909147, avg. F1=0.10092121231401052, nonzero outputs=122131.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "45\n",
      "Epoch 13, i=6000, avg. loss=1.3071798488132715, avg. F1=0.10124443588937354, nonzero outputs=146281.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "27\n",
      "Epoch 13, i=7000, avg. loss=1.3097469518396143, avg. F1=0.10094120575453619, nonzero outputs=170460.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "Epoch 13, i=8000, avg. loss=1.3113027499357084, avg. F1=0.10039801112796622, nonzero outputs=195511.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "29\n",
      "Epoch 13, i=9000, avg. loss=1.3104555882714666, avg. F1=0.10088686550185146, nonzero outputs=218993.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "18\n",
      "test Loss: 12967.1657 F1: 0.1007 Time elapsed: 17094 sec.\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "Epoch 14, i=0, avg. loss=0.8148759603500366, avg. F1=0.0, nonzero outputs=7.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 14, i=1000, avg. loss=1.2845488894474018, avg. F1=0.10728884488945138, nonzero outputs=36315.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "49\n",
      "Epoch 14, i=2000, avg. loss=1.2826200787095294, avg. F1=0.10723430262727096, nonzero outputs=78037.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "13\n",
      "Epoch 14, i=3000, avg. loss=1.2825137947408884, avg. F1=0.10539408364264666, nonzero outputs=123360.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "8\n",
      "Epoch 14, i=4000, avg. loss=1.28476997897852, avg. F1=0.1065523315356516, nonzero outputs=169580.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "112\n",
      "Epoch 14, i=5000, avg. loss=1.2812712384543163, avg. F1=0.10630042416266992, nonzero outputs=214588.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "34\n",
      "Epoch 14, i=6000, avg. loss=1.2812689790982363, avg. F1=0.10631586505510103, nonzero outputs=257672.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "106\n",
      "Epoch 14, i=7000, avg. loss=1.282182551974689, avg. F1=0.10639943436893955, nonzero outputs=305106.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "102\n",
      "Epoch 14, i=8000, avg. loss=1.281721522935911, avg. F1=0.10607727751522453, nonzero outputs=353229.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "18\n",
      "Epoch 14, i=9000, avg. loss=1.2818710274622713, avg. F1=0.10661809351280598, nonzero outputs=399415.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "25\n",
      "train Loss: 12707.5356 F1: 0.1064 Time elapsed: 17829 sec.\n",
      "Epoch 14, i=0, avg. loss=0.8867030739784241, avg. F1=0.0, nonzero outputs=26.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "26\n",
      "Epoch 14, i=1000, avg. loss=1.2805849925859587, avg. F1=0.10514482949960309, nonzero outputs=50935.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "6\n",
      "Epoch 14, i=2000, avg. loss=1.2774331301286899, avg. F1=0.10672752083798086, nonzero outputs=99038.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "137\n",
      "Epoch 14, i=3000, avg. loss=1.2879426542896701, avg. F1=0.10608041848208195, nonzero outputs=151610.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "60\n",
      "Epoch 14, i=4000, avg. loss=1.2851167443513096, avg. F1=0.10645475167066953, nonzero outputs=202228.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "47\n",
      "Epoch 14, i=5000, avg. loss=1.2856243739865156, avg. F1=0.1059464093769103, nonzero outputs=252692.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "30\n",
      "Epoch 14, i=6000, avg. loss=1.284860420288632, avg. F1=0.10615202088016888, nonzero outputs=303156.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "21\n",
      "Epoch 14, i=7000, avg. loss=1.2826590229231807, avg. F1=0.10587760578756278, nonzero outputs=353333.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "54\n",
      "Epoch 14, i=8000, avg. loss=1.2832132646209402, avg. F1=0.10520220022680027, nonzero outputs=401537.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "185\n",
      "Epoch 14, i=9000, avg. loss=1.2834088110272162, avg. F1=0.10512764585529888, nonzero outputs=451872.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "56\n",
      "test Loss: 12701.6575 F1: 0.1052 Time elapsed: 18070 sec.\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "Epoch 15, i=0, avg. loss=1.5010108947753906, avg. F1=0.13559322033898305, nonzero outputs=110.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "110\n",
      "Epoch 15, i=1000, avg. loss=1.2749856800347061, avg. F1=0.1023696274812303, nonzero outputs=44457.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "21\n",
      "Epoch 15, i=2000, avg. loss=1.2794879231376686, avg. F1=0.10542943598841543, nonzero outputs=89754.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "34\n",
      "Epoch 15, i=3000, avg. loss=1.2842302570657624, avg. F1=0.10742395793221851, nonzero outputs=138960.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "Epoch 15, i=4000, avg. loss=1.2815905622886556, avg. F1=0.10676307437820193, nonzero outputs=188439.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "33\n",
      "Epoch 15, i=5000, avg. loss=1.2829267501330477, avg. F1=0.1066522370399016, nonzero outputs=234818.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "63\n",
      "Epoch 15, i=6000, avg. loss=1.2849727752625475, avg. F1=0.1062277286474873, nonzero outputs=276126.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 15, i=7000, avg. loss=1.284139950936768, avg. F1=0.10619453063305449, nonzero outputs=320453.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "35\n",
      "Epoch 15, i=8000, avg. loss=1.2831530450523057, avg. F1=0.10587502545858282, nonzero outputs=367522.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "46\n",
      "Epoch 15, i=9000, avg. loss=1.2832068404665578, avg. F1=0.10588227917290781, nonzero outputs=417434.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "train Loss: 12699.0759 F1: 0.1061 Time elapsed: 18798 sec.\n",
      "Epoch 15, i=0, avg. loss=1.1881202459335327, avg. F1=0.0, nonzero outputs=41.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "41\n",
      "Epoch 15, i=1000, avg. loss=1.2867936820059747, avg. F1=0.10969810575400014, nonzero outputs=43593.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "60\n",
      "Epoch 15, i=2000, avg. loss=1.2847695658648985, avg. F1=0.10759064801103112, nonzero outputs=88120.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "104\n",
      "Epoch 15, i=3000, avg. loss=1.2806069871379073, avg. F1=0.10627694456354825, nonzero outputs=133897.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "24\n",
      "Epoch 15, i=4000, avg. loss=1.2811298543171596, avg. F1=0.10581808911839344, nonzero outputs=175498.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "172\n",
      "Epoch 15, i=5000, avg. loss=1.2776098542751204, avg. F1=0.10601497098719492, nonzero outputs=218114.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "33\n",
      "Epoch 15, i=6000, avg. loss=1.2771395829653187, avg. F1=0.10610045407040353, nonzero outputs=259718.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "19\n",
      "Epoch 15, i=7000, avg. loss=1.2773271862004012, avg. F1=0.10685468727976848, nonzero outputs=300442.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "17\n",
      "Epoch 15, i=8000, avg. loss=1.280040991856089, avg. F1=0.1066978826208994, nonzero outputs=346448.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "216\n",
      "Epoch 15, i=9000, avg. loss=1.2814521032059911, avg. F1=0.10712921334919051, nonzero outputs=390440.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "65\n",
      "test Loss: 12692.4982 F1: 0.1073 Time elapsed: 19033 sec.\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "Epoch 16, i=0, avg. loss=1.5009455680847168, avg. F1=0.0, nonzero outputs=33.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "33\n",
      "Epoch 16, i=1000, avg. loss=1.3107487652804348, avg. F1=0.10451507489548818, nonzero outputs=50970.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "97\n",
      "Epoch 16, i=2000, avg. loss=1.2974572706258278, avg. F1=0.10473439194717045, nonzero outputs=98038.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "95\n",
      "Epoch 16, i=3000, avg. loss=1.289229350064604, avg. F1=0.1054205542272797, nonzero outputs=145091.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "27\n",
      "Epoch 16, i=4000, avg. loss=1.2828392626285194, avg. F1=0.1063674028917611, nonzero outputs=191254.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "15\n",
      "Epoch 16, i=5000, avg. loss=1.2846940919247372, avg. F1=0.10624689090839712, nonzero outputs=235997.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "21\n",
      "Epoch 16, i=6000, avg. loss=1.2839253354898952, avg. F1=0.10589062099274511, nonzero outputs=283219.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "11\n",
      "Epoch 16, i=7000, avg. loss=1.2831853866202545, avg. F1=0.10668540016302715, nonzero outputs=328577.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "5\n",
      "Epoch 16, i=8000, avg. loss=1.2835377601441287, avg. F1=0.10650278039186994, nonzero outputs=375184.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "118\n",
      "Epoch 16, i=9000, avg. loss=1.281435089222048, avg. F1=0.10626144294352778, nonzero outputs=419336.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "186\n",
      "train Loss: 12699.1671 F1: 0.1057 Time elapsed: 19756 sec.\n",
      "Epoch 16, i=0, avg. loss=1.252984881401062, avg. F1=0.11494252873563218, nonzero outputs=82.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "82\n",
      "Epoch 16, i=1000, avg. loss=1.2731753956306946, avg. F1=0.10211941650148867, nonzero outputs=47866.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "120\n",
      "Epoch 16, i=2000, avg. loss=1.2720619961656612, avg. F1=0.106479008449379, nonzero outputs=94116.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "117\n",
      "Epoch 16, i=3000, avg. loss=1.2762381636433027, avg. F1=0.1080111989669356, nonzero outputs=142755.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "44\n",
      "Epoch 16, i=4000, avg. loss=1.2816029275485619, avg. F1=0.10831862526041176, nonzero outputs=189781.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "56\n",
      "Epoch 16, i=5000, avg. loss=1.2842692936141356, avg. F1=0.1080316783736959, nonzero outputs=238344.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "39\n",
      "Epoch 16, i=6000, avg. loss=1.2868914976256665, avg. F1=0.10692022486714627, nonzero outputs=288474.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "17\n",
      "Epoch 16, i=7000, avg. loss=1.2867220309509104, avg. F1=0.10600482414454598, nonzero outputs=336102.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "92\n",
      "Epoch 16, i=8000, avg. loss=1.2834985570123294, avg. F1=0.10558487699106457, nonzero outputs=381736.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "69\n",
      "Epoch 16, i=9000, avg. loss=1.2836819728100755, avg. F1=0.10606649309118334, nonzero outputs=429842.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "test Loss: 12686.5754 F1: 0.1059 Time elapsed: 19991 sec.\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "Epoch 17, i=0, avg. loss=1.2354060411453247, avg. F1=0.0, nonzero outputs=9.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "Epoch 17, i=1000, avg. loss=1.2926121984090242, avg. F1=0.10070527182592057, nonzero outputs=47359.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "43\n",
      "Epoch 17, i=2000, avg. loss=1.2802873140630098, avg. F1=0.10181332332983387, nonzero outputs=91903.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "8\n",
      "Epoch 17, i=3000, avg. loss=1.2740947233959263, avg. F1=0.10360543897245135, nonzero outputs=138050.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "11\n",
      "Epoch 17, i=4000, avg. loss=1.2770353431285724, avg. F1=0.10312302796778286, nonzero outputs=183168.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "23\n",
      "Epoch 17, i=5000, avg. loss=1.2758093696073445, avg. F1=0.1052239441431475, nonzero outputs=228963.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "6\n",
      "Epoch 17, i=6000, avg. loss=1.2766891908514522, avg. F1=0.10650018467563667, nonzero outputs=276495.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "45\n",
      "Epoch 17, i=7000, avg. loss=1.279281124713131, avg. F1=0.10706179782305239, nonzero outputs=323612.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "130\n",
      "Epoch 17, i=8000, avg. loss=1.2778515850152243, avg. F1=0.10712596333290587, nonzero outputs=371746.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "158\n",
      "Epoch 17, i=9000, avg. loss=1.280398093393254, avg. F1=0.10624537230558813, nonzero outputs=417039.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "54\n",
      "train Loss: 12690.5841 F1: 0.1062 Time elapsed: 20725 sec.\n",
      "Epoch 17, i=0, avg. loss=0.8607089519500732, avg. F1=0.2222222222222222, nonzero outputs=16.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "16\n",
      "Epoch 17, i=1000, avg. loss=1.2864141252848296, avg. F1=0.10268509778643675, nonzero outputs=48265.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "80\n",
      "Epoch 17, i=2000, avg. loss=1.2811721627382204, avg. F1=0.10433993783750373, nonzero outputs=97824.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "27\n",
      "Epoch 17, i=3000, avg. loss=1.2780600610831228, avg. F1=0.1033341822733581, nonzero outputs=146716.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "60\n",
      "Epoch 17, i=4000, avg. loss=1.2792001081925277, avg. F1=0.10480326249196202, nonzero outputs=196124.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "27\n",
      "Epoch 17, i=5000, avg. loss=1.2781243681836143, avg. F1=0.10551341288409033, nonzero outputs=244893.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "40\n",
      "Epoch 17, i=6000, avg. loss=1.2830524911008026, avg. F1=0.10585064422493451, nonzero outputs=295021.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "121\n",
      "Epoch 17, i=7000, avg. loss=1.2797694448879184, avg. F1=0.1062099299893282, nonzero outputs=344931.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "27\n",
      "Epoch 17, i=8000, avg. loss=1.2784495607284796, avg. F1=0.10545753297796163, nonzero outputs=393668.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "61\n",
      "Epoch 17, i=9000, avg. loss=1.280871399852014, avg. F1=0.10480659787857122, nonzero outputs=443415.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "18\n",
      "test Loss: 12686.7165 F1: 0.1054 Time elapsed: 20979 sec.\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "Epoch 18, i=0, avg. loss=2.0073297023773193, avg. F1=0.12765957446808512, nonzero outputs=37.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "37\n",
      "Epoch 18, i=1000, avg. loss=1.2895897123125288, avg. F1=0.10659313665532387, nonzero outputs=44824.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "14\n",
      "Epoch 18, i=2000, avg. loss=1.2927388658766625, avg. F1=0.10804389494787064, nonzero outputs=89438.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "47\n",
      "Epoch 18, i=3000, avg. loss=1.275373269978542, avg. F1=0.10778619217921682, nonzero outputs=134062.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "Epoch 18, i=4000, avg. loss=1.2774131866998297, avg. F1=0.10724618655160907, nonzero outputs=181361.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "40\n",
      "Epoch 18, i=5000, avg. loss=1.2802590584306806, avg. F1=0.10648647802285974, nonzero outputs=232739.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "24\n",
      "Epoch 18, i=6000, avg. loss=1.2814762023086848, avg. F1=0.10566641476235956, nonzero outputs=274919.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "10\n",
      "Epoch 18, i=7000, avg. loss=1.282232060745399, avg. F1=0.10577209569928417, nonzero outputs=324266.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "39\n",
      "Epoch 18, i=8000, avg. loss=1.2837562027491267, avg. F1=0.10560627940275756, nonzero outputs=373612.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "103\n",
      "Epoch 18, i=9000, avg. loss=1.2813806472030194, avg. F1=0.10627200618967397, nonzero outputs=417086.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "23\n",
      "train Loss: 12689.5907 F1: 0.1065 Time elapsed: 21807 sec.\n",
      "Epoch 18, i=0, avg. loss=1.4309728145599365, avg. F1=0.0, nonzero outputs=61.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "61\n",
      "Epoch 18, i=1000, avg. loss=1.2952309066241794, avg. F1=0.10344722557494791, nonzero outputs=45931.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "8\n",
      "Epoch 18, i=2000, avg. loss=1.2816237987487809, avg. F1=0.10556018988479149, nonzero outputs=94661.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "119\n",
      "Epoch 18, i=3000, avg. loss=1.2875886906468443, avg. F1=0.10318016205635745, nonzero outputs=139693.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "117\n",
      "Epoch 18, i=4000, avg. loss=1.2858858132654356, avg. F1=0.10301620853772998, nonzero outputs=186461.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "16\n",
      "Epoch 18, i=5000, avg. loss=1.2844214736521422, avg. F1=0.10348610393688903, nonzero outputs=233326.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "33\n",
      "Epoch 18, i=6000, avg. loss=1.282051000311422, avg. F1=0.1041415982835744, nonzero outputs=279307.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "81\n",
      "Epoch 18, i=7000, avg. loss=1.278385176273469, avg. F1=0.10601686492464582, nonzero outputs=325482.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "32\n",
      "Epoch 18, i=8000, avg. loss=1.2800767113753906, avg. F1=0.1065653231569246, nonzero outputs=372867.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "13\n",
      "Epoch 18, i=9000, avg. loss=1.280260540441518, avg. F1=0.1063387918391533, nonzero outputs=420924.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "test Loss: 12678.1019 F1: 0.1062 Time elapsed: 22079 sec.\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "Epoch 19, i=0, avg. loss=1.1710493564605713, avg. F1=0.0, nonzero outputs=19.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "19\n",
      "Epoch 19, i=1000, avg. loss=1.2536938978241874, avg. F1=0.11383888428088318, nonzero outputs=45916.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "Epoch 19, i=2000, avg. loss=1.2550582133013866, avg. F1=0.11010127696281602, nonzero outputs=89470.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "57\n",
      "Epoch 19, i=3000, avg. loss=1.2738798685726902, avg. F1=0.10999814342628943, nonzero outputs=140212.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "81\n",
      "Epoch 19, i=4000, avg. loss=1.2744720317190572, avg. F1=0.10804950574279014, nonzero outputs=184362.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "126\n",
      "Epoch 19, i=5000, avg. loss=1.2782080160262845, avg. F1=0.10752041641814952, nonzero outputs=231918.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "9\n",
      "Epoch 19, i=6000, avg. loss=1.279122072157314, avg. F1=0.10772715371137165, nonzero outputs=276898.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "21\n",
      "Epoch 19, i=7000, avg. loss=1.280581523896694, avg. F1=0.10704917880854835, nonzero outputs=323391.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "63\n",
      "Epoch 19, i=8000, avg. loss=1.2801485778152786, avg. F1=0.10602938094278404, nonzero outputs=366296.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "14\n",
      "Epoch 19, i=9000, avg. loss=1.280819038088726, avg. F1=0.10559828270284795, nonzero outputs=411243.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "32\n",
      "train Loss: 12685.7359 F1: 0.1062 Time elapsed: 22963 sec.\n",
      "Epoch 19, i=0, avg. loss=1.508421778678894, avg. F1=0.10666666666666667, nonzero outputs=142.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "142\n",
      "Epoch 19, i=1000, avg. loss=1.2915403357038011, avg. F1=0.10593217359982474, nonzero outputs=50376.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "7\n",
      "Epoch 19, i=2000, avg. loss=1.2835100692906778, avg. F1=0.10824323690609237, nonzero outputs=100654.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "15\n",
      "Epoch 19, i=3000, avg. loss=1.284203325756706, avg. F1=0.10616523763221991, nonzero outputs=148776.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "109\n",
      "Epoch 19, i=4000, avg. loss=1.2857395150577446, avg. F1=0.10484255035080638, nonzero outputs=198768.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "130\n",
      "Epoch 19, i=5000, avg. loss=1.2854072811889115, avg. F1=0.10503660540964976, nonzero outputs=249087.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "121\n",
      "Epoch 19, i=6000, avg. loss=1.2859974710410127, avg. F1=0.10564074865650606, nonzero outputs=302007.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "13\n",
      "Epoch 19, i=7000, avg. loss=1.2828318472505211, avg. F1=0.10521867202290651, nonzero outputs=352098.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "156\n",
      "Epoch 19, i=8000, avg. loss=1.2807852591429483, avg. F1=0.10507370122295129, nonzero outputs=401470.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "19\n",
      "Epoch 19, i=9000, avg. loss=1.2804039155240987, avg. F1=0.10488161349742675, nonzero outputs=451692.0\n",
      "Number of nonzero outputs (in sample prediction):\n",
      "86\n",
      "test Loss: 12683.9655 F1: 0.1050 Time elapsed: 23232 sec.\n",
      "\n",
      "Training complete in 387m 12s\n",
      "Best val Acc: 0.106245\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "open(logfile_name, \"w\", encoding=\"utf8\") # clear logs\n",
    "\n",
    "for lr in [0.001]:\n",
    "    # make model with vocab sizes, including placeholder indices\n",
    "    model = LSTMTokenScorer(EMBEDDING_DIM, HIDDEN_DIM)\n",
    "    # loss_function = nn.BCELoss()\n",
    "    pos_weight = torch.full((1,), neg_pos_ratio)\n",
    "    loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)#make positive class more valuable\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    log(f\"Starting model training for lr={lr}...\")\n",
    "    model, losses, f1s = train_model(model, loss_function, optimizer, exp_lr_scheduler, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, losses, f1s = train_model(model, loss_function, optimizer, exp_lr_scheduler, num_epochs=10,\n",
    "#                                  prev_losses=losses,\n",
    "#                                  prev_f1s=f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(pathjoin(cache_dir, \"losses2.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(losses, f)\n",
    "with open(pathjoin(cache_dir, \"f1s2.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(f1s, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0784095525741577"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2023ea71960>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth(x: Iterable, N: int = 10_000):\n",
    "    return np.convolve(x, np.ones(N)/N, mode='valid')\n",
    "    \n",
    "plt.plot(smooth(f1s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
