{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-04T08:53:41.598329Z","iopub.status.busy":"2022-05-04T08:53:41.598114Z","iopub.status.idle":"2022-05-04T08:53:51.035764Z","shell.execute_reply":"2022-05-04T08:53:51.034791Z","shell.execute_reply.started":"2022-05-04T08:53:41.598302Z"},"trusted":true},"outputs":[],"source":["#! TODO ../input/distilbertbaseuncased should contain config.json with pretrained tokenizer/model\n","import os\n","import spacy\n","import torch\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from ast import literal_eval\n","from torch.utils.data import DataLoader\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.model_selection import train_test_split\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","from sklearn.metrics import f1_score, precision_recall_fscore_support\n","\n","# !pip install -q \"../input/nbme-huggingface-datasets-pip-wheels/datasets-2.1.0-py3-none-any.whl\"\n","os.system(\"python -m pip install --no-index --find-links=../input/nbme-huggingface-datasets-pip-wheels datasets\")\n","\n","import transformers\n","from datasets import Dataset \n","from transformers import AutoModel, AutoTokenizer"]},{"cell_type":"markdown","metadata":{},"source":["# Data"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T07:30:18.863576Z","iopub.status.busy":"2022-05-04T07:30:18.863314Z","iopub.status.idle":"2022-05-04T07:30:20.223648Z","shell.execute_reply":"2022-05-04T07:30:20.222918Z","shell.execute_reply.started":"2022-05-04T07:30:18.863548Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(14300, 8)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>case_num</th>\n","      <th>pn_num</th>\n","      <th>feature_num</th>\n","      <th>annotation</th>\n","      <th>location</th>\n","      <th>pn_history</th>\n","      <th>feature_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00016_000</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>0</td>\n","      <td>[dad with recent heart attcak]</td>\n","      <td>[[696, 724]]</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","      <td>Family history of MI or Family history of myoc...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00016_001</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>[mom with \"thyroid disease]</td>\n","      <td>[[668, 693]]</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","      <td>Family history of thyroid disorder</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id  case_num  pn_num  feature_num                      annotation   \n","0  00016_000         0      16            0  [dad with recent heart attcak]  \\\n","1  00016_001         0      16            1     [mom with \"thyroid disease]   \n","\n","       location                                         pn_history   \n","0  [[696, 724]]  HPI: 17yo M presents with palpitations. Patien...  \\\n","1  [[668, 693]]  HPI: 17yo M presents with palpitations. Patien...   \n","\n","                                        feature_text  \n","0  Family history of MI or Family history of myoc...  \n","1                 Family history of thyroid disorder  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["DATA_PATH = \"../data\" # reading the 3 CSVs\n","\n","feats_df = pd.read_csv(DATA_PATH + \"/features.csv\")\n","notes_df = pd.read_csv(DATA_PATH + \"/patient_notes.csv\")\n","train_df = pd.read_csv(DATA_PATH + \"/train.csv\")\n","\n","def change_location(x): # parse the locations and translate to Python list\n","    if len(x) == 0: # there is no annotation and location\n","        return []\n","    # the data format like this [\"4 12\", \"24 29;33 36\"] and form of Python list thanks to \"literal_eval\"\n","    # first, iterate over elements and split by \";\" so that data becomes [\"4 12\", \"24 29\", \"33 36\"]\n","    # last thing is iterate over again, then make string to integer, data becomes [[4, 12], [24, 29], [33, 36]]\n","    locs_str = []\n","    locs_int = []\n","    for l in x:\n","        locs_str.extend(l.split(\";\"))\n","    for l in locs_str:\n","        l_split = l.split(\" \")\n","        locs_int.append([int(l_split[0]), int(l_split[1])])\n","    return locs_int\n","\n","merged_df = train_df.merge(notes_df, how=\"left\") # merge by common columns\n","patient_df = merged_df.merge(feats_df, how=\"left\") # merge by common columns\n","patient_df[\"location\"] = [literal_eval(x) for x in patient_df[\"location\"]] # string to Python list\n","patient_df[\"location\"] = [change_location(x) for x in patient_df[\"location\"]] # parse the locations\n","patient_df[\"annotation\"] = [literal_eval(x) for x in patient_df[\"annotation\"]] # string to Python list\n","patient_df[\"pn_history\"] = [x.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \") for x in patient_df[\"pn_history\"]] # remove chars\n","patient_df[\"feature_text\"] = [x.replace(\"-OR-\", \" or \").replace(\"-\", \" \") for x in patient_df[\"feature_text\"]] # make features readable\n","print(patient_df.shape)\n","patient_df.head(2)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T07:30:26.48001Z","iopub.status.busy":"2022-05-04T07:30:26.479723Z","iopub.status.idle":"2022-05-04T07:30:26.493485Z","shell.execute_reply":"2022-05-04T07:30:26.492696Z","shell.execute_reply.started":"2022-05-04T07:30:26.47998Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(11440, 8) (2860, 8)\n"]}],"source":["patient_train_df, patient_test_df = train_test_split(patient_df, test_size=0.2, random_state=42)\n","print(patient_train_df.shape, patient_test_df.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T08:21:16.065576Z","iopub.status.busy":"2022-05-04T08:21:16.064849Z","iopub.status.idle":"2022-05-04T08:21:16.401528Z","shell.execute_reply":"2022-05-04T08:21:16.400819Z","shell.execute_reply.started":"2022-05-04T08:21:16.065538Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>case_num</th>\n","      <th>pn_num</th>\n","      <th>feature_num</th>\n","      <th>pn_history</th>\n","      <th>feature_text</th>\n","      <th>location</th>\n","      <th>annotation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00016_000</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>0</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","      <td>Family history of MI; Family history of myocar...</td>\n","      <td>[[-1, -1]]</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00016_001</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","      <td>Family history of thyroid disorder</td>\n","      <td>[[-1, -1]]</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00016_002</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","      <td>Chest pressure</td>\n","      <td>[[-1, -1]]</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00016_003</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>3</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","      <td>Intermittent symptoms</td>\n","      <td>[[-1, -1]]</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00016_004</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>4</td>\n","      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n","      <td>Lightheaded</td>\n","      <td>[[-1, -1]]</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id  case_num  pn_num  feature_num   \n","0  00016_000         0      16            0  \\\n","1  00016_001         0      16            1   \n","2  00016_002         0      16            2   \n","3  00016_003         0      16            3   \n","4  00016_004         0      16            4   \n","\n","                                          pn_history   \n","0  HPI: 17yo M presents with palpitations. Patien...  \\\n","1  HPI: 17yo M presents with palpitations. Patien...   \n","2  HPI: 17yo M presents with palpitations. Patien...   \n","3  HPI: 17yo M presents with palpitations. Patien...   \n","4  HPI: 17yo M presents with palpitations. Patien...   \n","\n","                                        feature_text    location annotation  \n","0  Family history of MI; Family history of myocar...  [[-1, -1]]             \n","1                 Family history of thyroid disorder  [[-1, -1]]             \n","2                                     Chest pressure  [[-1, -1]]             \n","3                              Intermittent symptoms  [[-1, -1]]             \n","4                                        Lightheaded  [[-1, -1]]             "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["def create_submission_df():\n","    feats = pd.read_csv(f\"{DATA_PATH}/features.csv\")\n","    notes = pd.read_csv(f\"{DATA_PATH}/patient_notes.csv\")\n","    test = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n","\n","    merged = test.merge(notes, how=\"left\")\n","    merged = merged.merge(feats, how=\"left\")\n","\n","    def process_text(text):\n","        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n","    merged[\"feature_text\"] = [process_text(x) for x in merged[\"feature_text\"]]\n","    merged[\"pn_history\"] = [process_text(x) for x in merged[\"pn_history\"]]\n","    merged[\"location\"] = [[[-1, -1]], [[-1, -1]], [[-1, -1]], [[-1, -1]], [[-1, -1]]]\n","    merged[\"annotation\"] = \"\"\n","\n","    return merged\n","\n","patient_submission_df = create_submission_df()\n","patient_submission_df.head()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T08:21:19.690716Z","iopub.status.busy":"2022-05-04T08:21:19.689924Z","iopub.status.idle":"2022-05-04T08:21:19.75254Z","shell.execute_reply":"2022-05-04T08:21:19.751841Z","shell.execute_reply.started":"2022-05-04T08:21:19.690669Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['id', 'case_num', 'pn_num', 'feature_num', 'annotation', 'location', 'pn_history', 'feature_text', '__index_level_0__'],\n","     num_rows: 11440\n"," }),\n"," Dataset({\n","     features: ['id', 'case_num', 'pn_num', 'feature_num', 'annotation', 'location', 'pn_history', 'feature_text', '__index_level_0__'],\n","     num_rows: 2860\n"," }),\n"," Dataset({\n","     features: ['id', 'case_num', 'pn_num', 'feature_num', 'pn_history', 'feature_text', 'location', 'annotation'],\n","     num_rows: 5\n"," }))"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["patient_train = Dataset.from_pandas(patient_train_df)\n","patient_test = Dataset.from_pandas(patient_test_df)\n","patient_submission = Dataset.from_pandas(patient_submission_df)\n","patient_train, patient_test, patient_submission"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T08:21:20.461631Z","iopub.status.busy":"2022-05-04T08:21:20.461379Z","iopub.status.idle":"2022-05-04T08:21:20.53207Z","shell.execute_reply":"2022-05-04T08:21:20.531251Z","shell.execute_reply.started":"2022-05-04T08:21:20.461604Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["#checkpoint = \"../input/distilbertbaseuncased\"\n","#tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","#alternative from deberta:\n","# ====================================================\n","# CFG\n","# ====================================================\n","class CFG:\n","    wandb=False\n","    competition='NBME'\n","    _wandb_kernel='nakama'\n","    debug=False\n","    apex=True\n","    print_freq=100\n","    num_workers=4\n","    model=\"microsoft/deberta-base\"\n","    scheduler='cosine' # ['linear', 'cosine']\n","    batch_scheduler=True\n","    num_cycles=0.5\n","    num_warmup_steps=0\n","    epochs=5\n","    encoder_lr=2e-5\n","    decoder_lr=2e-5\n","    min_lr=1e-6\n","    eps=1e-6\n","    betas=(0.9, 0.999)\n","    batch_size=12\n","    fc_dropout=0.2\n","    max_len=512\n","    weight_decay=0.01\n","    gradient_accumulation_steps=1\n","    max_grad_norm=1000\n","    seed=42\n","    n_fold=5\n","    trn_fold=[0, 1, 2, 3, 4]\n","    train=True\n","    \n","if CFG.debug:\n","    CFG.epochs = 2\n","    CFG.trn_fold = [0]\n","tokenizer = AutoTokenizer.from_pretrained(CFG.model)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T08:21:28.304713Z","iopub.status.busy":"2022-05-04T08:21:28.304458Z","iopub.status.idle":"2022-05-04T08:22:03.716845Z","shell.execute_reply":"2022-05-04T08:22:03.716058Z","shell.execute_reply.started":"2022-05-04T08:21:28.304685Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 11440/11440 [00:47<00:00, 241.59 examples/s]\n","Map: 100%|██████████| 2860/2860 [00:12<00:00, 225.70 examples/s]\n","Map: 100%|██████████| 5/5 [00:00<00:00, 107.39 examples/s]\n"]},{"ename":"ValueError","evalue":"Column name __index_level_0__ not in the dataset. Current columns in the dataset: ['id', 'case_num', 'pn_num', 'feature_num', 'pn_history', 'feature_text', 'location', 'annotation', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'sequence_ids', 'labels']","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\HKA\\AILab2023\\Project\\examples\\nbme-distilbert.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 43>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m tokenized_patient_train \u001b[39m=\u001b[39m tokenized_patient_train\u001b[39m.\u001b[39mremove_columns(cols_to_remove) \u001b[39m# remove unused columns\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m tokenized_patient_test \u001b[39m=\u001b[39m tokenized_patient_test\u001b[39m.\u001b[39mremove_columns(cols_to_remove) \u001b[39m# remove unused columns\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m tokenized_patient_submission \u001b[39m=\u001b[39m tokenized_patient_submission\u001b[39m.\u001b[39;49mremove_columns(cols_to_remove) \u001b[39m# remove unused columns\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m tokenized_patient_train, tokenized_patient_test, tokenized_patient_submission\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:591\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    590\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    592\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    593\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    594\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:556\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    550\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    554\u001b[0m }\n\u001b[0;32m    555\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    557\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    558\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:2147\u001b[0m, in \u001b[0;36mDataset.remove_columns\u001b[1;34m(self, column_names, new_fingerprint)\u001b[0m\n\u001b[0;32m   2145\u001b[0m \u001b[39mfor\u001b[39;00m column_name \u001b[39min\u001b[39;00m column_names:\n\u001b[0;32m   2146\u001b[0m     \u001b[39mif\u001b[39;00m column_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcolumn_names:\n\u001b[1;32m-> 2147\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2148\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn name \u001b[39m\u001b[39m{\u001b[39;00mcolumn_name\u001b[39m}\u001b[39;00m\u001b[39m not in the dataset. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2149\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrent columns in the dataset: \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcolumn_names\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2150\u001b[0m         )\n\u001b[0;32m   2152\u001b[0m \u001b[39mfor\u001b[39;00m column_name \u001b[39min\u001b[39;00m column_names:\n\u001b[0;32m   2153\u001b[0m     \u001b[39mdel\u001b[39;00m dataset\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures[column_name]\n","\u001b[1;31mValueError\u001b[0m: Column name __index_level_0__ not in the dataset. Current columns in the dataset: ['id', 'case_num', 'pn_num', 'feature_num', 'pn_history', 'feature_text', 'location', 'annotation', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'sequence_ids', 'labels']"]}],"source":["def tokenize_function(x):\n","    tokenized = tokenizer(\n","        x[\"feature_text\"],          # first sequence (represented as 0)\n","        x[\"pn_history\"],            # second sequence (represented as 1)\n","        truncation=\"only_second\",   # only truncate from second sequence (pn_history)\n","        padding=\"max_length\",       # pad to 512\n","        max_length=512,             # max 512\n","        return_offsets_mapping=True # offsets of tokens (start and end locations)\n","    )\n","\n","    labels = [0.0] * len(tokenized[\"input_ids\"]) # labels indicates whether given token in pn_history is annotaion/feature (1.0) or not (0.0)\n","    tokenized[\"location\"] = x[\"location\"] # add new data to tokenized\n","    tokenized[\"sequence_ids\"] = tokenized.sequence_ids() # sequences and special characters between them\n","    \n","    for idx, (seq_id, offsets) in enumerate(zip(tokenized[\"sequence_ids\"], tokenized[\"offset_mapping\"])):\n","        if seq_id is None or seq_id == 0 or np.isnan(seq_id): # [token is not a sequence (special character)] or [it is first sequence (feature_text)]\n","            labels[idx] = -100.0 # give high loss and use as indicator\n","            continue\n","        token_start, token_end = offsets # get start and location of the token\n","        for location_start, location_end in tokenized[\"location\"]: # search in location\n","            if token_start >= location_start and token_end <= location_end: # whether falls in between them\n","                labels[idx] = 1.0 # found\n","    tokenized[\"labels\"] = labels # add created labels data\n","    tokenized[\"offset_mapping\"] = np.array(tokenized[\"offset_mapping\"]).flatten() # for correct batch shapes\n","    # offset = np.array([offset[::2], offset[1::2]]).reshape(-1, 2) # reverse of flatten\n","    \n","    return tokenized\n","\n","tokenized_patient_train = patient_train.map(tokenize_function) # map each entry to \"tokenize_function\"\n","tokenized_patient_test = patient_test.map(tokenize_function) # map each entry to \"tokenize_function\"\n","tokenized_patient_submission = patient_submission.map(tokenize_function) # map each entry to \"tokenize_function\"\n","\n","cols_all = tokenized_patient_train.column_names # get all columns\n","cols_to_use = [\"input_ids\", \"attention_mask\", \"offset_mapping\", \"sequence_ids\", \"labels\"] # we will use these columns\n","cols_to_remove = list(set(cols_all) - set(cols_to_use)) # find cols that will be removed\n","\n","tokenized_patient_train.set_format(type=\"torch\", columns=cols_to_use) # select columns make type of PyTorch tensor\n","tokenized_patient_test.set_format(type=\"torch\", columns=cols_to_use) # select columns make type of PyTorch tensor\n","tokenized_patient_submission.set_format(type=\"torch\", columns=cols_to_use) # select columns make type of PyTorch tensor\n","\n","tokenized_patient_train = tokenized_patient_train.remove_columns(cols_to_remove) # remove unused columns\n","tokenized_patient_test = tokenized_patient_test.remove_columns(cols_to_remove) # remove unused columns\n","tokenized_patient_submission = tokenized_patient_submission.remove_columns(cols_to_remove) # remove unused columns\n","\n","tokenized_patient_train, tokenized_patient_test, tokenized_patient_submission"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T08:22:06.409443Z","iopub.status.busy":"2022-05-04T08:22:06.409185Z","iopub.status.idle":"2022-05-04T08:22:06.414621Z","shell.execute_reply":"2022-05-04T08:22:06.413952Z","shell.execute_reply.started":"2022-05-04T08:22:06.409416Z"},"trusted":true},"outputs":[],"source":["batch_size = 1\n","\n","dataloader_train = DataLoader(tokenized_patient_train, batch_size=batch_size, drop_last=True)\n","dataloader_test = DataLoader(tokenized_patient_test, batch_size=batch_size, drop_last=True, shuffle=False)\n","dataloader_submission = DataLoader(tokenized_patient_submission, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Modeling"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T07:33:02.336274Z","iopub.status.busy":"2022-05-04T07:33:02.335551Z","iopub.status.idle":"2022-05-04T07:33:02.345861Z","shell.execute_reply":"2022-05-04T07:33:02.345136Z","shell.execute_reply.started":"2022-05-04T07:33:02.336229Z"},"trusted":true},"outputs":[],"source":["class PatientModel(nn.Module): # this is typical custom PyTorch model \n","    def __init__(self, checkpoint=\"../input/distilbertbaseuncased\"):\n","        super(PatientModel, self).__init__()\n","\n","        self.bert = AutoModel.from_pretrained(checkpoint) # load the model\n","        # for param in self.bert.parameters(): param.requires_grad = False\n","        \n","        self.dropout = nn.Dropout(0.2) # random zeroing\n","        self.fc = nn.Linear(768, 1) # classifier layer\n","\n","    def forward(self, batch):\n","        ids = batch[\"input_ids\"]\n","        mask = batch[\"attention_mask\"]\n","        output = self.bert(ids, attention_mask=mask) # feed only these two\n","        x = output[0] # or \"pooler_output\"\n","\n","        x = self.dropout(self.fc(x)).squeeze(-1) # classify and squeeze\n","\n","        return x"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T07:38:57.157972Z","iopub.status.busy":"2022-05-04T07:38:57.157685Z","iopub.status.idle":"2022-05-04T07:56:47.648199Z","shell.execute_reply":"2022-05-04T07:56:47.646838Z","shell.execute_reply.started":"2022-05-04T07:38:57.157925Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["401 Client Error: Unauthorized for url: https://huggingface.co/input/distilbertbaseuncased/resolve/main/config.json\n"]},{"ename":"OSError","evalue":"We couldn't connect to 'https://huggingface.co/' to load this model and it looks like ../input/distilbertbaseuncased is not the path to a directory conaining a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:585\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    584\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_path(\n\u001b[0;32m    586\u001b[0m         config_file,\n\u001b[0;32m    587\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    588\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    589\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    590\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    591\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    592\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    593\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    594\u001b[0m     )\n\u001b[0;32m    596\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m err:\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\file_utils.py:1846\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1844\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m   1845\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m-> 1846\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[0;32m   1847\u001b[0m         url_or_filename,\n\u001b[0;32m   1848\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1849\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   1850\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1851\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   1852\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   1853\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1854\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   1855\u001b[0m     )\n\u001b[0;32m   1856\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m   1857\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\file_utils.py:2050\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   2049\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mhead(url, headers\u001b[39m=\u001b[39mheaders, allow_redirects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, proxies\u001b[39m=\u001b[39mproxies, timeout\u001b[39m=\u001b[39metag_timeout)\n\u001b[1;32m-> 2050\u001b[0m _raise_for_status(r)\n\u001b[0;32m   2051\u001b[0m etag \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Linked-Etag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\file_utils.py:1977\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(request)\u001b[0m\n\u001b[0;32m   1975\u001b[0m         \u001b[39mraise\u001b[39;00m RevisionNotFoundError((\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m404 Client Error: Revision Not Found for url: \u001b[39m\u001b[39m{\u001b[39;00mrequest\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[1;32m-> 1977\u001b[0m request\u001b[39m.\u001b[39;49mraise_for_status()\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n","\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/input/distilbertbaseuncased/resolve/main/config.json","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[1;32mc:\\HKA\\AILab2023\\Project\\examples\\nbme-distilbert.ipynb Cell 12\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# gpu check\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m PatientModel()\u001b[39m.\u001b[39mto(device) \u001b[39m# initialize the model and send it to the device, hopefully to a GPU\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCEWithLogitsLoss(reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# initialize the loss\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.00001\u001b[39m) \u001b[39m# adamw with learning rate 0.00001\u001b[39;00m\n","\u001b[1;32mc:\\HKA\\AILab2023\\Project\\examples\\nbme-distilbert.ipynb Cell 12\u001b[0m line \u001b[0;36mPatientModel.__init__\u001b[1;34m(self, checkpoint)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, checkpoint\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../input/distilbertbaseuncased\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39msuper\u001b[39m(PatientModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(checkpoint) \u001b[39m# load the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# for param in self.bert.parameters(): param.requires_grad = False\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/HKA/AILab2023/Project/examples/nbme-distilbert.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(\u001b[39m0.2\u001b[39m) \u001b[39m# random zeroing\u001b[39;00m\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:424\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39m_from_auto\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 424\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    425\u001b[0m         pretrained_model_name_or_path, return_unused_kwargs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, trust_remote_code\u001b[39m=\u001b[39mtrust_remote_code, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    426\u001b[0m     )\n\u001b[0;32m    427\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[0;32m    428\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:612\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    610\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    611\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 612\u001b[0m config_dict, _ \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    614\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:537\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    536\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 537\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    539\u001b[0m \u001b[39m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconfiguration_files\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:618\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    617\u001b[0m     logger\u001b[39m.\u001b[39merror(err)\n\u001b[1;32m--> 618\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    619\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt connect to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to load this model and it looks like \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    620\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m is not the path to a directory conaining a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    621\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfile.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mCheckout your internet connection or see how to run the library in offline mode at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    622\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    623\u001b[0m     )\n\u001b[0;32m    624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    625\u001b[0m     logger\u001b[39m.\u001b[39merror(err)\n","\u001b[1;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co/' to load this model and it looks like ../input/distilbertbaseuncased is not the path to a directory conaining a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # gpu check\n","model = PatientModel().to(device) # initialize the model and send it to the device, hopefully to a GPU\n","criterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\") # initialize the loss\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001) # adamw with learning rate 0.00001\n","scheduler = StepLR(optimizer, step_size=1, gamma=0.7) # learning rate adjuster\n","\n","num_epochs = 2 # epoch size is 2\n","for epoch in range(num_epochs): # iterate epoch size\n","    for i, batch in enumerate(tqdm(dataloader_train)): # output data amount of batch_size\n","        batch = {k: v.to(device) for k, v in batch.items()} # send the data to device\n","        \n","        optimizer.zero_grad() # refresh/zero the gradient\n","        outputs = model(batch) # get outputs from model\n","        loss = criterion(outputs, batch[\"labels\"]) # criterion(y_pred, y_true)\n","        loss = torch.masked_select(loss, batch[\"labels\"] > -1).mean() # only select second sequence (patient notes) \n","        loss.backward() # train, calculate gradients\n","        optimizer.step() # update model parameters\n","\n","        if i % 800 == 0: # logs\n","            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n","                    epoch + 1,\n","                    i * batch_size, \n","                    len(dataloader_train.dataset),\n","                    100. * i / len(dataloader_train), \n","                    loss.item()\n","                )\n","            )\n","    \n","    scheduler.step() # learning rate adjuster\n","\n","torch.save(model.state_dict(), \"patient_model.pth\") # save our model"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T07:59:09.436276Z","iopub.status.busy":"2022-05-04T07:59:09.435304Z","iopub.status.idle":"2022-05-04T07:59:09.446394Z","shell.execute_reply":"2022-05-04T07:59:09.445575Z","shell.execute_reply.started":"2022-05-04T07:59:09.436234Z"},"trusted":true},"outputs":[],"source":["def predictions_to_locations(predictions_batch, offsets_batch, seq_ids_batch, test=False):\n","    sigmoid = lambda z: 1 / (1 + np.exp(-z)) # for mapping logits between -1.0 to 1.0\n","    all_predictions = [] # will contain 0s (not a annotations/features) and 1s (an annotation)\n","    for predictions, offsets, seq_ids in zip(predictions_batch, offsets_batch, seq_ids_batch): # iterate over data\n","        offsets = np.array([offsets[::2], offsets[1::2]]).reshape(-1, 2) # creating 2D array from 1D by pairwise\n","        \n","        predictions = sigmoid(predictions) # logits/probabilities mapped between -1.0 to 1.0\n","        \n","        start_idx = None # will hold starting location of annotation\n","        current_prediction = [] # will hold current and be appended to all_predictions\n","        for prediction, offset, seq_id in zip(predictions, offsets, seq_ids): # iterate over words/tokens\n","            if seq_id is None or seq_id == 0 or np.isnan(seq_id): # ignore other than second sequence (patient notes)\n","                continue\n","            if prediction > 0.5: # it likely be a annotation/feature/key phrase\n","                if start_idx is None: # a flag for holding very first index of annotation/key phrase\n","                    start_idx = offset[0]\n","                end_idx = offset[1] # will hold last index of annotation and be overrided until not a annotation \n","            elif start_idx is not None and start_idx != 0 and end_idx != 0: # found an annotation and location is not (0, 0)\n","                if test:\n","                    current_prediction.append(f\"{start_idx} {end_idx}\")\n","                else:\n","                    current_prediction.append((start_idx, end_idx)) # append found location\n","                start_idx = None # restart the process\n","        if test:\n","            all_predictions.append(\"; \".join(current_prediction))\n","        else:\n","            all_predictions.append(current_prediction) # append all locations from single data\n","    return all_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T07:59:10.437303Z","iopub.status.busy":"2022-05-04T07:59:10.436992Z","iopub.status.idle":"2022-05-04T07:59:10.451829Z","shell.execute_reply":"2022-05-04T07:59:10.450788Z","shell.execute_reply.started":"2022-05-04T07:59:10.43727Z"},"trusted":true},"outputs":[],"source":["def calculate_charwise_metrics(predictions_batch, offsets_batch, seq_ids_batch, labels_batch):\n","    y_true = [] # will hold all ground truths\n","    y_pred = [] # will hold all predictions\n","    for predictions, offsets, seq_ids, labels in zip(predictions_batch, offsets_batch, seq_ids_batch, labels_batch): # iterate over data\n","        offsets = np.array([offsets[::2], offsets[1::2]]).reshape(-1, 2) # creating 2D array from 1D by pairwise\n","        \n","        num_chars = 0 # calculate number of characters in the text\n","        for start_idx, end_idx in offsets: # look at offset because of it holds locations of words/tokens \n","            num_chars = max(num_chars, start_idx, end_idx) # get biggest location which means number of characters\n","        \n","        char_true = np.zeros((num_chars)) # will consist of 0s and 1s indicates not a annotation and an annotation for ground truths\n","        for offset, seq_id, label in zip(offsets, seq_ids, labels): # iterate over data\n","            if seq_id is None or seq_id == 0 or np.isnan(seq_id): # ignore other than second sequence (patient notes)\n","                continue\n","            if int(label) == 1: # an annotation is found\n","                char_true[offset[0]:offset[1]] = 1 # mark as 1\n","        \n","        char_preds = np.zeros((num_chars)) # will consist of 0s and 1s indicates not a annotation and an annotation for predictions\n","        for start_idx, end_idx in predictions: # we already predicted but only start and end pairs\n","            char_preds[start_idx:end_idx] = 1 # make it character base\n","        \n","        y_true.extend(char_true) # concatenate all for metrics\n","        y_pred.extend(char_preds) # concatenate all for metrics\n","    \n","    micro_f1 = f1_score(y_true, y_pred) # required micro-averaged F1 score.\n","    results = precision_recall_fscore_support(y_true, y_pred, average=\"binary\") # other metrics\n","    return {\n","        \"micro_f1\": micro_f1,\n","        \"precision\": results[0],\n","        \"recall\": results[1],\n","        \"fbeta_score\": results[2],\n","        \"support\": results[3],\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T07:59:13.341584Z","iopub.status.busy":"2022-05-04T07:59:13.341331Z","iopub.status.idle":"2022-05-04T07:59:52.729276Z","shell.execute_reply":"2022-05-04T07:59:52.728528Z","shell.execute_reply.started":"2022-05-04T07:59:13.341558Z"},"trusted":true},"outputs":[],"source":["# model = PatientModel().to(device)\n","# model.load_state_dict(torch.load(\"patient_model.pth\", map_location=device))\n","\n","model.eval() # switch to evaluation mode\n","predictions = [] # will hold data\n","offsets = []\n","seq_ids = []\n","labels = []\n","with torch.no_grad():\n","    for batch in tqdm(dataloader_test):\n","        batch = {k: v.to(device) for k, v in batch.items()} # send to the device\n","        \n","        outputs = model(batch) # get outputs from model\n","        \n","        predictions.append(outputs.cpu().numpy()) # store them\n","        offsets.append(batch[\"offset_mapping\"].cpu().numpy())\n","        seq_ids.append(batch[\"sequence_ids\"].cpu().numpy())\n","        labels.append(batch[\"labels\"].cpu().numpy())\n","\n","predictions = np.concatenate(predictions) # concatenate along axis 0 \n","offsets = np.concatenate(offsets)\n","seq_ids = np.concatenate(seq_ids)\n","labels = np.concatenate(labels)\n","\n","location_predictions = predictions_to_locations(predictions, offsets, seq_ids)\n","calculate_charwise_metrics(location_predictions, offsets, seq_ids, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-04T09:23:17.557918Z","iopub.status.busy":"2022-05-04T09:23:17.557181Z","iopub.status.idle":"2022-05-04T09:23:18.032503Z","shell.execute_reply":"2022-05-04T09:23:18.031772Z","shell.execute_reply.started":"2022-05-04T09:23:17.557876Z"},"trusted":true},"outputs":[],"source":["# model = PatientModel().to(device)\n","# model.load_state_dict(torch.load(\"patient_model.pth\", map_location=device))\n","\n","model.eval() # switch to evaluation mode\n","predictions = [] # will hold data\n","offsets = []\n","seq_ids = []\n","labels = []\n","with torch.no_grad():\n","    for batch in tqdm(dataloader_submission):\n","        batch = {k: v.to(device) for k, v in batch.items()} # send to the device\n","        \n","        outputs = model(batch) # get outputs from model\n","        \n","        predictions.append(outputs.cpu().numpy()) # store them\n","        offsets.append(batch[\"offset_mapping\"].cpu().numpy())\n","        seq_ids.append(batch[\"sequence_ids\"].cpu().numpy())\n","        labels.append(batch[\"labels\"].cpu().numpy())\n","\n","predictions = np.concatenate(predictions) # concatenate along axis 0 \n","offsets = np.concatenate(offsets)\n","seq_ids = np.concatenate(seq_ids)\n","labels = np.concatenate(labels)\n","\n","location_predictions = predictions_to_locations(predictions, offsets, seq_ids, test=True)\n","test_df = create_submission_df()\n","test_df[\"location\"] = location_predictions\n","test_df[[\"id\", \"location\"]].to_csv(\"submission.csv\", index=False)\n","pd.read_csv(\"submission.csv\").head()"]},{"cell_type":"markdown","metadata":{},"source":["# References"]},{"cell_type":"markdown","metadata":{},"source":["1. https://pytorch.org/\n","1. https://huggingface.co/\n","1. https://en.wikipedia.org/wiki/F-score\n","1. https://huggingface.co/bert-base-uncased\n","1. https://huggingface.co/docs/datasets/index\n","1. https://huggingface.co/distilbert-base-uncased\n","1. https://en.wikipedia.org/wiki/Sigmoid_function\n","1. https://huggingface.co/docs/transformers/index\n","1. https://www.kaggle.com/code/odins0n/nbme-detailed-eda\n","1. https://towardsdatascience.com/the-f1-score-bec2bbc38aa6\n","1. https://www.kaggle.com/c/nbme-score-clinical-patient-notes\n","1. https://www.kaggle.com/code/nbroad/qa-ner-hybrid-train-nbme\n","1. https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n","1. https://www.kaggle.com/code/utcarshagrawal/nbme-complete-eda\n","1. https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n","1. https://analyticsindiamag.com/ultimate-guide-to-pytorch-optimizers/\n","1. https://www.kaggle.com/code/theoviel/evaluation-metric-folds-baseline\n","1. https://www.kaggle.com/code/tomohiroh/nbme-bert-for-beginners/notebook\n","1. https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n","1. https://www.kaggle.com/code/ruchi798/score-clinical-patient-notes-spacy-w-b\n","1. https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\n","1. https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/\n","1. https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/\n","1. https://medium.com/dejunhuang/learning-day-57-practical-5-loss-function-crossentropyloss-vs-bceloss-in-pytorch-softmax-vs-bd866c8a0d23"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":3075283,"sourceId":33607,"sourceType":"competition"},{"datasetId":427411,"sourceId":813452,"sourceType":"datasetVersion"},{"sourceId":94729124,"sourceType":"kernelVersion"}],"dockerImageVersionId":30185,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"}},"nbformat":4,"nbformat_minor":4}
