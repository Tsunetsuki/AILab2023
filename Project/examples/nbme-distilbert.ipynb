{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":33607,"databundleVersionId":3075283,"sourceType":"competition"},{"sourceId":813452,"sourceType":"datasetVersion","datasetId":427411},{"sourceId":94729124,"sourceType":"kernelVersion"}],"dockerImageVersionId":30185,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport spacy\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom ast import literal_eval\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import StepLR\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support\n\n# !pip install -q \"../input/nbme-huggingface-datasets-pip-wheels/datasets-2.1.0-py3-none-any.whl\"\nos.system(\"python -m pip install --no-index --find-links=../input/nbme-huggingface-datasets-pip-wheels datasets\")\n\nimport transformers\nfrom datasets import Dataset \nfrom transformers import AutoModel, AutoTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-04T08:53:41.598114Z","iopub.execute_input":"2022-05-04T08:53:41.598329Z","iopub.status.idle":"2022-05-04T08:53:51.035764Z","shell.execute_reply.started":"2022-05-04T08:53:41.598302Z","shell.execute_reply":"2022-05-04T08:53:51.034791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"DATA_PATH = \"../input/nbme-score-clinical-patient-notes\" # reading the 3 CSVs\n\nfeats_df = pd.read_csv(DATA_PATH + \"/features.csv\")\nnotes_df = pd.read_csv(DATA_PATH + \"/patient_notes.csv\")\ntrain_df = pd.read_csv(DATA_PATH + \"/train.csv\")\n\ndef change_location(x): # parse the locations and translate to Python list\n    if len(x) == 0: # there is no annotation and location\n        return []\n    # the data format like this [\"4 12\", \"24 29;33 36\"] and form of Python list thanks to \"literal_eval\"\n    # first, iterate over elements and split by \";\" so that data becomes [\"4 12\", \"24 29\", \"33 36\"]\n    # last thing is iterate over again, then make string to integer, data becomes [[4, 12], [24, 29], [33, 36]]\n    locs_str = []\n    locs_int = []\n    for l in x:\n        locs_str.extend(l.split(\";\"))\n    for l in locs_str:\n        l_split = l.split(\" \")\n        locs_int.append([int(l_split[0]), int(l_split[1])])\n    return locs_int\n\nmerged_df = train_df.merge(notes_df, how=\"left\") # merge by common columns\npatient_df = merged_df.merge(feats_df, how=\"left\") # merge by common columns\npatient_df[\"location\"] = [literal_eval(x) for x in patient_df[\"location\"]] # string to Python list\npatient_df[\"location\"] = [change_location(x) for x in patient_df[\"location\"]] # parse the locations\npatient_df[\"annotation\"] = [literal_eval(x) for x in patient_df[\"annotation\"]] # string to Python list\npatient_df[\"pn_history\"] = [x.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \") for x in patient_df[\"pn_history\"]] # remove chars\npatient_df[\"feature_text\"] = [x.replace(\"-OR-\", \" or \").replace(\"-\", \" \") for x in patient_df[\"feature_text\"]] # make features readable\nprint(patient_df.shape)\npatient_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:30:18.863314Z","iopub.execute_input":"2022-05-04T07:30:18.863576Z","iopub.status.idle":"2022-05-04T07:30:20.223648Z","shell.execute_reply.started":"2022-05-04T07:30:18.863548Z","shell.execute_reply":"2022-05-04T07:30:20.222918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_train_df, patient_test_df = train_test_split(patient_df, test_size=0.2, random_state=42)\nprint(patient_train_df.shape, patient_test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:30:26.479723Z","iopub.execute_input":"2022-05-04T07:30:26.48001Z","iopub.status.idle":"2022-05-04T07:30:26.493485Z","shell.execute_reply.started":"2022-05-04T07:30:26.47998Z","shell.execute_reply":"2022-05-04T07:30:26.492696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission_df():\n    feats = pd.read_csv(f\"{DATA_PATH}/features.csv\")\n    notes = pd.read_csv(f\"{DATA_PATH}/patient_notes.csv\")\n    test = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n\n    merged = test.merge(notes, how=\"left\")\n    merged = merged.merge(feats, how=\"left\")\n\n    def process_text(text):\n        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \").replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n    merged[\"feature_text\"] = [process_text(x) for x in merged[\"feature_text\"]]\n    merged[\"pn_history\"] = [process_text(x) for x in merged[\"pn_history\"]]\n    merged[\"location\"] = [[[-1, -1]], [[-1, -1]], [[-1, -1]], [[-1, -1]], [[-1, -1]]]\n    merged[\"annotation\"] = \"\"\n\n    return merged\n\npatient_submission_df = create_submission_df()\npatient_submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:21:16.064849Z","iopub.execute_input":"2022-05-04T08:21:16.065576Z","iopub.status.idle":"2022-05-04T08:21:16.401528Z","shell.execute_reply.started":"2022-05-04T08:21:16.065538Z","shell.execute_reply":"2022-05-04T08:21:16.400819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_train = Dataset.from_pandas(patient_train_df)\npatient_test = Dataset.from_pandas(patient_test_df)\npatient_submission = Dataset.from_pandas(patient_submission_df)\npatient_train, patient_test, patient_submission","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:21:19.689924Z","iopub.execute_input":"2022-05-04T08:21:19.690716Z","iopub.status.idle":"2022-05-04T08:21:19.75254Z","shell.execute_reply.started":"2022-05-04T08:21:19.690669Z","shell.execute_reply":"2022-05-04T08:21:19.751841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = \"../input/distilbertbaseuncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:21:20.461379Z","iopub.execute_input":"2022-05-04T08:21:20.461631Z","iopub.status.idle":"2022-05-04T08:21:20.53207Z","shell.execute_reply.started":"2022-05-04T08:21:20.461604Z","shell.execute_reply":"2022-05-04T08:21:20.531251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(x):\n    tokenized = tokenizer(\n        x[\"feature_text\"],          # first sequence (represented as 0)\n        x[\"pn_history\"],            # second sequence (represented as 1)\n        truncation=\"only_second\",   # only truncate from second sequence (pn_history)\n        padding=\"max_length\",       # pad to 512\n        max_length=512,             # max 512\n        return_offsets_mapping=True # offsets of tokens (start and end locations)\n    )\n\n    labels = [0.0] * len(tokenized[\"input_ids\"]) # labels indicates whether given token in pn_history is annotaion/feature (1.0) or not (0.0)\n    tokenized[\"location\"] = x[\"location\"] # add new data to tokenized\n    tokenized[\"sequence_ids\"] = tokenized.sequence_ids() # sequences and special characters between them\n    \n    for idx, (seq_id, offsets) in enumerate(zip(tokenized[\"sequence_ids\"], tokenized[\"offset_mapping\"])):\n        if seq_id is None or seq_id == 0 or np.isnan(seq_id): # [token is not a sequence (special character)] or [it is first sequence (feature_text)]\n            labels[idx] = -100.0 # give high loss and use as indicator\n            continue\n        token_start, token_end = offsets # get start and location of the token\n        for location_start, location_end in tokenized[\"location\"]: # search in location\n            if token_start >= location_start and token_end <= location_end: # whether falls in between them\n                labels[idx] = 1.0 # found\n    tokenized[\"labels\"] = labels # add created labels data\n    tokenized[\"offset_mapping\"] = np.array(tokenized[\"offset_mapping\"]).flatten() # for correct batch shapes\n    # offset = np.array([offset[::2], offset[1::2]]).reshape(-1, 2) # reverse of flatten\n    \n    return tokenized\n\ntokenized_patient_train = patient_train.map(tokenize_function) # map each entry to \"tokenize_function\"\ntokenized_patient_test = patient_test.map(tokenize_function) # map each entry to \"tokenize_function\"\ntokenized_patient_submission = patient_submission.map(tokenize_function) # map each entry to \"tokenize_function\"\n\ncols_all = tokenized_patient_train.column_names # get all columns\ncols_to_use = [\"input_ids\", \"attention_mask\", \"offset_mapping\", \"sequence_ids\", \"labels\"] # we will use these columns\ncols_to_remove = list(set(cols_all) - set(cols_to_use)) # find cols that will be removed\n\ntokenized_patient_train.set_format(type=\"torch\", columns=cols_to_use) # select columns make type of PyTorch tensor\ntokenized_patient_test.set_format(type=\"torch\", columns=cols_to_use) # select columns make type of PyTorch tensor\ntokenized_patient_submission.set_format(type=\"torch\", columns=cols_to_use) # select columns make type of PyTorch tensor\n\ntokenized_patient_train = tokenized_patient_train.remove_columns(cols_to_remove) # remove unused columns\ntokenized_patient_test = tokenized_patient_test.remove_columns(cols_to_remove) # remove unused columns\ntokenized_patient_submission = tokenized_patient_submission.remove_columns(cols_to_remove) # remove unused columns\n\ntokenized_patient_train, tokenized_patient_test, tokenized_patient_submission","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:21:28.304458Z","iopub.execute_input":"2022-05-04T08:21:28.304713Z","iopub.status.idle":"2022-05-04T08:22:03.716845Z","shell.execute_reply.started":"2022-05-04T08:21:28.304685Z","shell.execute_reply":"2022-05-04T08:22:03.716058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 1\n\ndataloader_train = DataLoader(tokenized_patient_train, batch_size=batch_size, drop_last=True)\ndataloader_test = DataLoader(tokenized_patient_test, batch_size=batch_size, drop_last=True, shuffle=False)\ndataloader_submission = DataLoader(tokenized_patient_submission, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T08:22:06.409185Z","iopub.execute_input":"2022-05-04T08:22:06.409443Z","iopub.status.idle":"2022-05-04T08:22:06.414621Z","shell.execute_reply.started":"2022-05-04T08:22:06.409416Z","shell.execute_reply":"2022-05-04T08:22:06.413952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"class PatientModel(nn.Module): # this is typical custom PyTorch model \n    def __init__(self, checkpoint=\"../input/distilbertbaseuncased\"):\n        super(PatientModel, self).__init__()\n\n        self.bert = AutoModel.from_pretrained(checkpoint) # load the model\n        # for param in self.bert.parameters(): param.requires_grad = False\n        \n        self.dropout = nn.Dropout(0.2) # random zeroing\n        self.fc = nn.Linear(768, 1) # classifier layer\n\n    def forward(self, batch):\n        ids = batch[\"input_ids\"]\n        mask = batch[\"attention_mask\"]\n        output = self.bert(ids, attention_mask=mask) # feed only these two\n        x = output[0] # or \"pooler_output\"\n\n        x = self.dropout(self.fc(x)).squeeze(-1) # classify and squeeze\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:33:02.335551Z","iopub.execute_input":"2022-05-04T07:33:02.336274Z","iopub.status.idle":"2022-05-04T07:33:02.345861Z","shell.execute_reply.started":"2022-05-04T07:33:02.336229Z","shell.execute_reply":"2022-05-04T07:33:02.345136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # gpu check\nmodel = PatientModel().to(device) # initialize the model and send it to the device, hopefully to a GPU\ncriterion = torch.nn.BCEWithLogitsLoss(reduction=\"none\") # initialize the loss\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.00001) # adamw with learning rate 0.00001\nscheduler = StepLR(optimizer, step_size=1, gamma=0.7) # learning rate adjuster\n\nnum_epochs = 2 # epoch size is 2\nfor epoch in range(num_epochs): # iterate epoch size\n    for i, batch in enumerate(tqdm(dataloader_train)): # output data amount of batch_size\n        batch = {k: v.to(device) for k, v in batch.items()} # send the data to device\n        \n        optimizer.zero_grad() # refresh/zero the gradient\n        outputs = model(batch) # get outputs from model\n        loss = criterion(outputs, batch[\"labels\"]) # criterion(y_pred, y_true)\n        loss = torch.masked_select(loss, batch[\"labels\"] > -1).mean() # only select second sequence (patient notes) \n        loss.backward() # train, calculate gradients\n        optimizer.step() # update model parameters\n\n        if i % 800 == 0: # logs\n            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                    epoch + 1,\n                    i * batch_size, \n                    len(dataloader_train.dataset),\n                    100. * i / len(dataloader_train), \n                    loss.item()\n                )\n            )\n    \n    scheduler.step() # learning rate adjuster\n\ntorch.save(model.state_dict(), \"patient_model.pth\") # save our model","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:38:57.157685Z","iopub.execute_input":"2022-05-04T07:38:57.157972Z","iopub.status.idle":"2022-05-04T07:56:47.648199Z","shell.execute_reply.started":"2022-05-04T07:38:57.157925Z","shell.execute_reply":"2022-05-04T07:56:47.646838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def predictions_to_locations(predictions_batch, offsets_batch, seq_ids_batch, test=False):\n    sigmoid = lambda z: 1 / (1 + np.exp(-z)) # for mapping logits between -1.0 to 1.0\n    all_predictions = [] # will contain 0s (not a annotations/features) and 1s (an annotation)\n    for predictions, offsets, seq_ids in zip(predictions_batch, offsets_batch, seq_ids_batch): # iterate over data\n        offsets = np.array([offsets[::2], offsets[1::2]]).reshape(-1, 2) # creating 2D array from 1D by pairwise\n        \n        predictions = sigmoid(predictions) # logits/probabilities mapped between -1.0 to 1.0\n        \n        start_idx = None # will hold starting location of annotation\n        current_prediction = [] # will hold current and be appended to all_predictions\n        for prediction, offset, seq_id in zip(predictions, offsets, seq_ids): # iterate over words/tokens\n            if seq_id is None or seq_id == 0 or np.isnan(seq_id): # ignore other than second sequence (patient notes)\n                continue\n            if prediction > 0.5: # it likely be a annotation/feature/key phrase\n                if start_idx is None: # a flag for holding very first index of annotation/key phrase\n                    start_idx = offset[0]\n                end_idx = offset[1] # will hold last index of annotation and be overrided until not a annotation \n            elif start_idx is not None and start_idx != 0 and end_idx != 0: # found an annotation and location is not (0, 0)\n                if test:\n                    current_prediction.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_prediction.append((start_idx, end_idx)) # append found location\n                start_idx = None # restart the process\n        if test:\n            all_predictions.append(\"; \".join(current_prediction))\n        else:\n            all_predictions.append(current_prediction) # append all locations from single data\n    return all_predictions","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:59:09.435304Z","iopub.execute_input":"2022-05-04T07:59:09.436276Z","iopub.status.idle":"2022-05-04T07:59:09.446394Z","shell.execute_reply.started":"2022-05-04T07:59:09.436234Z","shell.execute_reply":"2022-05-04T07:59:09.445575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_charwise_metrics(predictions_batch, offsets_batch, seq_ids_batch, labels_batch):\n    y_true = [] # will hold all ground truths\n    y_pred = [] # will hold all predictions\n    for predictions, offsets, seq_ids, labels in zip(predictions_batch, offsets_batch, seq_ids_batch, labels_batch): # iterate over data\n        offsets = np.array([offsets[::2], offsets[1::2]]).reshape(-1, 2) # creating 2D array from 1D by pairwise\n        \n        num_chars = 0 # calculate number of characters in the text\n        for start_idx, end_idx in offsets: # look at offset because of it holds locations of words/tokens \n            num_chars = max(num_chars, start_idx, end_idx) # get biggest location which means number of characters\n        \n        char_true = np.zeros((num_chars)) # will consist of 0s and 1s indicates not a annotation and an annotation for ground truths\n        for offset, seq_id, label in zip(offsets, seq_ids, labels): # iterate over data\n            if seq_id is None or seq_id == 0 or np.isnan(seq_id): # ignore other than second sequence (patient notes)\n                continue\n            if int(label) == 1: # an annotation is found\n                char_true[offset[0]:offset[1]] = 1 # mark as 1\n        \n        char_preds = np.zeros((num_chars)) # will consist of 0s and 1s indicates not a annotation and an annotation for predictions\n        for start_idx, end_idx in predictions: # we already predicted but only start and end pairs\n            char_preds[start_idx:end_idx] = 1 # make it character base\n        \n        y_true.extend(char_true) # concatenate all for metrics\n        y_pred.extend(char_preds) # concatenate all for metrics\n    \n    micro_f1 = f1_score(y_true, y_pred) # required micro-averaged F1 score.\n    results = precision_recall_fscore_support(y_true, y_pred, average=\"binary\") # other metrics\n    return {\n        \"micro_f1\": micro_f1,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"fbeta_score\": results[2],\n        \"support\": results[3],\n    }","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:59:10.436992Z","iopub.execute_input":"2022-05-04T07:59:10.437303Z","iopub.status.idle":"2022-05-04T07:59:10.451829Z","shell.execute_reply.started":"2022-05-04T07:59:10.43727Z","shell.execute_reply":"2022-05-04T07:59:10.450788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = PatientModel().to(device)\n# model.load_state_dict(torch.load(\"patient_model.pth\", map_location=device))\n\nmodel.eval() # switch to evaluation mode\npredictions = [] # will hold data\noffsets = []\nseq_ids = []\nlabels = []\nwith torch.no_grad():\n    for batch in tqdm(dataloader_test):\n        batch = {k: v.to(device) for k, v in batch.items()} # send to the device\n        \n        outputs = model(batch) # get outputs from model\n        \n        predictions.append(outputs.cpu().numpy()) # store them\n        offsets.append(batch[\"offset_mapping\"].cpu().numpy())\n        seq_ids.append(batch[\"sequence_ids\"].cpu().numpy())\n        labels.append(batch[\"labels\"].cpu().numpy())\n\npredictions = np.concatenate(predictions) # concatenate along axis 0 \noffsets = np.concatenate(offsets)\nseq_ids = np.concatenate(seq_ids)\nlabels = np.concatenate(labels)\n\nlocation_predictions = predictions_to_locations(predictions, offsets, seq_ids)\ncalculate_charwise_metrics(location_predictions, offsets, seq_ids, labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T07:59:13.341331Z","iopub.execute_input":"2022-05-04T07:59:13.341584Z","iopub.status.idle":"2022-05-04T07:59:52.729276Z","shell.execute_reply.started":"2022-05-04T07:59:13.341558Z","shell.execute_reply":"2022-05-04T07:59:52.728528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = PatientModel().to(device)\n# model.load_state_dict(torch.load(\"patient_model.pth\", map_location=device))\n\nmodel.eval() # switch to evaluation mode\npredictions = [] # will hold data\noffsets = []\nseq_ids = []\nlabels = []\nwith torch.no_grad():\n    for batch in tqdm(dataloader_submission):\n        batch = {k: v.to(device) for k, v in batch.items()} # send to the device\n        \n        outputs = model(batch) # get outputs from model\n        \n        predictions.append(outputs.cpu().numpy()) # store them\n        offsets.append(batch[\"offset_mapping\"].cpu().numpy())\n        seq_ids.append(batch[\"sequence_ids\"].cpu().numpy())\n        labels.append(batch[\"labels\"].cpu().numpy())\n\npredictions = np.concatenate(predictions) # concatenate along axis 0 \noffsets = np.concatenate(offsets)\nseq_ids = np.concatenate(seq_ids)\nlabels = np.concatenate(labels)\n\nlocation_predictions = predictions_to_locations(predictions, offsets, seq_ids, test=True)\ntest_df = create_submission_df()\ntest_df[\"location\"] = location_predictions\ntest_df[[\"id\", \"location\"]].to_csv(\"submission.csv\", index=False)\npd.read_csv(\"submission.csv\").head()","metadata":{"execution":{"iopub.status.busy":"2022-05-04T09:23:17.557181Z","iopub.execute_input":"2022-05-04T09:23:17.557918Z","iopub.status.idle":"2022-05-04T09:23:18.032503Z","shell.execute_reply.started":"2022-05-04T09:23:17.557876Z","shell.execute_reply":"2022-05-04T09:23:18.031772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References","metadata":{}},{"cell_type":"markdown","source":"1. https://pytorch.org/\n1. https://huggingface.co/\n1. https://en.wikipedia.org/wiki/F-score\n1. https://huggingface.co/bert-base-uncased\n1. https://huggingface.co/docs/datasets/index\n1. https://huggingface.co/distilbert-base-uncased\n1. https://en.wikipedia.org/wiki/Sigmoid_function\n1. https://huggingface.co/docs/transformers/index\n1. https://www.kaggle.com/code/odins0n/nbme-detailed-eda\n1. https://towardsdatascience.com/the-f1-score-bec2bbc38aa6\n1. https://www.kaggle.com/c/nbme-score-clinical-patient-notes\n1. https://www.kaggle.com/code/nbroad/qa-ner-hybrid-train-nbme\n1. https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n1. https://www.kaggle.com/code/utcarshagrawal/nbme-complete-eda\n1. https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n1. https://analyticsindiamag.com/ultimate-guide-to-pytorch-optimizers/\n1. https://www.kaggle.com/code/theoviel/evaluation-metric-folds-baseline\n1. https://www.kaggle.com/code/tomohiroh/nbme-bert-for-beginners/notebook\n1. https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n1. https://www.kaggle.com/code/ruchi798/score-clinical-patient-notes-spacy-w-b\n1. https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\n1. https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/\n1. https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/\n1. https://medium.com/dejunhuang/learning-day-57-practical-5-loss-function-crossentropyloss-vs-bceloss-in-pytorch-softmax-vs-bd866c8a0d23","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}