{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Roberta Strikes Back !\n",
    "\n",
    " This notebook demonstates that you can reach decent performances with Roberta, if you process predictions correctly.\n",
    "\n",
    " The training procedure will not be shared as it would most likely destroy the leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/data/\"\n",
    "OUT_PATH = \"/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/AILab2023/input/\"\n",
    "WEIGHTS_FOLDER = \"/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/AILab2023/input/roberta-large/\"\n",
    "NUM_WORKERS = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature_text(text):\n",
    "    text = re.sub('I-year', '1-year', text)\n",
    "    text = re.sub('-OR-', \" or \", text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_spaces(txt):\n",
    "    txt = re.sub('\\n', ' ', txt)\n",
    "    txt = re.sub('\\t', ' ', txt)\n",
    "    txt = re.sub('\\r', ' ', txt)\n",
    "#     txt = re.sub(r'\\s+', ' ', txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def load_and_prepare_test(root=\"\"):\n",
    "    patient_notes = pd.read_csv(root + \"patient_notes.csv\")\n",
    "    features = pd.read_csv(root + \"features.csv\")\n",
    "    df = pd.read_csv(root + \"test.csv\")\n",
    "\n",
    "    df = df.merge(features, how=\"left\", on=[\"case_num\", \"feature_num\"])\n",
    "    df = df.merge(patient_notes, how=\"left\", on=['case_num', 'pn_num'])\n",
    "\n",
    "    df['pn_history'] = df['pn_history'].apply(lambda x: x.strip())\n",
    "    df['feature_text'] = df['feature_text'].apply(process_feature_text)\n",
    "\n",
    "    df['feature_text'] = df['feature_text'].apply(clean_spaces)\n",
    "    df['clean_text'] = df['pn_history'].apply(clean_spaces)\n",
    "\n",
    "    df['target'] = \"\"\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def token_pred_to_char_pred(token_pred, offsets):\n",
    "    char_pred = np.zeros((np.max(offsets), token_pred.shape[1]))\n",
    "    for i in range(len(token_pred)):\n",
    "        s, e = int(offsets[i][0]), int(offsets[i][1])  # start, end\n",
    "        char_pred[s:e] = token_pred[i]\n",
    "\n",
    "        if token_pred.shape[1] == 3:  # following characters cannot be tagged as start\n",
    "            s += 1\n",
    "            char_pred[s: e, 1], char_pred[s: e, 2] = (\n",
    "                np.max(char_pred[s: e, 1:], 1),\n",
    "                np.min(char_pred[s: e, 1:], 1),\n",
    "            )\n",
    "\n",
    "    return char_pred\n",
    "\n",
    "\n",
    "def labels_to_sub(labels):\n",
    "    all_spans = []\n",
    "    for label in labels:\n",
    "        indices = np.where(label > 0)[0]\n",
    "        indices_grouped = [\n",
    "            list(g) for _, g in itertools.groupby(\n",
    "                indices, key=lambda n, c=itertools.count(): n - next(c)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        spans = [f\"{min(r)} {max(r) + 1}\" for r in indices_grouped]\n",
    "        all_spans.append(\";\".join(spans))\n",
    "    return all_spans\n",
    "\n",
    "\n",
    "def char_target_to_span(char_target):\n",
    "    spans = []\n",
    "    start, end = 0, 0\n",
    "    for i in range(len(char_target)):\n",
    "        if char_target[i] == 1 and char_target[i - 1] == 0:\n",
    "            if end:\n",
    "                spans.append([start, end])\n",
    "            start = i\n",
    "            end = i + 1\n",
    "        elif char_target[i] == 1:\n",
    "            end = i + 1\n",
    "        else:\n",
    "            if end:\n",
    "                spans.append([start, end])\n",
    "            start, end = 0, 0\n",
    "    return spans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shannon/anaconda3/envs/ai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def get_tokenizer(name, precompute=False, df=None, folder=None):\n",
    "    if folder is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(folder)\n",
    "\n",
    "    tokenizer.name = name\n",
    "    tokenizer.special_tokens = {\n",
    "        \"sep\": tokenizer.sep_token_id,\n",
    "        \"cls\": tokenizer.cls_token_id,\n",
    "        \"pad\": tokenizer.pad_token_id,\n",
    "    }\n",
    "\n",
    "    if precompute:\n",
    "        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n",
    "    else:\n",
    "        tokenizer.precomputed = None\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def precompute_tokens(df, tokenizer):\n",
    "    feature_texts = df[\"feature_text\"].unique()\n",
    "\n",
    "    ids = {}\n",
    "    offsets = {}\n",
    "\n",
    "    for feature_text in feature_texts:\n",
    "        encoding = tokenizer(\n",
    "            feature_text,\n",
    "            return_token_type_ids=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_attention_mask=False,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        ids[feature_text] = encoding[\"input_ids\"]\n",
    "        offsets[feature_text] = encoding[\"offset_mapping\"]\n",
    "\n",
    "    texts = df[\"clean_text\"].unique()\n",
    "\n",
    "    for text in texts:\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            return_token_type_ids=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_attention_mask=False,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        ids[text] = encoding[\"input_ids\"]\n",
    "        offsets[text] = encoding[\"offset_mapping\"]\n",
    "\n",
    "    return {\"ids\": ids, \"offsets\": offsets}\n",
    "\n",
    "\n",
    "def encodings_from_precomputed(feature_text, text, precomputed, tokenizer, max_len=300):\n",
    "    tokens = tokenizer.special_tokens\n",
    "\n",
    "    # Input ids\n",
    "    if \"roberta\" in tokenizer.name:\n",
    "        qa_sep = [tokens[\"sep\"], tokens[\"sep\"]]\n",
    "    else:\n",
    "        qa_sep = [tokens[\"sep\"]]\n",
    "\n",
    "    input_ids = [tokens[\"cls\"]] + precomputed[\"ids\"][feature_text] + qa_sep\n",
    "    n_question_tokens = len(input_ids)\n",
    "\n",
    "    input_ids += precomputed[\"ids\"][text]\n",
    "    input_ids = input_ids[: max_len - 1] + [tokens[\"sep\"]]\n",
    "\n",
    "    # Token type ids\n",
    "    if \"roberta\" not in tokenizer.name:\n",
    "        token_type_ids = np.ones(len(input_ids))\n",
    "        token_type_ids[:n_question_tokens] = 0\n",
    "        token_type_ids = token_type_ids.tolist()\n",
    "    else:\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "\n",
    "    # Offsets\n",
    "    offsets = [(0, 0)] * n_question_tokens + precomputed[\"offsets\"][text]\n",
    "    offsets = offsets[: max_len - 1] + [(0, 0)]\n",
    "\n",
    "    # Padding\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "\n",
    "    encoding = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"offset_mapping\": offsets,\n",
    "    }\n",
    "\n",
    "    return encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PatientNoteDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.texts = df['clean_text'].values\n",
    "        self.feature_text = df['feature_text'].values\n",
    "        self.char_targets = df['target'].values.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        feature_text = self.feature_text[idx]\n",
    "        char_target = self.char_targets[idx]\n",
    "\n",
    "        # Tokenize\n",
    "        if self.tokenizer.precomputed is None:\n",
    "            encoding = self.tokenizer(\n",
    "                feature_text,\n",
    "                text,\n",
    "                return_token_type_ids=True,\n",
    "                return_offsets_mapping=True,\n",
    "                return_attention_mask=False,\n",
    "                truncation=\"only_second\",\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "            )\n",
    "            raise NotImplementedError(\"fix issues with question offsets\")\n",
    "        else:\n",
    "            encoding = encodings_from_precomputed(\n",
    "                feature_text,\n",
    "                text,\n",
    "                self.tokenizer.precomputed,\n",
    "                self.tokenizer,\n",
    "                max_len=self.max_len\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(encoding[\"token_type_ids\"], dtype=torch.long),\n",
    "            \"target\": torch.tensor([0], dtype=torch.float),\n",
    "            \"offsets\": np.array(encoding[\"offset_mapping\"]),\n",
    "            \"text\": text,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def plot_annotation(df, pn_num):\n",
    "    options = {\"colors\": {}}\n",
    "\n",
    "    df_text = df[df[\"pn_num\"] == pn_num].reset_index(drop=True)\n",
    "\n",
    "    text = df_text[\"pn_history\"][0]\n",
    "    ents = []\n",
    "\n",
    "    for spans, feature_text, feature_num in df_text[[\"span\", \"feature_text\", \"feature_num\"]].values:\n",
    "        for s in spans:\n",
    "            ents.append({\"start\": int(s[0]), \"end\": int(s[1]), \"label\": feature_text})\n",
    "\n",
    "        options[\"colors\"][feature_text] =  f\"rgb{tuple(np.random.randint(100, 255, size=3))}\"\n",
    "\n",
    "    doc = {\"text\": text, \"ents\": sorted(ents, key=lambda i: i[\"start\"])}\n",
    "\n",
    "    spacy.displacy.render(doc, style=\"ent\", options=options, manual=True, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "\n",
    "class NERTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes=1,\n",
    "        config_file=None,\n",
    "        pretrained=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.name = model\n",
    "        self.pad_idx = 1 if \"roberta\" in self.name else 0\n",
    "\n",
    "        transformers.logging.set_verbosity_error()\n",
    "\n",
    "        if config_file is None:\n",
    "            config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n",
    "        else:\n",
    "            config = torch.load(config_file)\n",
    "\n",
    "        if pretrained:\n",
    "            self.transformer = AutoModel.from_pretrained(model, config=config)\n",
    "        else:\n",
    "            self.transformer = AutoModel.from_config(config)\n",
    "\n",
    "        self.nb_features = config.hidden_size\n",
    "\n",
    "#         self.cnn = nn.Identity()\n",
    "        self.logits = nn.Linear(self.nb_features, num_classes)\n",
    "\n",
    "    def forward(self, tokens, token_type_ids):\n",
    "        \"\"\"\n",
    "        Usual torch forward function\n",
    "\n",
    "        Arguments:\n",
    "            tokens {torch tensor} -- Sentence tokens\n",
    "            token_type_ids {torch tensor} -- Sentence tokens ids\n",
    "        \"\"\"\n",
    "        hidden_states = self.transformer(\n",
    "            tokens,\n",
    "            attention_mask=(tokens != self.pad_idx).long(),\n",
    "            token_type_ids=token_type_ids,\n",
    "        )[-1]\n",
    "\n",
    "        features = hidden_states[-1]\n",
    "\n",
    "        logits = self.logits(features)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Loads weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def load_model_weights(model, filename, verbose=1, cp_folder=\"\", strict=True):\n",
    "    \"\"\"\n",
    "    Loads the weights of a PyTorch model. The exception handles cpu/gpu incompatibilities.\n",
    "\n",
    "    Args:\n",
    "        model (torch model): Model to load the weights to.\n",
    "        filename (str): Name of the checkpoint.\n",
    "        verbose (int, optional): Whether to display infos. Defaults to 1.\n",
    "        cp_folder (str, optional): Folder to load from. Defaults to \"\".\n",
    "        strict (bool, optional): Whether to allow missing/additional keys. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        torch model: Model with loaded weights.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n",
    "\n",
    "    model.load_state_dict(\n",
    "        torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n",
    "        strict=strict,\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def predict(model, dataset, data_config, activation=\"softmax\"):\n",
    "    \"\"\"\n",
    "    Usual predict torch function\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=data_config['val_bs'],\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader):\n",
    "            ids, token_type_ids = data[\"ids\"], data[\"token_type_ids\"]\n",
    "\n",
    "            y_pred = model(ids, token_type_ids)\n",
    "\n",
    "            if activation == \"sigmoid\":\n",
    "                y_pred = y_pred.sigmoid()\n",
    "            elif activation == \"softmax\":\n",
    "                y_pred = y_pred.softmax(-1)\n",
    "\n",
    "            preds += [\n",
    "                token_pred_to_char_pred(y, offsets) for y, offsets\n",
    "                in zip(y_pred.detach().cpu().numpy(), data[\"offsets\"].numpy())\n",
    "            ]\n",
    "\n",
    "    return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test(df, exp_folder, config, cfg_folder=None):\n",
    "    preds = []\n",
    "\n",
    "    if cfg_folder is not None:\n",
    "        model_config_file = cfg_folder + config.name.split('/')[-1] + \"/config.pth\"\n",
    "        tokenizer_folder = cfg_folder + config.name.split('/')[-1] + \"/tokenizers/\"\n",
    "    else:\n",
    "        model_config_file, tokenizer_folder = None, None\n",
    "\n",
    "    tokenizer = get_tokenizer(\n",
    "        config.name, precompute=config.precompute_tokens, df=df, folder=tokenizer_folder\n",
    "    )\n",
    "\n",
    "    dataset = PatientNoteDataset(\n",
    "        df,\n",
    "        tokenizer,\n",
    "        max_len=config.max_len,\n",
    "    )\n",
    "\n",
    "    model = NERTransformer(\n",
    "        config.name,\n",
    "        num_classes=config.num_classes,\n",
    "        config_file=model_config_file,\n",
    "        pretrained=False\n",
    "    )\n",
    "    model.zero_grad()\n",
    "\n",
    "    weights = sorted(glob.glob(exp_folder + \"*.pt\"))\n",
    "    for weight in weights:\n",
    "        model = load_model_weights(model, weight)\n",
    "\n",
    "        pred = predict(\n",
    "            model,\n",
    "            dataset,\n",
    "            data_config=config.data_config,\n",
    "            activation=config.loss_config[\"activation\"]\n",
    "        )\n",
    "        preds.append(pred)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Architecture\n",
    "    name = \"roberta-large\"\n",
    "    num_classes = 1\n",
    "\n",
    "    # Texts\n",
    "    max_len = 310\n",
    "    precompute_tokens = True\n",
    "\n",
    "    # Training    \n",
    "    loss_config = {\n",
    "        \"activation\": \"sigmoid\",\n",
    "    }\n",
    "\n",
    "    data_config = {\n",
    "        \"val_bs\": 16 if \"large\" in name else 32,\n",
    "        \"pad_token\": 1 if \"roberta\" in name else 0,\n",
    "    }\n",
    "\n",
    "    verbose = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>Family history of MI or Family history of myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Family history of thyroid disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>Chest pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>Intermittent symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family history of MI or Family history of myoc...   \n",
       "1                 Family history of thyroid disorder   \n",
       "2                                     Chest pressure   \n",
       "3                              Intermittent symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \\\n",
       "0  HPI: 17yo M presents with palpitations. Patien...   \n",
       "1  HPI: 17yo M presents with palpitations. Patien...   \n",
       "2  HPI: 17yo M presents with palpitations. Patien...   \n",
       "3  HPI: 17yo M presents with palpitations. Patien...   \n",
       "4  HPI: 17yo M presents with palpitations. Patien...   \n",
       "\n",
       "                                          clean_text target  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...         \n",
       "1  HPI: 17yo M presents with palpitations. Patien...         \n",
       "2  HPI: 17yo M presents with palpitations. Patien...         \n",
       "3  HPI: 17yo M presents with palpitations. Patien...         \n",
       "4  HPI: 17yo M presents with palpitations. Patien...         "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = load_and_prepare_test(root=DATA_PATH)\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -> Loading weights from /Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/AILab2023/input/roberta-large/roberta-large_0.pt\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for NERTransformer:\n\tUnexpected key(s) in state_dict: \"transformer.embeddings.position_ids\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb Cell 33\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m RobertaModel\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m preds \u001b[39m=\u001b[39m inference_test(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df_test,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     WEIGHTS_FOLDER,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     Config,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     cfg_folder\u001b[39m=\u001b[39;49mOUT_PATH\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb Cell 33\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(glob\u001b[39m.\u001b[39mglob(exp_folder \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*.pt\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m weight \u001b[39min\u001b[39;00m weights:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     model \u001b[39m=\u001b[39m load_model_weights(model, weight)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     pred \u001b[39m=\u001b[39m predict(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         data_config\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mdata_config,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         activation\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mloss_config[\u001b[39m\"\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     preds\u001b[39m.\u001b[39mappend(pred)\n",
      "\u001b[1;32m/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m -> Loading weights from \u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cp_folder,filename)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     torch\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(cp_folder, filename), map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     strict\u001b[39m=\u001b[39;49mstrict,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/01_AI/Code/AILab2023/Project/examples/roberta-strikes-back.ipynb#Y140sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for NERTransformer:\n\tUnexpected key(s) in state_dict: \"transformer.embeddings.position_ids\". "
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel\n",
    "\n",
    "preds = inference_test(\n",
    "    df_test,\n",
    "    WEIGHTS_FOLDER,\n",
    "    Config,\n",
    "    cfg_folder=OUT_PATH\n",
    ")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['preds'] = preds\n",
    "df_test['preds'] = df_test.apply(lambda x: x['preds'][:len(x['clean_text'])], 1)\n",
    "\n",
    "df_test['preds'] = df_test['preds'].apply(lambda x: (x > 0.5).flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_test['span'] = df_test['preds'].apply(char_target_to_span)\n",
    "    plot_annotation(df_test, df_test['pn_num'][0])\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Unlike for deberta, spaces are not included in the offsets, we need to add them manually otherwise this will hurt performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_spaces(target, text):\n",
    "    target = np.copy(target)\n",
    "\n",
    "    if len(text) > len(target):\n",
    "        padding = np.zeros(len(text) - len(target))\n",
    "        target = np.concatenate([target, padding])\n",
    "    else:\n",
    "        target = target[:len(text)]\n",
    "\n",
    "    if text[0] == \" \":\n",
    "        target[0] = 0\n",
    "    if text[-1] == \" \":\n",
    "        target[-1] = 0\n",
    "\n",
    "    for i in range(1, len(text) - 1):\n",
    "        if text[i] == \" \":\n",
    "            if target[i] and not target[i - 1]:  # space before\n",
    "                target[i] = 0\n",
    "\n",
    "            if target[i] and not target[i + 1]:  # space after\n",
    "                target[i] = 0\n",
    "\n",
    "            if target[i - 1] and target[i + 1]:\n",
    "                target[i] = 1\n",
    "\n",
    "    return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['preds_pp'] = df_test.apply(lambda x: post_process_spaces(x['preds'], x['clean_text']), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_test['span'] = df_test['preds_pp'].apply(char_target_to_span)\n",
    "    plot_annotation(df_test, df_test['pn_num'][0])\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['location'] = labels_to_sub(df_test['preds_pp'].values)\n",
    "\n",
    "sub = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n",
    "\n",
    "sub = sub[['id']].merge(df_test[['id', \"location\"]], how=\"left\", on=\"id\")\n",
    "\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Done !"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
