{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition: NBME - Score Clinical Patient Notes\n",
    "## Intro\n",
    "https://www.kaggle.com/c/nbme-score-clinical-patient-notes\n",
    "\n",
    "Task: \n",
    "\"[...] develop an automated method to map clinical concepts from an exam rubric (e.g., “diminished appetite”) to various ways in which these concepts are expressed in clinical patient notes written by medical students (e.g., “eating less,” “clothes fit looser”). \"\n",
    "\n",
    "If possible evaluate by a micro-averages F1-score like in the original competition\n",
    "\n",
    "\"For each instance there is a collection of ground-truth spans and a collection of predicted spans. The spans we delimit with a semicolon, like: 0 3; 5 9.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful notebooks\n",
    "https://www.kaggle.com/code/leemop/version-4-simply-understand-competition-itself\n",
    "https://www.kaggle.com/code/drcapa/nbme-starter\n",
    "\n",
    "relatively simple and fast:\n",
    "https://www.kaggle.com/code/yasufuminakama/nbme-deberta-base-baseline-inference\n",
    "https://www.kaggle.com/code/hengzheng/nbme-bert-for-beginners\n",
    "\n",
    "transformers for the deberta one:\n",
    "https://www.kaggle.com/code/yasufuminakama/nbme-pip-wheels/notebook\n",
    "\n",
    "well commented:\n",
    "https://www.kaggle.com/code/nbroad/qa-ner-hybrid-train-nbme/notebook\n",
    "\n",
    "Roberta instead of Deberta:\n",
    "https://www.kaggle.com/code/theoviel/roberta-strikes-back#Data\n",
    "\n",
    "winner:\n",
    "https://www.kaggle.com/code/currypurin/nbme-final-submit-currypurin/notebook\n",
    "\n",
    "High scoring solutions are mostly based on the Microsoft DeBERTa model: https://github.com/microsoft/DeBERTa\n",
    "An ensemble/combination of models is used to create additional pseudo-labels\n",
    "\n",
    "Try DistilBERT (not trained from scratch, BERT is)\n",
    "\n",
    "\n",
    "What is OOF?: https://stackoverflow.com/questions/52396191/what-is-oof-approach-in-machine-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXES:\n",
    "# manually fixed bert.py to use collections.abc instead of sequence\n",
    "from DeBERTa import deberta\n",
    "import torch\n",
    "class MyModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Your existing model code\n",
    "    self.deberta = deberta.DeBERTa(pre_trained='large') # Or 'large' 'base-mnli' 'large-mnli' 'xlarge' 'xlarge-mnli' 'xlarge-v2' 'xxlarge-v2'\n",
    "    # Your existing model code\n",
    "    # do inilization as before\n",
    "    # \n",
    "    self.deberta.apply_state() # Apply the pre-trained model of DeBERTa at the end of the constructor\n",
    "    #\n",
    "  def forward(self, input_ids):\n",
    "    # The inputs to DeBERTa forward are\n",
    "    # `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary\n",
    "    # `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices selected in [0, 1]. \n",
    "    #    Type 0 corresponds to a `sentence A` and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n",
    "    # `attention_mask`: an optional parameter for input mask or attention mask. \n",
    "    #   - If it's an input mask, then it will be torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1]. \n",
    "    #      It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch. \n",
    "    #      It's the mask that we typically use for attention when a batch has varying length sentences.\n",
    "    #   - If it's an attention mask then if will be torch.LongTensor of shape [batch_size, sequence_length, sequence_length]. \n",
    "    #      In this case, it's a mask indicate which tokens in the sequence should be attended by other tokens in the sequence. \n",
    "    # `output_all_encoded_layers`: whether to output results of all encoder layers, default, True\n",
    "    encoding = deberta.bert(input_ids)[-1]\n",
    "\n",
    "# 2. Change your tokenizer with the the tokenizer built in DeBERta\n",
    "from DeBERTa import deberta\n",
    "vocab_path, vocab_type = deberta.load_vocab(pretrained_id='base')\n",
    "tokenizer = deberta.tokenizers[vocab_type](vocab_path)\n",
    "# We apply the same schema of special tokens as BERT, e.g. [CLS], [SEP], [MASK]\n",
    "max_seq_len = 512\n",
    "tokens = tokenizer.tokenize('Examples input text of DeBERTa')\n",
    "# Truncate long sequence\n",
    "tokens = tokens[:max_seq_len -2]\n",
    "# Add special tokens to the `tokens`\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_mask = [1]*len(input_ids)\n",
    "# padding\n",
    "paddings = max_seq_len-len(input_ids)\n",
    "input_ids = input_ids + [0]*paddings\n",
    "input_mask = input_mask + [0]*paddings\n",
    "features = {\n",
    "'input_ids': torch.tensor(input_ids, dtype=torch.int),\n",
    "'input_mask': torch.tensor(input_mask, dtype=torch.int)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
